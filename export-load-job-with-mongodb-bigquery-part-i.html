<!DOCTYPE html>
<html lang="en">

<head>
  <!-- ## for client-side less
  <link rel="stylesheet/less" type="text/css" href="https://sunnykrGupta.github.io/theme/css/style.less">
  <script src="http://cdnjs.cloudflare.com/ajax/libs/less.js/1.7.3/less.min.js" type="text/javascript"></script>
  -->
  <link rel="stylesheet" type="text/css" href="https://sunnykrGupta.github.io/theme/css/style.css">
  <link rel="stylesheet" type="text/css" href="https://sunnykrGupta.github.io/theme/css/pygments.css">
  <link rel="stylesheet" type="text/css" href="//fonts.googleapis.com/css?family=PT+Sans|PT+Serif|PT+Mono">

  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="author" content="Sunny Kumar">
  <meta name="description" content="Posts and writings by Sunny Kumar">


<meta name="keywords" content="Google BigQuery, GCP, MongoDB, Cloud Storage, Google Cloud Platform">

  <title>
    Daemon Blog
&ndash; Export & Load Job with MongoDB - BigQuery Part-I  </title>

<script type="text/javascript">
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-63834161-1', 'auto');
  ga('send', 'pageview');

</script>
</head>

<body>
  <aside>
    <div id="user_meta">
      <a id="user_meta_logo" href="https://sunnykrGupta.github.io">
        <img src="https://sunnykrGupta.github.io/images/logo.jpg" alt="logo">
      </a>
      <h3><a href="https://sunnykrGupta.github.io">Sunny Kumar</a></h3>
      <p>Data Engineer, DevOps on Kubernetes, SysAdmin, Programmer, Optimist, Footballer :D</p>
      <nav>
          <h4><a href="https://sunnykrGupta.github.io/pages/projects.html">Project</a></h4>
          <h4><a href="https://sunnykrGupta.github.io/pages/about-me.html">About</a></h4>
          <h4><a href="https://sunnykrGupta.github.io/pages/contact-me.html">Contact</a></h4>
      </nav>
    </div>
  </aside>

  <main>
    <header>
      <nav>
          <a id="github" href="https://github.com/sunnykrGupta" target="_blank"><img src="https://sunnykrGupta.github.io/theme/images/Mono/webicon-github.png" alt="github"></a>
          <a id="twitter" href="https://twitter.com/Sunny_KrGupta" target="_blank"><img src="https://sunnykrGupta.github.io/theme/images/Mono/webicon-twitter.png" alt="twitter"></a>
          <a id="linkedin" href="https://www.linkedin.com/in/sunnykrgupta/" target="_blank"><img src="https://sunnykrGupta.github.io/theme/images/Mono/webicon-linkedin.png" alt="linkedin"></a>
          <a id="googleplus" href="https://plus.google.com/u/0/+SunnyKrGUPTA" target="_blank"><img src="https://sunnykrGupta.github.io/theme/images/Mono/webicon-googleplus.png" alt="googleplus"></a>
          <a id="mail" href="mailto:sunnygupta.kr@gmail.com" target="_blank"><img src="https://sunnykrGupta.github.io/theme/images/Mono/webicon-mail.png" alt="mail"></a>
      </nav>
      <br />
      <p>
        <a href="https://sunnykrGupta.github.io/index.html">Index</a> &nbsp;&nbsp; &brvbar; <a href="https://sunnykrGupta.github.io/archives.html">&nbsp;&nbsp;Archives</a>
      </p>

    </header>

<article>
  <div class="article_title">
    <h3><a href="https://sunnykrGupta.github.io/export-load-job-with-mongodb-bigquery-part-i.html">Export & Load Job with MongoDB - BigQuery Part-I</a></h3>
  </div>
  <div class="article_text">
    <p><img alt="Google BigQuery" src="/images/bq-series/part1/bquery.svg"></p>
<p>This blog is intended for audience who wanted to get into fundamentals of BigQuery (short for BQ) and related jobs needed to get your data inside BigQuery system.</p>
<h3><a href="https://cloud.google.com/bigquery/what-is-bigquery">Google BigQuery</a></h3>
<p><strong>What ?</strong> - BigQuery is Google's fully managed, petabyte scale, low cost analytics data warehouse.</p>
<p><strong>Why ?</strong> - BigQuery is NoOps—there is no infrastructure to manage and you don't need a database administrator—so you can focus on analyzing data to find meaningful insights, use familiar SQL, and take advantage of our pay-as-you-go model.</p>
<p><strong>How ?</strong> - Signup to <a href="https://bigquery.cloud.google.com/">Google Cloud platform - GCP</a> using your google account, start loading your data and leverage the power of this NoOps system.</p>
<hr>
<p><br></p>
<h3>Terminology in BigQuery</h3>
<p><strong>Dataset</strong></p>
<p>A dataset is contained within a specific project. Datasets enable you to organize and control access to your tables. A table must belong to a dataset, so you need to create at least one dataset before loading data into BigQuery.</p>
<p><strong>Tables</strong></p>
<p>A BigQuery table contains individual records organized in rows, and a data type assigned to each column (also called a field).</p>
<p><strong>Schema</strong></p>
<p>Each Every table is defined by a schema that describes field names, types, and other information. You can specify the schema of a table during the initial table creation request, or you can create a table without a schema and declare the schema in the query or load job that first populates the table. If you need to change the schema later, you can update the schema.</p>
<hr>
<p><br></p>
<h3>Loading Data into BigQuery</h3>
<p>In this article, we are going to use a mongoDB server to export our data and going to import into BQ. There are several other ways to import data into BQ.</p>
<p><br></p>
<h4>1. Export data from MongoDB</h4>
<p><img alt="MongoDB" src="/images/bq-series/part1/mongo-db-blog.jpeg"></p>
<p>In this example, i have a database in mongoDB server with name <code>restaurantdb</code> with collection name <code>restaurantCollection</code>. We are going to export using <code>mongoexport</code> binary available with mongodb server tools.</p>
<p><img alt="MongoDB server" src="/images/bq-series/part1/mongoexport.png"></p>
<div class="highlight"><pre><span class="nv">$ </span>mongoexport -d restaurantdb -c restaurantCollection -o restaurant.json
</pre></div>


<p>Once export is done, we can see content of <code>restaurant.json</code> file</p>
<div class="highlight"><pre><span class="nv">$ </span>head -n <span class="m">1</span> restaurant.json

<span class="o">{</span> <span class="s2">&quot;_id&quot;</span> : <span class="o">{</span> <span class="s2">&quot;</span><span class="nv">$oid</span><span class="s2">&quot;</span> : <span class="s2">&quot;55f14312c7447c3da7051b26&quot;</span> <span class="o">}</span>, <span class="se">\</span>
    <span class="s2">&quot;URL&quot;</span> : <span class="s2">&quot;http://www.just-eat.co.uk/restaurants-cn-chinese-cardiff/menu&quot;</span>, <span class="se">\</span>
    <span class="s2">&quot;address&quot;</span> : <span class="s2">&quot;228 City Road&quot;</span>, <span class="s2">&quot;name&quot;</span> : <span class="s2">&quot;.CN Chinese&quot;</span>, <span class="se">\</span>
    <span class="s2">&quot;outcode&quot;</span> : <span class="s2">&quot;CF24&quot;</span>, <span class="s2">&quot;postcode&quot;</span> : <span class="s2">&quot;3JH&quot;</span>, <span class="s2">&quot;type_of_food&quot;</span> : <span class="s2">&quot;Chinese&quot;</span> <span class="o">}</span>

//pretty json
<span class="o">{</span>
    <span class="s2">&quot;name&quot;</span>: <span class="s2">&quot;.CN Chinese&quot;</span>,
    <span class="s2">&quot;URL&quot;</span>: <span class="s2">&quot;http://www.just-eat.co.uk/restaurants-cn-chinese-cardiff/menu&quot;</span>,
    <span class="s2">&quot;outcode&quot;</span>: <span class="s2">&quot;CF24&quot;</span>,
    <span class="s2">&quot;postcode&quot;</span>: <span class="s2">&quot;3JH&quot;</span>,
    <span class="s2">&quot;address&quot;</span>: <span class="s2">&quot;228 City Road&quot;</span>,
    <span class="s2">&quot;_id&quot;</span>: <span class="o">{</span>
        <span class="s2">&quot;</span><span class="nv">$oid</span><span class="s2">&quot;</span>: <span class="s2">&quot;55f14312c7447c3da7051b26&quot;</span>
    <span class="o">}</span>,
    <span class="s2">&quot;type_of_food&quot;</span>: <span class="s2">&quot;Chinese&quot;</span>
<span class="o">}</span>
</pre></div>


<hr>
<p><br></p>
<h4>2. Prepare schema for Table</h4>
<p>Now we have our data ready in json format to be imported into BQ table. We need schema to design in order to import these records. Schema is skeleton of each field with datatype and the field not described in schema will not be imported. We have given all fields as <strong>NULLABLE</strong> ie, if field didn't came in any records BQ will define null value.</p>
<div class="highlight"><pre><span class="nv">$ </span>cat restaurantSchema.json
</pre></div>


<div class="highlight"><pre>[
    {
        &quot;name&quot;: &quot;name&quot;,
        &quot;type&quot;: &quot;STRING&quot;,
        &quot;mode&quot;: &quot;NULLABLE&quot;
    },
    {
        &quot;name&quot;: &quot;URL&quot;,
        &quot;type&quot;: &quot;STRING&quot;,
        &quot;mode&quot;: &quot;NULLABLE&quot;
    },
    {
        &quot;name&quot;: &quot;address&quot;,
        &quot;type&quot;: &quot;STRING&quot;,
        &quot;mode&quot;: &quot;NULLABLE&quot;
    },
    {
        &quot;name&quot;: &quot;outcode&quot;,
        &quot;type&quot;: &quot;STRING&quot;,
        &quot;mode&quot;: &quot;NULLABLE&quot;
    },
    {
        &quot;name&quot;: &quot;postcode&quot;,
        &quot;type&quot;: &quot;STRING&quot;,
        &quot;mode&quot;: &quot;NULLABLE&quot;
    },
    {
        &quot;name&quot;: &quot;type_of_food&quot;,
        &quot;type&quot;: &quot;STRING&quot;,
        &quot;mode&quot;: &quot;NULLABLE&quot;
    }
]
</pre></div>


<p>Reference for schema : <a href="https://cloud.google.com/bigquery/docs/schemas">https://cloud.google.com/bigquery/docs/schemas</a></p>
<hr>
<p><br></p>
<h4>3. Google Cloud SDK Installation</h4>
<p>We have schema, data ready to be imported in BQ, but in order to talk to APIs of GCP services, we need Cloud SDK to be installed in our system. We are going to use <code>gcloud</code> CLI tools in order to interact with our Cloud services running in GCP.</p>
<p><strong>Installation : </strong> <a href="https://cloud.google.com/sdk/">https://cloud.google.com/sdk/</a></p>
<p>Once installation is done, run following command to verify account setup.</p>
<div class="highlight"><pre><span class="nv">$ </span>gcloud auth list

<span class="nv">$ </span>gcloud config list
</pre></div>


<hr>
<p><br></p>
<h4>4. Create BigQuery Dataset</h4>
<p>Go to your BigQuery in google console. <a href="https://bigquery.cloud.google.com">https://bigquery.cloud.google.com</a></p>
<p>Follow below instruction to create dataset.</p>
<blockquote>
<p><strong>Step 1:</strong></p>
</blockquote>
<p><img alt="Create Dataset Step 1" src="/images/bq-series/part1/create-ds-1.png"></p>
<blockquote>
<p><strong>Step 2:</strong></p>
</blockquote>
<p><img alt="Create Dataset Step 2" src="/images/bq-series/part1/create-ds-2.png"></p>
<p><br></p>
<blockquote>
<p>To create dataset through <code>bq</code> command line interface.</p>
</blockquote>
<div class="highlight"><pre><span class="nv">$ </span>bq mk -d --data_location<span class="o">=</span>US   BQ_Dataset

// Verify your dataset creation
<span class="nv">$ </span>bq ls

    datasetId
 ----------------
  BQ_Dataset
</pre></div>


<p><strong>Read :</strong> <a href="https://cloud.google.com/bigquery/docs/datasets#create-dataset">https://cloud.google.com/bigquery/docs/datasets#create-dataset</a></p>
<hr>
<p><br></p>
<h4>5. Load data into BigQuery</h4>
<p>Now, my directory consists two files ie, data and schema.</p>
<div class="highlight"><pre><span class="nv">$ </span>tree
.
├── restaurant.json
└── restaurantSchema.json

<span class="m">0</span> directories, <span class="m">2</span> files
</pre></div>


<p>Run command to load data into BQ.  Once you submit load job, it will take seconds to minute depends on size of data you are importing into BQ table.</p>
<div class="highlight"><pre><span class="n">Ex</span><span class="o">:</span> <span class="n">bq</span> <span class="n">load</span> <span class="o">--</span><span class="n">project_id</span><span class="o">=&lt;</span><span class="n">PROJECT</span><span class="o">-</span><span class="n">ID</span><span class="o">&gt;</span> <span class="o">--</span><span class="n">source_format</span><span class="o">=</span><span class="n">NEWLINE_DELIMITED_JSON</span> <span class="o">\</span>
    <span class="n">mydataset</span><span class="o">.</span><span class="na">mytable</span> <span class="o">./</span><span class="n">myfile</span><span class="o">.</span><span class="na">json</span> <span class="o">./</span><span class="n">myschema</span><span class="o">.</span><span class="na">json</span>

<span class="n">$</span> <span class="n">bq</span> <span class="n">load</span> <span class="o">--</span><span class="n">project_id</span><span class="o">=</span><span class="n">mimetic</span><span class="o">-</span><span class="n">slate</span><span class="o">-</span><span class="mi">179915</span>   <span class="o">\</span>
    <span class="o">--</span><span class="n">source_format</span><span class="o">=</span><span class="n">NEWLINE_DELIMITED_JSON</span> <span class="o">--</span><span class="n">max_bad_records</span> <span class="mi">10</span> <span class="o">\</span>
    <span class="n">BQ_Dataset</span><span class="o">.</span><span class="na">Restaurant</span> <span class="o">./</span><span class="n">restaurant</span><span class="o">.</span><span class="na">json</span>  <span class="o">./</span><span class="n">restaurantSchema</span><span class="o">.</span><span class="na">json</span>
</pre></div>


<p><code>--max_bad_records 10</code> are additional flags to allow 10 bad records while importing your job, exceeding this value will result in import failure.</p>
<p><br></p>
<p><strong>Import through Cloud Storage</strong></p>
<p><img alt="Cloud Storage" src="/images/bq-series/part1/google-cloud-storage.png"></p>
<blockquote>
<p>Another methods to import the data through <code>Cloud Storage</code>, this method is lot faster compared to above one.</p>
</blockquote>
<p><strong>Read : </strong> <a href="https://cloud.google.com/storage/docs/creating-buckets">https://cloud.google.com/storage/docs/creating-buckets</a></p>
<div class="highlight"><pre>//to create Cloud storage bucket for this example.
$ gsutil mb  gs://bq-storage

//to verify bucket creation
$ gsutil ls
gs://bq-storage/
</pre></div>


<p>Now , we will upload our data <code>restaurant.json</code> to storage in bucket <code>gs://bq-storage/</code></p>
<div class="highlight"><pre>//Run command to upload
$ gsutil cp restaurant.json gs://bq-storage/restaurant.json
</pre></div>


<p>Now, we can use storage URL to import into BQ tables.</p>
<div class="highlight"><pre><span class="nv">$ </span>bq load --project_id<span class="o">=</span>mimetic-slate-179915  <span class="se">\</span>
    --source_format<span class="o">=</span>NEWLINE_DELIMITED_JSON --max_bad_records <span class="m">10</span>  <span class="se">\</span>
    BQ_Dataset.Restaurant <span class="se">\</span>
    gs://bq-storage/restaurant.json ./restaurantSchema.json
</pre></div>


<p><br>
<br></p>
<p>After <code>bq load</code> finished, run following command to verify the table creation.</p>
<div class="highlight"><pre><span class="nv">$ </span>bq show BQ_Dataset.Restaurant
</pre></div>


<blockquote>
<p><strong>Output will be similar to this :</strong></p>
</blockquote>
<p><img alt="bq show" src="/images/bq-series/part1/bq-show.png"></p>
<p>You can also verify in bigQuery UI after hitting refresh. Visit to table and click <em>preview</em>. You will start seeing records in table.</p>
<p><img alt="bq show" src="/images/bq-series/part1/table-preview.png"></p>
<p><strong>More into bq CLI :</strong> <a href="https://cloud.google.com/storage/docs/creating-buckets">https://cloud.google.com/bigquery/bq-command-line-tool</a></p>
<p><strong>More into gsutil CLI :</strong> <a href="https://cloud.google.com/storage/docs/quickstart-gsutil">https://cloud.google.com/storage/docs/quickstart-gsutil</a></p>
<hr>
<p><br>
<br></p>
<h3>SQL Query in BQ Table</h3>
<p>We are going to run a simple query to show the output.</p>
<div class="highlight"><pre>SELECT
  name,
  address
FROM
  [mimetic-slate-179915:BQ_Dataset.Restaurant]
WHERE
  type_of_food = &#39;Thai&#39;
GROUP BY
  name, address
</pre></div>


<p><img alt="query" src="/images/bq-series/part1/query.png"></p>
<hr>
<p><br></p>
<h3>Conclusion</h3>
<ul>
<li>BigQuery is a query service that allows you to run SQL-like queries against
multiple terabytes of data in a matter of seconds. The technology is one of the Google’s core technologies, like MapReduce and Bigtable, and has been used
by Google internally for various analytic tasks since 2006.</li>
<li>While MapReduce is suitable for long-running batch processes such as data
mining, BigQuery is the best choice for ad hoc OLAP/BI queries that require
results as fast as possible.</li>
<li>Wildcard can also be applied into Bigquery tables to expand your computations to multiple tables.</li>
<li>BigQuery is the cloud-powered massively parallel
query database that provides extremely high full-scan query performance
and cost effectiveness compared to traditional data warehouse solutions
and appliances</li>
</ul>
<blockquote>
<p><strong>Next from here:</strong> Play around with more <a href="https://cloud.google.com/bigquery/docs/how-to">functionality available in BigQuery</a> and dive into it for more computation hungry jobs.</p>
</blockquote>
<hr>
<p>That's all from this <strong>series Part-I</strong>. Hope you get basic understanding of import jobs, storage and basic outline of BigQuery from this page. I have seen power of BigQuery in my workplace to crunch 100-120 TB of data and getting results in minute or two, its really incredibly awesome. I would appreciate a feedback via comments available below and claps on medium.</p>
<h5>Medium Blog : <a href="https://medium.com/@sunnykrgupta/export-load-job-with-mongodb-bigquery-part-i-64a00eb5266b">medium.com/@sunnykrgupta/export-load-job-with-mongodb-bigquery-part-i</a></h5>
<p>In next blog which is part of this series, i will be covering more into <em>Streaming feature available in Bigquery</em> to push data in BQ tables in real-time to make it available for instant query on changing dataset.</p>
  </div>
  <div class="article_meta">
    <p>Posted on: Mon 16 October 2017</p>
    <p>Category: <a href="https://sunnykrGupta.github.io/category/blog-series.html">Blog, Series</a>
 &ndash; Tags:
      <a href="https://sunnykrGupta.github.io/tag/google-bigquery.html">Google BigQuery</a>,      <a href="https://sunnykrGupta.github.io/tag/gcp.html">GCP</a>,      <a href="https://sunnykrGupta.github.io/tag/mongodb.html">MongoDB</a>,      <a href="https://sunnykrGupta.github.io/tag/cloud-storage.html">Cloud Storage</a>,      <a href="https://sunnykrGupta.github.io/tag/google-cloud-platform.html">Google Cloud Platform</a>    </p>
  </div>
  <section>
      <p id="post-share-links">
          Share on:
          <a href="https://twitter.com/home?status=Export%20%26%20Load%20Job%20with%20MongoDB%20-%20BigQuery%20Part-I%20https%3A//sunnykrGupta.github.io/export-load-job-with-mongodb-bigquery-part-i.html" target="_blank" title="Share on Twitter"><img src="https://sunnykrGupta.github.io/theme/images/Share/twitter.svg" alt="twitter"></a>
          <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A//sunnykrGupta.github.io/export-load-job-with-mongodb-bigquery-part-i.html" target="_blank" title="Share on Facebook"><img src="https://sunnykrGupta.github.io/theme/images/Share/facebook.svg" alt="facebook"></a>
          <a href="https://plus.google.com/share?url=https%3A//sunnykrGupta.github.io/export-load-job-with-mongodb-bigquery-part-i.html" target="_blank" title="Share on Google Plus"><img src="https://sunnykrGupta.github.io/theme/images/Share/googleplus.svg" alt="google+"></a>
          <a href="mailto:?subject=Export%20%26%20Load%20Job%20with%20MongoDB%20-%20BigQuery%20Part-I&amp;body=https%3A//sunnykrGupta.github.io/export-load-job-with-mongodb-bigquery-part-i.html" target="_blank" title="Share via Email"><img src="https://sunnykrGupta.github.io/theme/images/Share/mail.svg" alt="email"></a>
      </p>
  </section>

  <div id="article_comments">
    <div id="disqus_thread"></div>
    <script type="text/javascript">
        var disqus_identifier = "export-load-job-with-mongodb-bigquery-part-i.html";
        (function() {
             var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
             dsq.src = 'https://sunnydaemon.disqus.com/embed.js';
             (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
         })();
    </script>
  </div>

</article>


    <div id="ending_message">
      <p>&copy; Sunny Kumar. Built using <a href="http://getpelican.com" target="_blank">Pelican</a>. Theme by Sunny Kumar on <a href="https://github.com/sunnykrgupta/pelican-svbhack-responsive" target="_blank">github</a>. Member of the <a href="http://internetdefenseleague.org">Internet Defense League</a>.</p>
    </div>
  </main>

  <div id="right-feed">
    <a class="twitter-timeline" href="" data-widget-id="605800181926199296">Tweets by </a>
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+"://platform.twitter.com/widgets.js";fjs.parentNode.insertBefore(js,fjs);}}(document,"script","twitter-wjs");</script>
  </div>
<script type="text/javascript">
  window._idl = {};
  _idl.variant = "banner";
  (function() {
    var idl = document.createElement('script');
    idl.type = 'text/javascript';
    idl.async = true;
    idl.src = ('https:' == document.location.protocol ? 'https://' : 'http://') + 'members.internetdefenseleague.org/include/?url=' + (_idl.url || '') + '&campaign=' + (_idl.campaign || '') + '&variant=' + (_idl.variant || 'banner');
    document.getElementsByTagName('body')[0].appendChild(idl);
  })();
</script>
</body>
</html>