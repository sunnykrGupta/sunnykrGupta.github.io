<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Daemon Blog - Sunny Kr Gupta</title><link href="https://sunnykrGupta.github.io/" rel="alternate"></link><link href="https://sunnykrGupta.github.io/feeds/sunny-kr-gupta.atom.xml" rel="self"></link><id>https://sunnykrGupta.github.io/</id><updated>2018-12-25T02:18:31+05:30</updated><entry><title>Kubernetes HPA Autoscaling with Kafka metrics</title><link href="https://sunnykrGupta.github.io/kubernetes-hpa-autoscaling-with-kafka-metrics.html" rel="alternate"></link><published>2018-12-25T02:18:31+05:30</published><updated>2018-12-25T02:18:31+05:30</updated><author><name>Sunny Kr Gupta</name></author><id>tag:sunnykrgupta.github.io,2018-12-25:/kubernetes-hpa-autoscaling-with-kafka-metrics.html</id><summary type="html">&lt;p&gt;&lt;img alt="K8s Autoscaling with Stackdriver and Kafka" src="/images/k8s-hpa-kafka/k8s-sd-kf.svg" height="400px"&gt;&lt;/p&gt;
&lt;p&gt;Autoscaling is natively supported on Kubernetes. Since 1.7 release, Kubernetes added a feature to scale your workload based on custom metrics. Prior release only supported scaling your apps based on CPU and memory.&lt;/p&gt;
&lt;p&gt;Kubernetes 1.7 introduced "Aggregator Layer" which allows Kubernetes to be extended with additional APIs, beyond …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;img alt="K8s Autoscaling with Stackdriver and Kafka" src="/images/k8s-hpa-kafka/k8s-sd-kf.svg" height="400px"&gt;&lt;/p&gt;
&lt;p&gt;Autoscaling is natively supported on Kubernetes. Since 1.7 release, Kubernetes added a feature to scale your workload based on custom metrics. Prior release only supported scaling your apps based on CPU and memory.&lt;/p&gt;
&lt;p&gt;Kubernetes 1.7 introduced "Aggregator Layer" which allows Kubernetes to be extended with additional APIs, beyond what is offered by the core Kubernetes APIs. This gives you the power to enable your own custom APIs.&lt;/p&gt;
&lt;p&gt;In this guide, we are going to understand the basics of HPAs, custom, external metrics APIs working and scaling workloads based on external metrics scraped from Kafka Cluster.&lt;/p&gt;
&lt;p&gt;The following are the steps you will complete in this guide:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Step 1: Enable cluster monitoring for Stackdriver.&lt;/li&gt;
&lt;li&gt;Step 2: Deploy a custom API server and register it to the aggregator layer.&lt;/li&gt;
&lt;li&gt;Step 3: Deploy metrics exporter and write to Stackdriver.&lt;/li&gt;
&lt;li&gt;Step 4: Deploy a sample application written in Golang to test the autoscaling.&lt;/li&gt;
&lt;li&gt;Step 5: Write a custom metrics based HPA to scale application.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We need to cover some concepts which are good to know before we move forward to the demonstration.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;How Kubernetes HPA works?&lt;/h3&gt;
&lt;p&gt;The Horizontal Pod Autoscaler is implemented as a Kubernetes API resource and a controller. The resource determines the behaviour of the controller. The controller periodically adjusts the number of replicas in a replication controller or deployment.&lt;/p&gt;
&lt;p&gt;The HorizontalPodAutoscaler normally fetches metrics from a series of aggregated APIs (&lt;code&gt;metrics.k8s.io&lt;/code&gt;, &lt;code&gt;custom.metrics.k8s.io&lt;/code&gt; and &lt;code&gt;external.metrics.k8s.io&lt;/code&gt;). The &lt;code&gt;metrics.k8s.io&lt;/code&gt; API is usually provided by &lt;strong&gt;metrics-server&lt;/strong&gt;, which needs to be launched separately. The HorizontalPodAutoscaler can also fetch metrics directly from Heapster.&lt;/p&gt;
&lt;p&gt;Here, in this guide, we will deploy our HPA reading from &lt;code&gt;external.metrics.k8s.io&lt;/code&gt; as our kafka metrics will be exposed to that API.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;What is custom metrics?&lt;/h3&gt;
&lt;p&gt;Kubernetes allows us to deploy your own metrics solutions. By default, &lt;strong&gt;metrics-server&lt;/strong&gt; and &lt;strong&gt;heapster&lt;/strong&gt; act as core metrics backend.&lt;/p&gt;
&lt;p&gt;Kubernetes has extended the support to allow custom APIs to expose other metrics provider. Few adapters are written by the third party to implement custom APIs which can be used to expose these metrics to Kubernetes resources such like HPA.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Current Implementations: &lt;a href="https://github.com/Kubernetes/metrics/blob/master/IMPLEMENTATIONS.md"&gt;github.com/Kubernetes/IMPLEMENTATIONS.md&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;How custom API server and HPA ties together ?&lt;/h3&gt;
&lt;p&gt;The custom API server that we deploy registers an API to Kubernetes and allows the HPA controller query custom metrics from that. API server that we are going to deploy here is Stackdriver adapter which can collect metrics from Stackdriver and send them to the HPA controller via REST queries.&lt;/p&gt;
&lt;p&gt;Our custom API server will register two APIs to Kubernetes : &lt;code&gt;custom.metrics.k8s.io&lt;/code&gt; and &lt;code&gt;external.metrics.k8s.io&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We will be also deploy an application to write metrics (in this case kafka metrics) to google stackdriver. The kind of metrics which we are going to write to Stackdriver will be exposed under &lt;code&gt;external.metrics.k8s.io&lt;/code&gt; instead of custom.metrics.k8s.io.&lt;/p&gt;
&lt;h3&gt;Prerequisites&lt;/h3&gt;
&lt;p&gt;Ensure the following dependencies are already fulfilled:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;You have a Docker running. You know the rules of this game. ;)&lt;/li&gt;
&lt;li&gt;You have a Kubernetes cluster (GKE) running on GCP.&lt;/li&gt;
&lt;li&gt;You have kubectl CLI installed and configured to your GKE cluster.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Let's begin the demonstration.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;1. Enable cluster monitoring for Stackdriver&lt;/h3&gt;
&lt;p&gt;GCP helper docs : &lt;a href="https://cloud.google.com/kubernetes-engine/docs/how-to/monitoring#enabling_monitoring_for_an_existing_cluster"&gt;https://cloud.google.com/Kubernetes-engine/docs/how-to/monitoring&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Monitoring scope should be enabled on cluster nodes. It is enabled by default, so you need not do anything. If you have an older version, upgrade it to the latest version and then update your node version as well. The scope will enable write permission to stackdriver which is important for writing metrics.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ gcloud container clusters list
NAME                 LOCATION       MASTER_VERSION  MASTER_IP       MACHINE_TYPE   NODE_VERSION   NUM_NODES  STATUS
my-kube-cluster  us-central1-b  &lt;span class="m"&gt;1&lt;/span&gt;.10.7-gke.11   xxx.xxx.xxx.xxx  n1-standard-1    &lt;span class="m"&gt;1&lt;/span&gt;.10.7-gke.11    &lt;span class="m"&gt;2&lt;/span&gt;          RUNNING


$ gcloud container clusters describe my-kube-cluster
............
..&amp;lt;output&amp;gt;..
............
oauthScopes:
    - https://www.googleapis.com/auth/compute
    - https://www.googleapis.com/auth/devstorage.read_only
    - https://www.googleapis.com/auth/service.management
    - https://www.googleapis.com/auth/servicecontrol
    - https://www.googleapis.com/auth/logging.write
    - https://www.googleapis.com/auth/monitoring   &lt;span class="c1"&gt;# enabled&lt;/span&gt;
............
..&amp;lt;output&amp;gt;..
............
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;2. Deploy a custom API server and register it to the aggregator layer.&lt;/h3&gt;
&lt;p&gt;Before we deploy our custom server, let's see what APIs are available in our k8s.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;## Type commands to see existing available APIs
$ kubectl api-versions

admissionregistration.k8s.io/v1beta1
apiextensions.k8s.io/v1beta1
apiregistration.k8s.io/v1
apiregistration.k8s.io/v1beta1
............
..&amp;lt;other-APIs&amp;gt;..
............
metrics.k8s.io/v1beta1    # this is our core metrics APIs
............
..&amp;lt;other-APIs&amp;gt;..
............
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Here, you can see we have &lt;code&gt;metrics.k8s.io&lt;/code&gt; but we need &lt;code&gt;external.metrics.k8s.io&lt;/code&gt; to expose our stackdriver metrics and allow HPA to read it through.&lt;/p&gt;
&lt;p&gt;We are going to use custom metrics stackdriver adapter to register our APIs.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;# Use one of google user account to create a cluster role
$ kubectl create clusterrolebinding cluster-admin-binding \
    --clusterrole cluster-admin --user &amp;quot;$(gcloud config get-value account)&amp;quot;

# We will deploy new resource model based APIs

$ kubectl apply -f https://raw.githubusercontent.com/GoogleCloudPlatform/k8s-stackdriver/master/custom-metrics-stackdriver-adapter/deploy/production/adapter_new_resource_model.yaml

namespace &amp;quot;custom-metrics&amp;quot; created
serviceaccount &amp;quot;custom-metrics-stackdriver-adapter&amp;quot; created
clusterrolebinding.rbac.authorization.k8s.io &amp;quot;custom-metrics:system:auth-delegator&amp;quot; created
rolebinding.rbac.authorization.k8s.io &amp;quot;custom-metrics-auth-reader&amp;quot; created
clusterrolebinding.rbac.authorization.k8s.io &amp;quot;custom-metrics-resource-reader&amp;quot; created
deployment.extensions &amp;quot;custom-metrics-stackdriver-adapter&amp;quot; created
service &amp;quot;custom-metrics-stackdriver-adapter&amp;quot; created
apiservice.apiregistration.k8s.io &amp;quot;v1beta1.custom.metrics.k8s.io&amp;quot; created
apiservice.apiregistration.k8s.io &amp;quot;v1beta1.external.metrics.k8s.io&amp;quot; created
clusterrole.rbac.authorization.k8s.io &amp;quot;external-metrics-reader&amp;quot; created
clusterrolebinding.rbac.authorization.k8s.io &amp;quot;external-metrics-reader&amp;quot; created
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now, we have deployed our custom server and registered APIs to Aggregator Layer.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;# Custom Metrics APIs verification

$  kubectl get all -n custom-metrics
NAME                                                 READY     STATUS    RESTARTS   AGE
custom-metrics-stackdriver-adapter-6c9bd9679-m2cnh   1/1       Running   0          2m

# Type again to get available APIs
$ kubectl api-versions
............
..&amp;lt;other-APIs&amp;gt;..
............
metrics.k8s.io/v1beta1
.....
custom.metrics.k8s.io/v1beta1   # for custom Kubernetes metrics
external.metrics.k8s.io/v1beta1 # for external metrics
............
..&amp;lt;other-APIs&amp;gt;..
............
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now, query our new APIs which just came online.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ kubectl get --raw &lt;span class="s2"&gt;&amp;quot;/apis/external.metrics.k8s.io/v1beta1&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt; jq
$ kubectl get --raw &lt;span class="s2"&gt;&amp;quot;/apis/custom.metrics.k8s.io/v1beta1&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt; jq
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now you can see, both APIs are available are online with a lot of custom metrics are available for query. Next, we will be writing kafka metrics to stackdriver and these APIs will help us read from it.&lt;/p&gt;
&lt;p&gt;Resource in details : &lt;a href="https://github.com/GoogleCloudPlatform/k8s-stackdriver/tree/master/custom-metrics-stackdriver-adapter"&gt;k8s-stackdriver/custom-metrics-stackdriver-adapter&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;3. Deploy metrics exporter and write to stackdriver.&lt;/h3&gt;
&lt;p&gt;We are using &lt;a href="https://github.com/danielqsj/kafka_exporter"&gt;Kafka exporter&lt;/a&gt; which will read my kafka cluster and expose metrics in &lt;a href="https://prometheus.io/docs/instrumenting/exposition_formats/"&gt;prometheus format&lt;/a&gt; and we combine our deployment with this &lt;a href="https://github.com/GoogleCloudPlatform/k8s-stackdriver/tree/master/prometheus-to-sd"&gt;k8s-stackdriver/prometheus-to-sd&lt;/a&gt; sidecar container. Kafka-exporter will read the Kafka cluster and expose the metrics on particular web-URL and our sidecar container will read those and write to stackdriver. Easy enough, let's do it.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c"&gt;# prometheus-to-sd-custom-metrics-kafka-exporter.yaml&lt;/span&gt;
&lt;span class="nf"&gt;apiVersion&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;extensions&lt;/span&gt;/&lt;span class="n"&gt;v&lt;/span&gt;1&lt;span class="n"&gt;beta&lt;/span&gt;1
&lt;span class="nf"&gt;kind&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Deployment&lt;/span&gt;
&lt;span class="nf"&gt;metadata&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
  name: custom-metrics-kafka-exporter
&lt;span class="nf"&gt;spec&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
  replicas: &lt;span class="m"&gt;1&lt;/span&gt;
  selector:
    matchLabels:
      custom: metrics
  template:
    metadata:
      labels:
        custom: metrics
    spec:
      hostNetwork: &lt;span class="nb"&gt;true&lt;/span&gt;
      containers:
      - name: kafka-exporter
        image: danielqsj/kafka-exporter
        command:
        - kafka_exporter
        - &lt;span class="s2"&gt;&amp;quot;--kafka.server=my-kafka-broker-1:9092&amp;quot;&lt;/span&gt;
        - &lt;span class="s2"&gt;&amp;quot;--kafka.server=my-kafka-broker-2:9092&amp;quot;&lt;/span&gt;
        ports:
          - name: http-metrics
            containerPort: &lt;span class="m"&gt;9308&lt;/span&gt;
        readinessProbe:
          httpGet:
            path: /
            port: &lt;span class="m"&gt;9308&lt;/span&gt;
          initialDelaySeconds: &lt;span class="m"&gt;5&lt;/span&gt;
          timeoutSeconds: &lt;span class="m"&gt;5&lt;/span&gt;
      - name: prometheus-to-sd
        image: gcr.io/google-containers/prometheus-to-sd:v0.3.2
        ports:
          - name: profiler
            containerPort: &lt;span class="m"&gt;6060&lt;/span&gt;
        command:
          - /monitor
          - --stackdriver-prefix&lt;span class="o"&gt;=&lt;/span&gt;custom.googleapis.com
          - --source&lt;span class="o"&gt;=&lt;/span&gt;kafka-exporter:http://localhost:9308?whitelisted&lt;span class="o"&gt;=&lt;/span&gt;kafka_brokers,kafka_topic_partitions,kafka_consumergroup_current_offset_sum,kafka_consumergroup_lag_sum
          - --pod-id&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;POD_NAME&lt;span class="k"&gt;)&lt;/span&gt;
          - --namespace-id&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;POD_NAMESPACE&lt;span class="k"&gt;)&lt;/span&gt;
        env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Deploy this to ship your metrics to stackdriver.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;kubectl apply -f prometheus-to-sd-custom-metrics-kafka-exporter.yaml
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Next step is exploring stackdriver and filter your external metrics. Available metrics : &lt;a href="https://github.com/danielqsj/kafka_exporter#metrics"&gt;https://github.com/danielqsj/kafka_exporter#metrics&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;For example search : kafka_brokers&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img alt="metrics-explorer" src="/images/k8s-hpa-kafka/metrics-explorer.png"&gt;
&lt;div style="text-align: right"&gt;&lt;sub&gt;Stackdriver metrics explorer&lt;/sub&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;You can also query registered APIs to read these external values.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ kubectl get --raw &lt;span class="s2"&gt;&amp;quot;/apis/external.metrics.k8s.io/v1beta1/namespaces/default/custom.googleapis.com|kafka-exporter|kafka_brokers&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt; jq

&lt;span class="o"&gt;{&lt;/span&gt;
  &lt;span class="s2"&gt;&amp;quot;kind&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;ExternalMetricValueList&amp;quot;&lt;/span&gt;,
  &lt;span class="s2"&gt;&amp;quot;apiVersion&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;external.metrics.k8s.io/v1beta1&amp;quot;&lt;/span&gt;,
  &lt;span class="s2"&gt;&amp;quot;metadata&amp;quot;&lt;/span&gt;: &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;selfLink&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;/apis/external.metrics.k8s.io/v1beta1/namespaces/default/custom.googleapis.com%7Ckafka-exporter%7Ckafka_brokers&amp;quot;&lt;/span&gt;
  &lt;span class="o"&gt;}&lt;/span&gt;,
  &lt;span class="s2"&gt;&amp;quot;items&amp;quot;&lt;/span&gt;: &lt;span class="o"&gt;[&lt;/span&gt;
    &lt;span class="o"&gt;{&lt;/span&gt;
      &lt;span class="s2"&gt;&amp;quot;metricName&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;custom.googleapis.com|kafka-exporter|kafka_brokers&amp;quot;&lt;/span&gt;,
      &lt;span class="s2"&gt;&amp;quot;metricLabels&amp;quot;&lt;/span&gt;: &lt;span class="o"&gt;{&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;resource.labels.cluster_name&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;my-kube-cluster&amp;quot;&lt;/span&gt;,
        &lt;span class="s2"&gt;&amp;quot;resource.labels.container_name&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;&amp;quot;&lt;/span&gt;,
        &lt;span class="s2"&gt;&amp;quot;resource.labels.instance_id&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;gke-my-kube-cluster-69201eb2-dvdg&amp;quot;&lt;/span&gt;,
        &lt;span class="s2"&gt;&amp;quot;resource.labels.namespace_id&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;default&amp;quot;&lt;/span&gt;,
        &lt;span class="s2"&gt;&amp;quot;resource.labels.pod_id&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;custom-metrics-kafka-exporter-56764bbbc9-p5xqb&amp;quot;&lt;/span&gt;,
        &lt;span class="s2"&gt;&amp;quot;resource.labels.zone&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;us-central1-b&amp;quot;&lt;/span&gt;,
        &lt;span class="s2"&gt;&amp;quot;resource.type&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;gke_container&amp;quot;&lt;/span&gt;
      &lt;span class="o"&gt;}&lt;/span&gt;,
      &lt;span class="s2"&gt;&amp;quot;timestamp&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;2018-12-25T12:56:46Z&amp;quot;&lt;/span&gt;,
      &lt;span class="s2"&gt;&amp;quot;value&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;2&amp;quot;&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;
  &lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;hr&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;4. Deploy a sample application written in golang to test autoscaling.&lt;/h3&gt;
&lt;p&gt;Till now, we went through all the sophisticated pieces of stuff which will help to expose our metrics to HPA. From now on, we will run some basic ops that we usually do in Kubernetes.&lt;/p&gt;
&lt;p&gt;To simulate the autoscaling, I have deployed a sample application written in golang which will act as Kafka client ( producer and consumer ) for Kafka topics.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Code for reference : &lt;a href="https://github.com/sunnykrGupta/k8s-hpa-custom-autoscaling-kafka-metrics/tree/master/go-kafka"&gt;k8s-hpa-custom-autoscaling-kafka-metrics/go-kafka&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ kubectl get deploy -llang&lt;span class="o"&gt;=&lt;/span&gt;golang
NAME                       DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
consumer-kafka-go-client   &lt;span class="m"&gt;1&lt;/span&gt;         &lt;span class="m"&gt;1&lt;/span&gt;         &lt;span class="m"&gt;1&lt;/span&gt;            &lt;span class="m"&gt;1&lt;/span&gt;           25m
producer-kafka-go-client   &lt;span class="m"&gt;1&lt;/span&gt;         &lt;span class="m"&gt;1&lt;/span&gt;         &lt;span class="m"&gt;1&lt;/span&gt;            &lt;span class="m"&gt;1&lt;/span&gt;           30m

&lt;span class="c1"&gt;# scaling the producer app to build a consumer lag on kafka topic&lt;/span&gt;
$ kubectl scale --replicas&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt; deployment/producer-kafka-go-client
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;I have scaled my producer so that it will push enough messages to build lag for my consumer clients and we will test our HPA on consumer deployment.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;5. Write a custom metrics based HPA to scale application.&lt;/h3&gt;
&lt;p&gt;Now that I have scaled my Kafka producer, there should be a consumer lag build up in the Kafka topic. We have prepared HPA to read from external metrics coming from stackdriver through our newly registered APIs. All the hard work above is for this moment.&lt;/p&gt;
&lt;p&gt;Below is my HPA manifest file, which is going to help scale our Kafka consumer :&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;# kafka-custom-metrics-hpa.yaml
apiVersion: autoscaling/v2beta1
kind: HorizontalPodAutoscaler
metadata:
  name: consumer-kafka-go-client
spec:
  minReplicas: 1
  maxReplicas: 5
  metrics:
  - type: External
    external:
      # which metrics to read from stackdriver
      metricName: custom.googleapis.com|kafka-exporter|kafka_consumergroup_lag_sum
      metricSelector:
        matchLabels:
          # define labels to target
          metric.labels.consumergroup: golang-consumer
      # scale +1 whenever it crosses multiples of mentioned value
      targetAverageValue: &amp;quot;1000&amp;quot;
  # define deployment to control
  scaleTargetRef:
    apiVersion: extensions/v1beta1
    kind: Deployment
    name: consumer-kafka-go-client
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ kubectl apply -f kafka-custom-metrics-hpa.yaml
horizontalpodautoscaler.autoscaling &lt;span class="s2"&gt;&amp;quot;consumer-kafka-go-client&amp;quot;&lt;/span&gt; configured
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Let's query our kafka_consumer_lag to see what are the current stats.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt; kubectl get --raw &amp;quot;/apis/external.metrics.k8s.io/v1beta1/namespaces/default/custom.googleapis.com|kafka-exporter|kafka_consumergroup_lag_sum&amp;quot; | jq

{
  &amp;quot;kind&amp;quot;: &amp;quot;ExternalMetricValueList&amp;quot;,
  &amp;quot;apiVersion&amp;quot;: &amp;quot;external.metrics.k8s.io/v1beta1&amp;quot;,
  &amp;quot;metadata&amp;quot;: {
    &amp;quot;selfLink&amp;quot;: &amp;quot;/apis/external.metrics.k8s.io/v1beta1/namespaces/default/custom.googleapis.com%7Ckafka-exporter%7Ckafka_consumergroup_lag_sum&amp;quot;
  },
  &amp;quot;items&amp;quot;: [
    {
      &amp;quot;metricName&amp;quot;: &amp;quot;custom.googleapis.com|kafka-exporter|kafka_consumergroup_lag_sum&amp;quot;,
      &amp;quot;metricLabels&amp;quot;: {
        &amp;quot;metric.labels.consumergroup&amp;quot;: &amp;quot;golang-consumer&amp;quot;,
        &amp;quot;metric.labels.topic&amp;quot;: &amp;quot;custom-topic&amp;quot;,
        &amp;quot;resource.labels.container_name&amp;quot;: &amp;quot;&amp;quot;,
        &amp;quot;resource.labels.namespace_id&amp;quot;: &amp;quot;default&amp;quot;,
        &amp;quot;resource.labels.pod_id&amp;quot;: &amp;quot;custom-metrics-kafka-exporter-547c7f4d5c-tqtw8&amp;quot;,
        &amp;quot;resource.labels.zone&amp;quot;: &amp;quot;us-central1-b&amp;quot;,
        &amp;quot;resource.type&amp;quot;: &amp;quot;gke_container&amp;quot;
      },
      &amp;quot;timestamp&amp;quot;: &amp;quot;2018-12-25T18:46:54Z&amp;quot;,
      &amp;quot;value&amp;quot;: &amp;quot;732&amp;quot;
    }
  ]
}
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;After polling some stats for a while, I saw auto scale trigger after some time. Yey!! In this case, it scaled the app to 4 replicas based on the numbers found on stackdriver.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ kubectl get hpa
NAME                       REFERENCE                             TARGETS        MINPODS   MAXPODS   REPLICAS   AGE
consumer-kafka-go-client   Deployment/consumer-kafka-go-client   &lt;span class="m"&gt;732&lt;/span&gt;/1k &lt;span class="o"&gt;(&lt;/span&gt;avg&lt;span class="o"&gt;)&lt;/span&gt;   &lt;span class="m"&gt;1&lt;/span&gt;         &lt;span class="m"&gt;5&lt;/span&gt;         &lt;span class="m"&gt;1&lt;/span&gt;          4m

$ kubectl get hpa
NAME                       REFERENCE                             TARGETS         MINPODS   MAXPODS   REPLICAS   AGE
consumer-kafka-go-client   Deployment/consumer-kafka-go-client   &lt;span class="m"&gt;4032&lt;/span&gt;/1k &lt;span class="o"&gt;(&lt;/span&gt;avg&lt;span class="o"&gt;)&lt;/span&gt;   &lt;span class="m"&gt;1&lt;/span&gt;         &lt;span class="m"&gt;5&lt;/span&gt;         &lt;span class="m"&gt;1&lt;/span&gt;          12m

$ kubectl get hpa
NAME                       REFERENCE                             TARGETS         MINPODS   MAXPODS   REPLICAS   AGE
consumer-kafka-go-client   Deployment/consumer-kafka-go-client   &lt;span class="m"&gt;1008&lt;/span&gt;/1k &lt;span class="o"&gt;(&lt;/span&gt;avg&lt;span class="o"&gt;)&lt;/span&gt;   &lt;span class="m"&gt;1&lt;/span&gt;         &lt;span class="m"&gt;5&lt;/span&gt;         &lt;span class="m"&gt;4&lt;/span&gt;          13m
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="consumer-lag-graph" src="/images/k8s-hpa-kafka/consumer-lag.png"&gt;
&lt;div style="text-align: right"&gt;&lt;sub&gt;Kafka consumer lags metrics&lt;/sub&gt;&lt;/div&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Custom autoscaling is a very useful feature when you wanted to scale your workloads based on the complexity of your works deployed in your production. It could be disk size, networking, Loadbalancers connections etc.&lt;/p&gt;
&lt;p&gt;Hope you enjoyed this guide. Give thumbs up and ask questions in the comments.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Github : &lt;a href="https://github.com/sunnykrGupta/k8s-hpa-custom-autoscaling-kafka-metrics"&gt;https://github.com/sunnykrGupta/k8s-hpa-custom-autoscaling-kafka-metrics&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h4&gt;Resources :&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/GoogleCloudPlatform/k8s-stackdriver/tree/master/prometheus-to-sd"&gt;prometheus-to-sd&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/danielqsj/kafka_exporter"&gt;kafka_exporter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://Kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale"&gt;run-application/horizontal-pod-autoscale&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/GoogleCloudPlatform/k8s-stackdriver/tree/master/custom-metrics-stackdriver-adapter"&gt;custom-metrics-stackdriver-adapter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://cloud.google.com/kubernetes-engine/docs/tutorials/external-metrics-autoscaling"&gt;https://cloud.google.com/Kubernetes-engine/docs/tutorials/external-metrics-autoscaling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://Kubernetes.io/docs/concepts/extend-Kubernetes/api-extension/apiserver-aggregation/"&gt;https://Kubernetes.io/docs/concepts/extend-Kubernetes/api-extension/apiserver-aggregation/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://Kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/#autoscaling-on-metrics-not-related-to-Kubernetes-objects"&gt;autoscaling-on-metrics-not-related-to-Kubernetes-objects&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;hr&gt;</content><category term="Kubernetes"></category><category term="Kafka"></category><category term="Autoscaling"></category><category term="Stackdriver"></category></entry><entry><title>Sharded Mongodb in Kubernetes StatefulSets on GKE</title><link href="https://sunnykrGupta.github.io/sharded-mongodb-in-kubernetes-statefulsets-on-gke.html" rel="alternate"></link><published>2017-12-31T20:23:31+05:30</published><updated>2017-12-31T20:23:31+05:30</updated><author><name>Sunny Kr Gupta</name></author><id>tag:sunnykrgupta.github.io,2017-12-31:/sharded-mongodb-in-kubernetes-statefulsets-on-gke.html</id><summary type="html">&lt;p&gt;&lt;img alt="MongoDB in K8s" src="/images/kubes/k8s-mongodb.png"&gt;&lt;/p&gt;
&lt;p&gt;This blog is going to demonstrate the setup of Sharded MongoDB Cluster on Google Kubernetes Engine. We will use kubernetes StatefulSets feature to deploy mongodb containers.&lt;/p&gt;
&lt;p&gt;We need to cover some concepts before we move on to demonstration.&lt;/p&gt;
&lt;h3&gt;&lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset"&gt;StatefulSets&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;A StatefulSets is like a Deployment which manages Pods and guarantees …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;img alt="MongoDB in K8s" src="/images/kubes/k8s-mongodb.png"&gt;&lt;/p&gt;
&lt;p&gt;This blog is going to demonstrate the setup of Sharded MongoDB Cluster on Google Kubernetes Engine. We will use kubernetes StatefulSets feature to deploy mongodb containers.&lt;/p&gt;
&lt;p&gt;We need to cover some concepts before we move on to demonstration.&lt;/p&gt;
&lt;h3&gt;&lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset"&gt;StatefulSets&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;A StatefulSets is like a Deployment which manages Pods and guarantees about the ordering and uniqueness of these Pods.
It maintains a sticky identity for each of their Pods. It helps in deployment of application that needs persistency, unique network identifiers (DNS, Hostnames etc) and are meants for stateful application. If a pod gets terminated or deleted, a volume data will still remain intact if managed by persistentvolumes.  &lt;/p&gt;
&lt;h3&gt;&lt;a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#gce"&gt;StorageClass&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;StorageClass helps in administration to describe the “classes” of storage offered by Kubernetes. Each StorageClass has different provisioner (GCEPersistentDisk, AWSElasticBlockStore, AzureDisk etc) that determines what volume plugin is used for provisioning storage.&lt;/p&gt;
&lt;h3&gt;&lt;a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/"&gt;PersistentVolume&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;A PersistentVolume (PV) is a piece of storage in the cluster that has been provisioned by an administrator. PVs are resources available to be used by any Pod. Any Pod can claim these volumes by mean of PersistentVolumeClaims (PVC) and released eventually when claim is deleted.&lt;/p&gt;
&lt;h3&gt;&lt;a href="https://kubernetes.io/docs/concepts/services-networking/service/#headless-services"&gt;Headless Services&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Headless Services are used to configure DNS of pods having same selectors defined by services. It is not generally used for load-balancing purpose. Each headless services configured with label selectors helps in defining unique network identifiers for pods running in statefulset.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Lets begins the demonstration. Please switch to your terminal and follow the instructions.  &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Note : This setup is compatible with &amp;lt;= mongo 3.2.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;1. Prerequisites&lt;/h3&gt;
&lt;p&gt;Ensure the following dependencies are already fulfilled on your host Linux system:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;GCP’s cloud client command line tool &lt;a href="https://cloud.google.com/sdk/docs/quickstarts"&gt;gcloud&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;gcloud authentication to a project to manage container engine.&lt;/li&gt;
&lt;li&gt;Install the Kubernetes command tool (“kubectl”),&lt;/li&gt;
&lt;li&gt;Configure kubernetes authentication credentials.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;2. Create namespace, storageclass, Google compute Disk and persistentvolumes.&lt;/h3&gt;
&lt;p&gt;Our Mongodb Setup will be as follows :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;1x Config Server  (k8s deployment type: "StatefulSet")&lt;/li&gt;
&lt;li&gt;2x Shards with each Shard being a Replica Set containing 1x replicas (k8s deployment type: "StatefulSet")&lt;/li&gt;
&lt;li&gt;2x Mongos Routers (k8s deployment type: "Deployment")&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We will create a kubernetes namespace and will deploy all our above resources in our defined namespaces. We will define disk that will be used by our statefulset containers. Disk will be mounted on pods running our mongodb server by means of APIs defined in StorageClass and PersistentVolume.&lt;/p&gt;
&lt;hr&gt;
&lt;h4&gt;2.1 Create Namespace&lt;/h4&gt;
&lt;p&gt;Create a file as &lt;code&gt;namespace.yaml&lt;/code&gt; and replace &lt;code&gt;NAMESPACE_ID&lt;/code&gt; with your handle name or any other name. I will create a namespace with &lt;code&gt;daemonsl&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;#namespace.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: NAMESPACE_ID
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;#To apply resources to kubernetes, run
sed -e &amp;quot;s/NAMESPACE_ID/daemonsl/g&amp;quot; namespace.yaml &amp;gt; tmp-namespace.yaml
kubectl apply -f tmp-namespace.yaml

#To verify namespaces
kubectl get ns
&lt;/pre&gt;&lt;/div&gt;


&lt;hr&gt;
&lt;h4&gt;2.2 Create StorageClass&lt;/h4&gt;
&lt;p&gt;Create a file as &lt;code&gt;gce-ssd-storageclass.yaml&lt;/code&gt;. We are defining our storageclass name as &lt;strong&gt;&lt;code&gt;fast&lt;/code&gt;&lt;/strong&gt; and using GCE persistent disk as our provisioner with &lt;code&gt;type: pd-ssd&lt;/code&gt; to allow SSD disk type allocation to requester (ie, statefulset container here).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;#gce-ssd-storageclass.yaml
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: fast
provisioner: kubernetes.io/gce-pd
parameters:
  type: pd-ssd
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;#To apply resources to kubernetes, run
kubectl apply -f gce-ssd-storageclass.yaml

#To verify storageclass
kubectl get sc
&lt;/pre&gt;&lt;/div&gt;


&lt;hr&gt;
&lt;h4&gt;2.3 Create GCE SSD Disks&lt;/h4&gt;
&lt;p&gt;We will create some disk to be used by mongodb statefulset container. We are ordering two &lt;code&gt;10GB&lt;/code&gt; disk and one &lt;code&gt;5GB&lt;/code&gt; disk of type &lt;code&gt;SSD&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;#For MainDB servers
gcloud compute disks create --size 10GB --type pd-ssd pd-ssd-disk-k8s-mongodb-daemonsl-10g-1
gcloud compute disks create --size 10GB --type pd-ssd pd-ssd-disk-k8s-mongodb-daemonsl-10g-2

#For Config servers
gcloud compute disks create --size 5GB --type pd-ssd pd-ssd-disk-k8s-mongodb-daemonsl-5g-1
&lt;/pre&gt;&lt;/div&gt;


&lt;hr&gt;
&lt;h4&gt;2.4 Create PersistentVolume&lt;/h4&gt;
&lt;p&gt;Create a file as &lt;code&gt;ext4-gce-ssd-persistentvolume.yaml&lt;/code&gt;. We are defining our PersistentVolume storage capacity &lt;code&gt;10GB&lt;/code&gt; to be bounded by &lt;code&gt;maindb&lt;/code&gt; pod and &lt;code&gt;5GB&lt;/code&gt; to be bounded by &lt;code&gt;configdb&lt;/code&gt; pod.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;#ext4-gce-ssd-persistentvolume.yaml
apiVersion: &amp;quot;v1&amp;quot;
kind: &amp;quot;PersistentVolume&amp;quot;
metadata:
  name: data-volume-k8s-mongodb-daemonsl-SIZEg-INSTANCE
spec:
  capacity:
      storage: SIZEGi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: fast
  gcePersistentDisk:
    fsType: ext4
    pdName: pd-ssd-disk-k8s-mongodb-daemonsl-SIZEg-INSTANCE
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Use above template, modify and apply in following order :&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;#Replace &amp;#39;SIZE&amp;#39; with 10 and &amp;#39;INSTANCE&amp;#39; with 1,
# Ex: data-volume-k8s-mongodb-daemonsl-10g-1, storage: 10Gi,
sed -e &amp;quot;s/INSTANCE/1/g; s/SIZE/10/g&amp;quot; ext4-gce-ssd-persistentvolume.yaml &amp;gt; tmp-ext4-gce-ssd-persistentvolume.yaml
kubectl apply -f tmp-ext4-gce-ssd-persistentvolume.yaml

#Replace &amp;#39;SIZE&amp;#39; with 10 and &amp;#39;INSTANCE&amp;#39; with 2
sed -e &amp;quot;s/INSTANCE/2/g; s/SIZE/10/g&amp;quot; ext4-gce-ssd-persistentvolume.yaml &amp;gt; tmp-ext4-gce-ssd-persistentvolume.yaml
kubectl apply -f tmp-ext4-gce-ssd-persistentvolume.yaml

#Replace &amp;#39;SIZE&amp;#39; with 5 and &amp;#39;INSTANCE&amp;#39; with 1
sed -e &amp;quot;s/INSTANCE/1/g; s/SIZE/5/g&amp;quot; ext4-gce-ssd-persistentvolume.yaml &amp;gt; tmp-ext4-gce-ssd-persistentvolume.yaml
kubectl apply -f tmp-ext4-gce-ssd-persistentvolume.yaml


#To verify PersistentVolume creation,
kubectl get pv
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now we have &lt;strong&gt;storageclass, namespace, disks&lt;/strong&gt; and &lt;strong&gt;persistentvolume&lt;/strong&gt; ready as resources for statefulset container.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;3. StatefulSet containers and Mongos Deployment.&lt;/h3&gt;
&lt;h4&gt;3.1 Statefulset ConfigDB&lt;/h4&gt;
&lt;p&gt;Create a file as &lt;code&gt;mongodb-configdb-service-stateful.yaml&lt;/code&gt; and copy the following template. Replace &lt;code&gt;NAMESPACE_ID&lt;/code&gt; with &lt;code&gt;daemonsl&lt;/code&gt;, or whatever name you have defined and &lt;code&gt;DB_DISK&lt;/code&gt; with &lt;code&gt;5Gi&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We have created a headless service with clusterIP &lt;code&gt;None&lt;/code&gt; with selector as &lt;code&gt;role: mongodb-configdb&lt;/code&gt; listening to port &lt;code&gt;27019&lt;/code&gt;. We have defined our statefulset definition with mongodb arguments and volumeClaimTemplates. Here, &lt;code&gt;VolumeClaimTemplates&lt;/code&gt; is requesting storageclass &lt;code&gt;fast&lt;/code&gt; with &lt;code&gt;storage capacity 5GB&lt;/code&gt;. This volumeClaimTemplates register this requests to storageclass and storageclass fulfill this requests by  &lt;code&gt;PersistentVolume (PV)&lt;/code&gt; and register claim in &lt;code&gt;PersistentVolumeClaims (PVC)&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;#mongodb-configdb-service-stateful.yaml
apiVersion: v1
kind: Service
metadata:
  name: mongodb-configdb-headless-service
  namespace: NAMESPACE_ID
  labels:
    name: mongodb-configdb
spec:
  ports:
  - port: 27019
    targetPort: 27019
  clusterIP: None
  selector:
    role: mongodb-configdb
---
apiVersion: apps/v1beta2  #change this version based on master version
kind: StatefulSet
metadata:
  name: mongodb-configdb
  namespace: NAMESPACE_ID
spec:
  selector:
    matchLabels:
      role: mongodb-configdb # has to match .spec.template.metadata.labels
  serviceName: mongodb-configdb-headless-service
  replicas: 1
  template:
    metadata:
      labels:
        role: mongodb-configdb
        tier: configdb
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: tier
                  operator: In
                  values:
                  - configdb
              topologyKey: kubernetes.io/hostname
      terminationGracePeriodSeconds: 10
      containers:
        - name: mongodb-configdb-container
          image: mongo
          command:
            - &amp;quot;mongod&amp;quot;
            - &amp;quot;--port&amp;quot;
            - &amp;quot;27019&amp;quot;
            - &amp;quot;--dbpath&amp;quot;
            - &amp;quot;/mongo-disk&amp;quot;
            - &amp;quot;--bind_ip&amp;quot;
            - &amp;quot;0.0.0.0&amp;quot;
            - &amp;quot;--configsvr&amp;quot;
          resources:
            requests:
              cpu: 50m
              memory: 100Mi
          ports:
            - containerPort: 27019
          volumeMounts:
            - name: mongodb-configdb-persistent-storage-claim
              mountPath: /mongo-disk
  volumeClaimTemplates:
  - metadata:
      name: mongodb-configdb-persistent-storage-claim
      annotations:
        volume.beta.kubernetes.io/storage-class: &amp;quot;fast&amp;quot;
    spec:
      accessModes: [ &amp;quot;ReadWriteOnce&amp;quot; ]
      resources:
        requests:
          storage: DB_DISK
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;sed -e &amp;quot;s/NAMESPACE_ID/daemonsl/g; s/DB_DISK/5Gi/g&amp;quot;  mongodb-configdb-service-stateful.yaml &amp;gt; tmp-mongodb-configdb-service-stateful.yaml
kubectl apply -f tmp-mongodb-configdb-service-stateful.yaml
&lt;/pre&gt;&lt;/div&gt;


&lt;hr&gt;
&lt;h4&gt;3.2 Statefulset mainDB&lt;/h4&gt;
&lt;p&gt;Create a file as &lt;code&gt;mongodb-maindb-service-stateful.yaml&lt;/code&gt; and copy the following template. Replace &lt;code&gt;NAMESPACE_ID&lt;/code&gt; with &lt;code&gt;daemonsl&lt;/code&gt;, or whatever name you have defined, &lt;code&gt;DB_DISK&lt;/code&gt; with &lt;code&gt;10Gi&lt;/code&gt; and &lt;code&gt;shardX&lt;/code&gt; &amp;amp; &lt;code&gt;ShardX&lt;/code&gt; to &lt;code&gt;1&lt;/code&gt; and then &lt;code&gt;2&lt;/code&gt; and apply template two times to create two different statefulsets configuration. After deploying in kubernetes, we will have two statefulsets running with name as &lt;code&gt;mongodb-shard1&lt;/code&gt; and &lt;code&gt;mongodb-shard2&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Here again, We have created headless service and &lt;code&gt;VolumeClaimTemplates&lt;/code&gt; which is requesting storageclass &lt;code&gt;fast&lt;/code&gt; with storage capacity 10GB.kubectl apply -f tmp-ext4-gce-ssd-persistentvolume.yaml&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;#mongodb-maindb-service-stateful.yaml
apiVersion: v1
kind: Service
metadata:
  name: mongodb-shardX-headless-service
  namespace: NAMESPACE_ID
  labels:
    name: mongodb-shardX
spec:
  ports:
  - port: 27017
    targetPort: 27017
  clusterIP: None
  selector:
    role: mongodb-shardX
---
apiVersion: apps/v1beta2
kind: StatefulSet
metadata:
  name: mongodb-shardX
  namespace: NAMESPACE_ID
spec:
  selector:
    matchLabels:
      role: mongodb-shardX # has to match .spec.template.metadata.labels
  serviceName: mongodb-shardX-headless-service
  replicas: 1
  template:
    metadata:
      labels:
        role: mongodb-shardX
        tier: maindb
        replicaset: ShardX
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: replicaset
                  operator: In
                  values:
                  - ShardX
              topologyKey: kubernetes.io/hostname
      terminationGracePeriodSeconds: 10
      containers:
        - name: mongodb-shardX-container
          image: mongo
          command:
            - &amp;quot;mongod&amp;quot;
            - &amp;quot;--port&amp;quot;
            - &amp;quot;27017&amp;quot;
            - &amp;quot;--bind_ip&amp;quot;
            - &amp;quot;0.0.0.0&amp;quot;
            - &amp;quot;--replSet&amp;quot;
            - &amp;quot;ShardX&amp;quot;
            - &amp;quot;--dbpath&amp;quot;
            - &amp;quot;/mongo-disk&amp;quot;
          resources:
            requests:
              cpu: 50m
              memory: 100Mi
          ports:
            - containerPort: 27017
          volumeMounts:
            - name: mongo-shardX-persistent-storage-claim
              mountPath: /mongo-disk
  volumeClaimTemplates:
  - metadata:
      name: mongo-shardX-persistent-storage-claim
      annotations:
        volume.beta.kubernetes.io/storage-class: &amp;quot;fast&amp;quot;
    spec:
      accessModes: [ &amp;quot;ReadWriteOnce&amp;quot; ]
      resources:
        requests:
          storage: DB_DISK
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;#replace &amp;#39;shardX&amp;#39; &amp;amp; &amp;#39;ShardX&amp;#39; with shard1 &amp;amp; Shard1. Mind case sensitivity.
sed -e &amp;quot;s/shardX/shard1/g; s/ShardX/Shard1/g; s/NAMESPACE_ID/daemonsl/g; s/DB_DISK/10Gi/g&amp;quot; mongodb-maindb-service-stateful.yaml &amp;gt; tmp-mongodb-maindb-service-stateful.yaml
kubectl apply -f tmp-mongodb-maindb-service-stateful.yaml

#replace &amp;#39;shardX&amp;#39; &amp;amp; &amp;#39;ShardX&amp;#39; with shard2 &amp;amp; Shard2. Mind case sensitivity.
sed -e &amp;quot;s/shardX/shard2/g; s/ShardX/Shard2/g; s/NAMESPACE_ID/daemonsl/g; s/DB_DISK/10Gi/g&amp;quot; mongodb-maindb-service-stateful.yaml &amp;gt; tmp-mongodb-maindb-service-stateful.yaml
kubectl apply -f tmp-mongodb-maindb-service-stateful.yaml

#run command to see Pods &amp;amp; Services spinning up
kubectl get svc,po --namespace=daemonsl
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Till here, we have accomplished statefulsets container running along with headless services and mounted a SSD volume that fulfills Pods requirement.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;kubectl get persistentvolumes

# Get persistent volume claims
kubectl get persistentvolumeclaims --namespace=daemonsl
&lt;/pre&gt;&lt;/div&gt;


&lt;hr&gt;
&lt;h4&gt;3.3 Mongos Deployment&lt;/h4&gt;
&lt;p&gt;We have configdb and maindb pods up and running. We will spin up mongos server to establish a sharding cluster. Replace &lt;code&gt;NAMESPACE_ID&lt;/code&gt; with &lt;code&gt;daemonsl&lt;/code&gt;, or whatever name you have defined.&lt;/p&gt;
&lt;p&gt;We have configured config server information in mongos using &lt;code&gt;--configdb&lt;/code&gt; flag with unique network identifiers of configdb pod. DNS of statefulset pods goes by convention &lt;code&gt;&amp;lt;POD_NAME&amp;gt;.&amp;lt;SERVICE_NAME&amp;gt;.&amp;lt;NAMESPACE&amp;gt;.svc.&amp;lt;CLUSTER_DOMAIN&amp;gt;&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Reference : &lt;/strong&gt; &lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#stable-network-id"&gt;https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#stable-network-id&lt;/a&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;apiVersion&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;apps&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;v1beta1&lt;/span&gt;
&lt;span class="n"&gt;kind&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Deployment&lt;/span&gt;
&lt;span class="n"&gt;metadata&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
  &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;mongos&lt;/span&gt;
  &lt;span class="kd"&gt;namespace&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;NAMESPACE_ID&lt;/span&gt;
&lt;span class="n"&gt;spec&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
  &lt;span class="n"&gt;replicas&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
  &lt;span class="n"&gt;template&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;metadata&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
      &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;role&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;mongos&lt;/span&gt;
        &lt;span class="n"&gt;tier&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;routers&lt;/span&gt;
    &lt;span class="n"&gt;spec&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
      &lt;span class="n"&gt;affinity&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;podAntiAffinity&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
          &lt;span class="n"&gt;preferredDuringSchedulingIgnoredDuringExecution&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
          &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;weight&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;
            &lt;span class="n"&gt;podAffinityTerm&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
              &lt;span class="n"&gt;labelSelector&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;matchExpressions&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
                &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tier&lt;/span&gt;
                  &lt;span class="n"&gt;operator&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;In&lt;/span&gt;
                  &lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
                  &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;routers&lt;/span&gt;
              &lt;span class="n"&gt;topologyKey&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;kubernetes&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;io&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;hostname&lt;/span&gt;
      &lt;span class="n"&gt;terminationGracePeriodSeconds&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;
      &lt;span class="n"&gt;containers&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;mongos&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;container&lt;/span&gt;
          &lt;span class="n"&gt;image&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;mongo&lt;/span&gt;
          &lt;span class="n"&gt;command&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
            &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;mongos&amp;quot;&lt;/span&gt;
            &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;--port&amp;quot;&lt;/span&gt;
            &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;27017&amp;quot;&lt;/span&gt;
            &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;--bind_ip&amp;quot;&lt;/span&gt;
            &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;0.0.0.0&amp;quot;&lt;/span&gt;
            &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;--configdb&amp;quot;&lt;/span&gt;
            &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;mongodb-configdb-0.mongodb-configdb-headless-service.daemonsl.svc.cluster.local:27019&amp;quot;&lt;/span&gt;
          &lt;span class="n"&gt;resources&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;requests&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
              &lt;span class="n"&gt;cpu&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="n"&gt;m&lt;/span&gt;
              &lt;span class="n"&gt;memory&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="n"&gt;Mi&lt;/span&gt;
          &lt;span class="n"&gt;ports&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
            &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;containerPort&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;27017&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;sed -e &amp;quot;s/NAMESPACE_ID/daemonsl/g&amp;quot; mongodb-mongos-deployment-service.yaml &amp;gt; tmp-mongodb-mongos-deployment-service.yaml
kubectl apply -f tmp-mongodb-mongos-deployment-service.yaml
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;4. Configure Sharding&lt;/h3&gt;
&lt;p&gt;Now, we have &lt;code&gt;mongos, configdb&lt;/code&gt; and &lt;code&gt;maindb&lt;/code&gt; up and running. We need to create Replicaset in MainDB servers that we are intending to make shard. We will run &lt;code&gt;rs.initiate()&lt;/code&gt; command to make &lt;code&gt;PRIMARY&lt;/code&gt; replica. Since we are going with one replica member in each shard. We will run initiate command in each of the &lt;code&gt;maindb&lt;/code&gt; pod.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nt"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Replicaset Init mongodb-shard1-0 &amp;quot;&lt;/span&gt;
&lt;span class="nt"&gt;kubectl&lt;/span&gt; &lt;span class="nt"&gt;exec&lt;/span&gt; &lt;span class="nt"&gt;--namespace&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nt"&gt;daemonsl&lt;/span&gt; &lt;span class="nt"&gt;mongodb-shard1-0&lt;/span&gt; &lt;span class="nt"&gt;-c&lt;/span&gt; &lt;span class="nt"&gt;mongodb-shard1-container&lt;/span&gt; &lt;span class="nt"&gt;--&lt;/span&gt; &lt;span class="nt"&gt;mongo&lt;/span&gt; &lt;span class="nt"&gt;--port&lt;/span&gt; &lt;span class="nt"&gt;27017&lt;/span&gt; &lt;span class="nt"&gt;--eval&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;rs.initiate({_id: \&amp;quot;Shard1\&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nt"&gt;version&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nt"&gt;1&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nt"&gt;members&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="cp"&gt;[&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="nx"&gt;_id&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;host&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="o"&gt;\&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;mongodb-shard1-0.mongodb-shard1-headless-service.daemonsl.svc.cluster.local:27017&lt;/span&gt;&lt;span class="se"&gt;\&amp;quot;&lt;/span&gt;&lt;span class="s2"&gt;} ] });&amp;quot;&lt;/span&gt;


&lt;span class="nx"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Replicaset Init mongodb-shard2-0 &amp;quot;&lt;/span&gt;  
&lt;span class="nx"&gt;kubectl&lt;/span&gt; &lt;span class="nx"&gt;exec&lt;/span&gt; &lt;span class="o"&gt;--&lt;/span&gt;&lt;span class="n"&gt;namespace&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nx"&gt;daemonsl&lt;/span&gt; &lt;span class="nx"&gt;mongodb&lt;/span&gt;&lt;span class="na"&gt;-shard2&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="na"&gt;-c&lt;/span&gt; &lt;span class="nx"&gt;mongodb&lt;/span&gt;&lt;span class="na"&gt;-shard2-container&lt;/span&gt; &lt;span class="o"&gt;--&lt;/span&gt; &lt;span class="nx"&gt;mongo&lt;/span&gt; &lt;span class="o"&gt;--&lt;/span&gt;&lt;span class="nx"&gt;port&lt;/span&gt; &lt;span class="mi"&gt;27017&lt;/span&gt; &lt;span class="o"&gt;--&lt;/span&gt;&lt;span class="nx"&gt;eval&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;rs.initiate({_id: &lt;/span&gt;&lt;span class="se"&gt;\&amp;quot;&lt;/span&gt;&lt;span class="s2"&gt;Shard2&lt;/span&gt;&lt;span class="se"&gt;\&amp;quot;&lt;/span&gt;&lt;span class="s2"&gt;, version: 1, members: [ {_id: 0, host: &lt;/span&gt;&lt;span class="se"&gt;\&amp;quot;&lt;/span&gt;&lt;span class="s2"&gt;mongodb-shard2-0.mongodb-shard2-headless-service.daemonsl.svc.cluster.local:27017&lt;/span&gt;&lt;span class="se"&gt;\&amp;quot;&lt;/span&gt;&lt;span class="s2"&gt;} ] });&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Above lines, will make both pods PRIMARY of their respective replicaset. You can even go into container to verify replicaset status by running &lt;code&gt;rs.status()&lt;/code&gt; command.&lt;/p&gt;
&lt;p&gt;We are proceeding now to add shards to mongos server. We will run below command in any of the mongos pod. Mongos server are stateless application, they save the configuration in configdb server which we have made stateful application by declaring them under statefulset container.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nt"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Adding Shard 1 : Shard1 &amp;quot;&lt;/span&gt;
&lt;span class="nt"&gt;kubectl&lt;/span&gt; &lt;span class="nt"&gt;exec&lt;/span&gt; &lt;span class="nt"&gt;--namespace&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nt"&gt;daemonsl&lt;/span&gt; &lt;span class="o"&gt;$(&lt;/span&gt;&lt;span class="nt"&gt;kubectl&lt;/span&gt; &lt;span class="nt"&gt;get&lt;/span&gt; &lt;span class="nt"&gt;pod&lt;/span&gt; &lt;span class="nt"&gt;-l&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;tier=routers&amp;quot;&lt;/span&gt; &lt;span class="nt"&gt;-o&lt;/span&gt; &lt;span class="nt"&gt;jsonpath&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;{.items&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;&lt;span class="s1"&gt;.metadata.name}&amp;#39;&lt;/span&gt; &lt;span class="nt"&gt;--namespace&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nt"&gt;daemonsl&lt;/span&gt; &lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="nt"&gt;-c&lt;/span&gt; &lt;span class="nt"&gt;mongos-container&lt;/span&gt; &lt;span class="nt"&gt;--&lt;/span&gt; &lt;span class="nt"&gt;mongo&lt;/span&gt; &lt;span class="nt"&gt;--port&lt;/span&gt; &lt;span class="nt"&gt;27017&lt;/span&gt; &lt;span class="nt"&gt;--eval&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;sh.addShard(\&amp;quot;Shard1/mongodb-shard1-0.mongodb-shard1-headless-service.daemonsl.svc.cluster.local:27017\&amp;quot;);&amp;quot;&lt;/span&gt;

&lt;span class="nt"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Adding Shard 2 : Shard2 &amp;quot;&lt;/span&gt;
&lt;span class="nt"&gt;kubectl&lt;/span&gt; &lt;span class="nt"&gt;exec&lt;/span&gt; &lt;span class="nt"&gt;--namespace&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nt"&gt;daemonsl&lt;/span&gt; &lt;span class="o"&gt;$(&lt;/span&gt;&lt;span class="nt"&gt;kubectl&lt;/span&gt; &lt;span class="nt"&gt;get&lt;/span&gt; &lt;span class="nt"&gt;pod&lt;/span&gt; &lt;span class="nt"&gt;-l&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;tier=routers&amp;quot;&lt;/span&gt; &lt;span class="nt"&gt;-o&lt;/span&gt; &lt;span class="nt"&gt;jsonpath&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;{.items&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;&lt;span class="s1"&gt;.metadata.name}&amp;#39;&lt;/span&gt; &lt;span class="nt"&gt;--namespace&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nt"&gt;daemonsl&lt;/span&gt; &lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="nt"&gt;-c&lt;/span&gt; &lt;span class="nt"&gt;mongos-container&lt;/span&gt; &lt;span class="nt"&gt;--&lt;/span&gt; &lt;span class="nt"&gt;mongo&lt;/span&gt; &lt;span class="nt"&gt;--port&lt;/span&gt; &lt;span class="nt"&gt;27017&lt;/span&gt; &lt;span class="nt"&gt;--eval&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;sh.addShard(\&amp;quot;Shard2/mongodb-shard2-0.mongodb-shard2-headless-service.daemonsl.svc.cluster.local:27017\&amp;quot;);&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now, we can get into one of the mongos container to verify the sharding status of cluster. All the above steps can be automated to make any number of shards within your cluster and thus concepts are very trivial to support stateful application powered by GKE.&lt;/p&gt;
&lt;hr&gt;
&lt;h4&gt;Test Sharding&lt;/h4&gt;
&lt;p&gt;To test that the sharded cluster is working properly, connect to the container running the first "mongos" router, then use the Mongo Shell to authenticate, enable sharding on a specific database &amp;amp; collection, add some test data to this collection and then view the status of the Sharded cluster and collection:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ kubectl &lt;span class="nb"&gt;exec&lt;/span&gt; -it &lt;span class="k"&gt;$(&lt;/span&gt;kubectl get pod -l &lt;span class="s2"&gt;&amp;quot;tier=routers&amp;quot;&lt;/span&gt; -o &lt;span class="nv"&gt;jsonpath&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;{.items[0].metadata.name}&amp;#39;&lt;/span&gt;&lt;span class="k"&gt;)&lt;/span&gt; -c mongos-container bash
$ mongo
&amp;gt; sh.enableSharding&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&amp;lt;Database_name&amp;gt;&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&amp;gt; sh.status&lt;span class="o"&gt;()&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&amp;gt; use admin
&amp;gt; db.admin.runCommand&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;getShardMap&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;hr&gt;
&lt;h4&gt;Tearing &amp;amp; Cleaning Down the Kubernetes Environment&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Important:&lt;/strong&gt; This step is required to ensure you aren't continuously charged by Google Cloud for an environment you no longer need.&lt;/p&gt;
&lt;p&gt;Run the following script to undeploy the MongoDB Services &amp;amp; StatefulSets/Deployments plus related Kubernetes resources, followed by the removal of the GCE disks. This script is available in repository.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ sh teardown.sh   &lt;span class="c1"&gt;#To delete all resources provisioned above&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;hr&gt;
&lt;h3&gt;Factors Addressed in this Demonstration&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Deployment of a MongoDB on the Google Kubernetes Engine&lt;/li&gt;
&lt;li&gt;Use of Kubernetes StatefulSets and PersistentVolumeClaims to ensure data is not lost when containers are recycled&lt;/li&gt;
&lt;li&gt;Proper configuration of a MongoDB Sharded Cluster for Scalability with each Shard being a Replica Set for full resiliency&lt;/li&gt;
&lt;li&gt;Controlling Anti-Affinity for Mongod Replicas to avoid a Single Point of Failure&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Github reference : &lt;a href="https://github.com/sunnykrGupta/gke-mongodb-shards"&gt;https://github.com/sunnykrGupta/gke-mongodb-shards&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Credit :&lt;/strong&gt; This blog is based on workdone by &lt;a href="https://twitter.com/TheDonester"&gt;Paul Done&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h4&gt;Must read below resources in order to get detailed understanding :&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset"&gt;https://kubernetes.io/docs/concepts/workloads/controllers/statefulset&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://kubernetes.io/docs/concepts/services-networking/service/#headless-services"&gt;https://kubernetes.io/docs/concepts/services-networking/service/#headless-services&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#stable-network-id"&gt;https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#stable-network-id&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#gce"&gt;https://kubernetes.io/docs/concepts/storage/storage-classes/#gce&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/"&gt;https://kubernetes.io/docs/concepts/storage/persistent-volumes/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.kubernetes.io/2017/03/dynamic-provisioning-and-storage-classes-kubernetes.html"&gt;http://blog.kubernetes.io/2017/03/dynamic-provisioning-and-storage-classes-kubernetes.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.kubernetes.io/2017/03/advanced-scheduling-in-kubernetes.html"&gt;http://blog.kubernetes.io/2017/03/advanced-scheduling-in-kubernetes.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;</content><category term="Kubernetes"></category><category term="Mongodb"></category><category term="StatefulSets"></category></entry><entry><title>Patch and Update Table - BigQuery Part-III</title><link href="https://sunnykrGupta.github.io/patch-and-update-table-bigquery-part-iii.html" rel="alternate"></link><published>2017-11-05T11:12:31+05:30</published><updated>2017-11-05T11:12:31+05:30</updated><author><name>Sunny Kr Gupta</name></author><id>tag:sunnykrgupta.github.io,2017-11-05:/patch-and-update-table-bigquery-part-iii.html</id><summary type="html">&lt;p&gt;Table updation is important when you have data ready inside the table and suddenly you have a requirement of adding more fields in the table to do analysis. In this post, we are going to dive into Streaming feature of BigQuery.&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;img alt="Google BigQuery" src="/images/bq-series/part3/bigquery.png"&gt;&lt;/p&gt;
&lt;p&gt;This post is 3rd part of 3-post series. In the earlier post, we understood the streaming in BigQuery &lt;strong&gt;&lt;a href="https://sunnykrgupta.github.io/streaming-with-redis-bigquery-part-ii.html"&gt;Streaming with Redis - BigQuery Part-II&lt;/a&gt;&lt;/strong&gt;. In this post, we are going to learn patching and updating table schemas.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Update/Patch Table in BigQuery&lt;/h3&gt;
&lt;p&gt;Table updation is important when you have data ready inside the table and suddenly you have a requirement of adding more fields in the table to do analysis.&lt;/p&gt;
&lt;p&gt;Here, we are going to add a field and see the changes in the table after update operation.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h4&gt;1. Add field into schema&lt;/h4&gt;
&lt;p&gt;We are going to use same table &lt;strong&gt;&lt;code&gt;StreamTable&lt;/code&gt;&lt;/strong&gt; which we used earlier for streaming and introducing one more field as &lt;strong&gt;&lt;code&gt;UniqueSocialNumber&lt;/code&gt;&lt;/strong&gt; with datatype &lt;strong&gt;&lt;code&gt;INTEGER&lt;/code&gt;&lt;/strong&gt;. Lets add these changes to &lt;strong&gt;&lt;code&gt;schema.py&lt;/code&gt;&lt;/strong&gt; which will be used by our main program written in later steps.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ cat schema.py

&lt;span class="c1"&gt;#Add field schema&lt;/span&gt;
&lt;span class="nv"&gt;TableObject&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;tableReference&amp;quot;&lt;/span&gt;: &lt;span class="o"&gt;{&lt;/span&gt;
      &lt;span class="s2"&gt;&amp;quot;projectId&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;mimetic-slate&amp;quot;&lt;/span&gt;,
      &lt;span class="s2"&gt;&amp;quot;tableId&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;StreamTable&amp;quot;&lt;/span&gt;,
      &lt;span class="s2"&gt;&amp;quot;datasetId&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;BQ_Dataset&amp;quot;&lt;/span&gt;,
    &lt;span class="o"&gt;}&lt;/span&gt;,

    &lt;span class="s2"&gt;&amp;quot;schema&amp;quot;&lt;/span&gt;: &lt;span class="o"&gt;{&lt;/span&gt;
      &lt;span class="s2"&gt;&amp;quot;fields&amp;quot;&lt;/span&gt;: &lt;span class="o"&gt;[&lt;/span&gt;
          &lt;span class="o"&gt;{&lt;/span&gt;
              &lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;username&amp;quot;&lt;/span&gt;,
              &lt;span class="s2"&gt;&amp;quot;type&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;STRING&amp;quot;&lt;/span&gt;,
              &lt;span class="s2"&gt;&amp;quot;mode&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;NULLABLE&amp;quot;&lt;/span&gt;
          &lt;span class="o"&gt;}&lt;/span&gt;,
          &lt;span class="o"&gt;{&lt;/span&gt;
              &lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;,
              &lt;span class="s2"&gt;&amp;quot;type&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;STRING&amp;quot;&lt;/span&gt;,
              &lt;span class="s2"&gt;&amp;quot;mode&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;NULLABLE&amp;quot;&lt;/span&gt;
          &lt;span class="o"&gt;}&lt;/span&gt;,
          &lt;span class="o"&gt;{&lt;/span&gt;
              &lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;birthdate&amp;quot;&lt;/span&gt;,
              &lt;span class="s2"&gt;&amp;quot;type&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;STRING&amp;quot;&lt;/span&gt;,
              &lt;span class="s2"&gt;&amp;quot;mode&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;NULLABLE&amp;quot;&lt;/span&gt;
          &lt;span class="o"&gt;}&lt;/span&gt;,
          &lt;span class="o"&gt;{&lt;/span&gt;
              &lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;sex&amp;quot;&lt;/span&gt;,
              &lt;span class="s2"&gt;&amp;quot;type&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;STRING&amp;quot;&lt;/span&gt;,
              &lt;span class="s2"&gt;&amp;quot;mode&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;NULLABLE&amp;quot;&lt;/span&gt;
          &lt;span class="o"&gt;}&lt;/span&gt;,
          &lt;span class="o"&gt;{&lt;/span&gt;
              &lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;address&amp;quot;&lt;/span&gt;,
              &lt;span class="s2"&gt;&amp;quot;type&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;STRING&amp;quot;&lt;/span&gt;,
              &lt;span class="s2"&gt;&amp;quot;mode&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;NULLABLE&amp;quot;&lt;/span&gt;
          &lt;span class="o"&gt;}&lt;/span&gt;,
          &lt;span class="o"&gt;{&lt;/span&gt;
              &lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;mail&amp;quot;&lt;/span&gt;,
              &lt;span class="s2"&gt;&amp;quot;type&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;STRING&amp;quot;&lt;/span&gt;,
              &lt;span class="s2"&gt;&amp;quot;mode&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;NULLABLE&amp;quot;&lt;/span&gt;
          &lt;span class="o"&gt;}&lt;/span&gt;,
          &lt;span class="o"&gt;{&lt;/span&gt;
              &lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;UniqueSocialNumber&amp;quot;&lt;/span&gt;,   &lt;span class="c1"&gt;#New Field&lt;/span&gt;
              &lt;span class="s2"&gt;&amp;quot;type&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;INTEGER&amp;quot;&lt;/span&gt;,
              &lt;span class="s2"&gt;&amp;quot;mode&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;NULLABLE&amp;quot;&lt;/span&gt;
          &lt;span class="o"&gt;}&lt;/span&gt;
      &lt;span class="o"&gt;]&lt;/span&gt;,
  &lt;span class="o"&gt;}&lt;/span&gt;,
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h4&gt;2. Patch/Update API in BigQuery&lt;/h4&gt;
&lt;p&gt;We have &lt;code&gt;schema.py&lt;/code&gt; script ready and below is our main program &lt;strong&gt;&lt;code&gt;tablePatch.py&lt;/code&gt;&lt;/strong&gt; that will execute the table patch API call to bigquery.&lt;/p&gt;
&lt;p&gt;We have two methods available in BigQuery to make updates in table.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;tables.patch&lt;/code&gt;&lt;/strong&gt; - This method only replaces fields that are provided in the resources&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;tables.update&lt;/code&gt;&lt;/strong&gt; - This method replaces the entire table resource.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Patch Example :&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;$&lt;/span&gt; &lt;span class="n"&gt;cat&lt;/span&gt; &lt;span class="n"&gt;tablePatch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;py&lt;/span&gt;

&lt;span class="c1"&gt;#!/usr/bin/env python&lt;/span&gt;

&lt;span class="c1"&gt;#https://developers.google.com/api-client-library/python/&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;googleapiclient&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;discovery&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;oauth2client.client&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;GoogleCredentials&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;schema&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;TableObject&lt;/span&gt;


&lt;span class="c1"&gt;# [START Table Creater ]&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;PatchTable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bigquery&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;tables&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;bigquery&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tables&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="n"&gt;tableStatusObject&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tables&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;patch&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;projectId&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;TableObject&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;tableReference&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;projectId&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; \
                &lt;span class="n"&gt;datasetId&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;TableObject&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;tableReference&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;datasetId&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; \
                &lt;span class="n"&gt;tableId&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;TableObject&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;tableReference&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;tableId&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; \
                &lt;span class="n"&gt;body&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;TableObject&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;execute&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="se"&gt;\n\n&lt;/span&gt;&lt;span class="s2"&gt;Table Patched&amp;quot;&lt;/span&gt;
&lt;span class="c1"&gt;# [END]&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="c1"&gt;#to get credentials from my laptop&lt;/span&gt;
    &lt;span class="n"&gt;credentials&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;GoogleCredentials&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_application_default&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="c1"&gt;# Construct the service object for interacting with the BigQuery API.&lt;/span&gt;
    &lt;span class="n"&gt;bigquery&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;discovery&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;build&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;bigquery&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;v2&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;credentials&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;credentials&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;PatchTable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bigquery&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;https://cloud.google.com/bigquery/docs/tables#update-schema&lt;/span&gt;
&lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="vm"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;__main__&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;main&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;BQ Table Patch !!&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Update Example :&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;#instead of calling patch(), we call update() to apply updates
tables.update(projectId=TableObject[&amp;#39;tableReference&amp;#39;][&amp;#39;projectId&amp;#39;],\
                datasetId=TableObject[&amp;#39;tableReference&amp;#39;][&amp;#39;datasetId&amp;#39;],\
                tableId=TableObject[&amp;#39;tableReference&amp;#39;][&amp;#39;tableId&amp;#39;], \
                body=TableObject).execute()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;After applying changes we can verify changes in schema of bigquery table.&lt;/p&gt;
&lt;p&gt;Visit UI :&lt;a href="https://bigquery.cloud.google.com"&gt;https://bigquery.cloud.google.com&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;You can also verify table schema by running &lt;strong&gt;&lt;code&gt;bq&lt;/code&gt;&lt;/strong&gt; CLI commands&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ bq show BQ_Dataset.StreamTable
Table mimetic-slate:BQ_Dataset.StreamTable

   Last modified                Schema               Total Rows   Total Bytes   Expiration   Time Partitioning   Labels  
 ----------------- -------------------------------- ------------ ------------- ------------ ------------------- --------
  &lt;span class="m"&gt;04&lt;/span&gt; Nov &lt;span class="m"&gt;15&lt;/span&gt;:48:43   &lt;span class="p"&gt;|&lt;/span&gt;-username: string              &lt;span class="m"&gt;451&lt;/span&gt;          &lt;span class="m"&gt;50837&lt;/span&gt;                                                  
                    &lt;span class="p"&gt;|&lt;/span&gt;-name:string                                                                                      
                    &lt;span class="p"&gt;|&lt;/span&gt;-birthdate:string                                                                                 
                    &lt;span class="p"&gt;|&lt;/span&gt;-sex:string                                                                                       
                    &lt;span class="p"&gt;|&lt;/span&gt;-address:string                                                                                   
                    &lt;span class="p"&gt;|&lt;/span&gt;-mail:string                                                                                      
                    &lt;span class="p"&gt;|&lt;/span&gt;-UniqueSocialNumber: integer
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Reference &lt;/strong&gt; :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Table Patch and Update :&lt;/strong&gt; &lt;a href="https://developers.google.com/resources/api-libraries/documentation/bigquery/v2/python/latest/bigquery_v2.tables.html"&gt;https://developers.google.com/resources/api-libraries/documentation/bigquery/v2/python/latest/bigquery_v2.tables.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h4&gt;3. Preview of data in bigquery table&lt;/h4&gt;
&lt;p&gt;Click on table preview to see new field. It will show &lt;strong&gt;&lt;code&gt;null&lt;/code&gt;&lt;/strong&gt; with old records. You can start streaming data which contains this newly added field to be written in table going forward.&lt;/p&gt;
&lt;div style="text-align: center"&gt;&lt;u&gt;&lt;b&gt;Table Preview&lt;/b&gt;&lt;/u&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="New Field" src="/images/bq-series/part3/new-field.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Github reference&lt;/strong&gt; : &lt;a href="https://github.com/sunnykrGupta/Bigquery-series"&gt;https://github.com/sunnykrGupta/Bigquery-series&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;This was introduction to updation in BigQuery table, thus concludes our 3-post series.&lt;/p&gt;
&lt;p&gt;From this series, we covered basic features of Google BigQuery serverless product. There are other similar product available on other clouds like AWS redshift, Azure SQL Data Warehouse which serve infinite computing resources like BigQuery.&lt;/p&gt;
&lt;p&gt;Please comment your thoughts/feedback about this series.&lt;/p&gt;
&lt;hr&gt;</content><category term="Google BigQuery"></category><category term="GCP"></category><category term="Google Cloud Platform"></category></entry><entry><title>Streaming with Redis - BigQuery Part-II</title><link href="https://sunnykrGupta.github.io/streaming-with-redis-bigquery-part-ii.html" rel="alternate"></link><published>2017-11-01T17:12:31+05:30</published><updated>2017-11-01T17:12:31+05:30</updated><author><name>Sunny Kr Gupta</name></author><id>tag:sunnykrgupta.github.io,2017-11-01:/streaming-with-redis-bigquery-part-ii.html</id><summary type="html">&lt;p&gt;Streaming helps in pushing our data into BigQuery (short for BQ) and helps in making data available for query without delay of running load jobs. In this post, we are going to dive into Streaming feature of BigQuery.&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;img alt="Google BigQuery" src="/images/bq-series/part2/bq-stream.png"&gt;&lt;/p&gt;
&lt;p&gt;This post is 2nd part of 3-post series. In the earlier post, we understood the fundamentals of BigQuery Load Jobs &lt;strong&gt;&lt;a href="https://sunnykrgupta.github.io/export-load-job-with-mongodb-bigquery-part-i.html"&gt;Export &amp;amp; Load Job with MongoDB - BigQuery Part-I&lt;/a&gt;&lt;/strong&gt;. In this post, we are going to dive into Streaming feature of BigQuery.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a href="https://cloud.google.com/bigquery/streaming-data-into-bigquery"&gt;Streaming&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Why ?&lt;/strong&gt; - Streaming helps in pushing our data into BigQuery (short for BQ) and helps in making data available for query without delay of running load jobs.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;There are some trade-offs to choose Streaming. A few are belows :&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;We need to follow a few quotas like http body size, maximum rows / request etc while making streaming API calls.&lt;/li&gt;
&lt;li&gt;Written data in tables are not instantly available for copy or for export jobs in bigquery, it will take upto 90 minutes to be made available while load based tables are available instantly.&lt;/li&gt;
&lt;li&gt;At the time of writing this post, charges incurred in streaming whereas load jobs were free.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Keeping above in mind, we need to choose streaming vs load jobs in BigQuery.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Quotas :&lt;/strong&gt; &lt;a href="https://cloud.google.com/bigquery/quotas#streaminginserts"&gt;https://cloud.google.com/bigquery/quotas#streaminginserts&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Streaming Data into BigQuery&lt;/h3&gt;
&lt;p&gt;In this article, we are going to use a &lt;a href="https://redis.io"&gt;redis server&lt;/a&gt; as a message broker to hold our data.&lt;/p&gt;
&lt;p&gt;We are going to prepare data and the skeleton of data is going to be basic information of any person (username, name, birthdate, sex, address, email). As per this information, we need schema and table in bigquery to be created in advance before streaming. Post table creation, we are going to run streaming program to ingest our data in bulk which will be read from redis and same will be written to bigquery table in real time.
We are going to use &lt;code&gt;python&lt;/code&gt; as our programming language.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h4&gt;1. Prepare data in Redis&lt;/h4&gt;
&lt;p&gt;&lt;img alt="Redis" src="/images/bq-series/part2/redis.png"&gt;&lt;/p&gt;
&lt;p&gt;We are going to write a small python script to preapare data in &lt;a href="https://redis.io/topics/data-types"&gt;redis List&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Redis Installation :&lt;/strong&gt; &lt;a href="https://redis.io/download"&gt;https://redis.io/download&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;If you have docker running, run redis inside container with simple command&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;docker run -d --name redis-streaming -p 6379:6379 redis
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Script is going to execute &lt;a href="https://redis.io/commands/lpush"&gt;LPUSH command&lt;/a&gt; in redis to insert data into list named as &lt;code&gt;redisList&lt;/code&gt; . With the help of &lt;code&gt;Faker&lt;/code&gt; library we are going to generate some fake profile as our data.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Faker :&lt;/strong&gt; &lt;a href="https://github.com/joke2k/faker"&gt;https://github.com/joke2k/faker&lt;/a&gt;&lt;/p&gt;
&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt; 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="ch"&gt;#!/usr/bin/env python&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;redis&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;json&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nn"&gt;time&lt;/span&gt;
&lt;span class="c1"&gt;#Faker : https://github.com/joke2k/faker&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;faker&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Faker&lt;/span&gt;

&lt;span class="n"&gt;streamRedis&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;redis&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Redis&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;host&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;127.0.0.1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
             &lt;span class="n"&gt;port&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;6379&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;password&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;fake&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Faker&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;profile_generator&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;fake&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;simple_profile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sex&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;streamRedis&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;lpush&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;redisList&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;json&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dumps&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;profile_generator&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="c1"&gt;#sleep 200ms&lt;/span&gt;
        &lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sleep&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="vm"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;__main__&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;main&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h4&gt;2. Inspect data and prepare schema for Table&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Our data looks like :&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;{
    &amp;#39;username&amp;#39;: u&amp;#39;tarawade&amp;#39;,
    &amp;#39;name&amp;#39;: u&amp;#39;Jennifer Lewis&amp;#39;,
    &amp;#39;birthdate&amp;#39;: &amp;#39;2005-06-14&amp;#39;,
    &amp;#39;sex&amp;#39;: &amp;#39;F&amp;#39;,
    &amp;#39;address&amp;#39;: u&amp;#39;7134 Robinson Club Apt. 530\nPort Andreachester, GA 19011-6162&amp;#39;,
    &amp;#39;mail&amp;#39;: u&amp;#39;tmorgan@yahoo.com&amp;#39;
}
&lt;/pre&gt;&lt;/div&gt;


&lt;blockquote&gt;
&lt;p&gt;For the above formatted data, below schema is going to work :&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;[
    {
        &amp;quot;name&amp;quot;: &amp;quot;username&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;STRING&amp;quot;, &amp;quot;mode&amp;quot;: &amp;quot;NULLABLE&amp;quot;
    },
    {
        &amp;quot;name&amp;quot;: &amp;quot;name&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;STRING&amp;quot;, &amp;quot;mode&amp;quot;: &amp;quot;NULLABLE&amp;quot;
    },
    {
        &amp;quot;name&amp;quot;: &amp;quot;birthdate&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;STRING&amp;quot;, &amp;quot;mode&amp;quot;: &amp;quot;NULLABLE&amp;quot;
    },
    {
        &amp;quot;name&amp;quot;: &amp;quot;sex&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;STRING&amp;quot;, &amp;quot;mode&amp;quot;: &amp;quot;NULLABLE&amp;quot;
    },
    {
        &amp;quot;name&amp;quot;: &amp;quot;address&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;STRING&amp;quot;, &amp;quot;mode&amp;quot;: &amp;quot;NULLABLE&amp;quot;
    },
    {
        &amp;quot;name&amp;quot;: &amp;quot;mail&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;STRING&amp;quot;, &amp;quot;mode&amp;quot;: &amp;quot;NULLABLE&amp;quot;
    }
]
&lt;/pre&gt;&lt;/div&gt;


&lt;hr&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h4&gt;3. Create a table in BigQuery&lt;/h4&gt;
&lt;p&gt;We are going to create two python file, ie, &lt;strong&gt;&lt;code&gt;createConfig.py&lt;/code&gt;&lt;/strong&gt; that will keep schema configuration and &lt;strong&gt;&lt;code&gt;tableCreate.py&lt;/code&gt;&lt;/strong&gt; that will execute the table creation API call to bigquery. We are going to use &lt;strong&gt;&lt;code&gt;Google Application Default Credentials&lt;/code&gt;&lt;/strong&gt; to authorize our python application to talk to bigquery APIs.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ cat createConfig.py

&lt;span class="nv"&gt;TableObject&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;tableReference&amp;quot;&lt;/span&gt;: &lt;span class="o"&gt;{&lt;/span&gt;
      &lt;span class="s2"&gt;&amp;quot;projectId&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;mimetic-slate&amp;quot;&lt;/span&gt;,
      &lt;span class="s2"&gt;&amp;quot;tableId&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;StreamTable&amp;quot;&lt;/span&gt;,
      &lt;span class="s2"&gt;&amp;quot;datasetId&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;BQ_Dataset&amp;quot;&lt;/span&gt;,
    &lt;span class="o"&gt;}&lt;/span&gt;,

    &lt;span class="s2"&gt;&amp;quot;schema&amp;quot;&lt;/span&gt;: &lt;span class="o"&gt;{&lt;/span&gt;
      &lt;span class="s2"&gt;&amp;quot;fields&amp;quot;&lt;/span&gt;: &lt;span class="o"&gt;[&lt;/span&gt;
          &lt;span class="o"&gt;{&lt;/span&gt;
              &lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;username&amp;quot;&lt;/span&gt;,
              &lt;span class="s2"&gt;&amp;quot;type&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;STRING&amp;quot;&lt;/span&gt;,
              &lt;span class="s2"&gt;&amp;quot;mode&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;NULLABLE&amp;quot;&lt;/span&gt;
          &lt;span class="o"&gt;}&lt;/span&gt;,
          &lt;span class="o"&gt;{&lt;/span&gt;
              &lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;,
              &lt;span class="s2"&gt;&amp;quot;type&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;STRING&amp;quot;&lt;/span&gt;,
              &lt;span class="s2"&gt;&amp;quot;mode&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;NULLABLE&amp;quot;&lt;/span&gt;
          &lt;span class="o"&gt;}&lt;/span&gt;,
          &lt;span class="o"&gt;{&lt;/span&gt;
              &lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;birthdate&amp;quot;&lt;/span&gt;,
              &lt;span class="s2"&gt;&amp;quot;type&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;STRING&amp;quot;&lt;/span&gt;,
              &lt;span class="s2"&gt;&amp;quot;mode&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;NULLABLE&amp;quot;&lt;/span&gt;
          &lt;span class="o"&gt;}&lt;/span&gt;,
          &lt;span class="o"&gt;{&lt;/span&gt;
              &lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;sex&amp;quot;&lt;/span&gt;,
              &lt;span class="s2"&gt;&amp;quot;type&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;STRING&amp;quot;&lt;/span&gt;,
              &lt;span class="s2"&gt;&amp;quot;mode&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;NULLABLE&amp;quot;&lt;/span&gt;
          &lt;span class="o"&gt;}&lt;/span&gt;,
          &lt;span class="o"&gt;{&lt;/span&gt;
              &lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;address&amp;quot;&lt;/span&gt;,
              &lt;span class="s2"&gt;&amp;quot;type&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;STRING&amp;quot;&lt;/span&gt;,
              &lt;span class="s2"&gt;&amp;quot;mode&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;NULLABLE&amp;quot;&lt;/span&gt;
          &lt;span class="o"&gt;}&lt;/span&gt;,
          &lt;span class="o"&gt;{&lt;/span&gt;
              &lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;mail&amp;quot;&lt;/span&gt;,
              &lt;span class="s2"&gt;&amp;quot;type&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;STRING&amp;quot;&lt;/span&gt;,
              &lt;span class="s2"&gt;&amp;quot;mode&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;NULLABLE&amp;quot;&lt;/span&gt;
          &lt;span class="o"&gt;}&lt;/span&gt;
      &lt;span class="o"&gt;]&lt;/span&gt;,
  &lt;span class="o"&gt;}&lt;/span&gt;,
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We are going to use &lt;strong&gt; &lt;a href="https://developers.google.com/api-client-library/python/"&gt;google-api-python-client&lt;/a&gt;&lt;/strong&gt; library for interacting to our bigquery APIs.&lt;/p&gt;
&lt;p&gt;We are building service object by calling our API name and version supported by API. In this case we are using &lt;strong&gt;&lt;code&gt;bigquery&lt;/code&gt;&lt;/strong&gt; with version &lt;strong&gt;&lt;code&gt;v2&lt;/code&gt;&lt;/strong&gt;. This service object will be used to make tables related operation.  As of now we are going to use &lt;strong&gt;&lt;code&gt;insert&lt;/code&gt;&lt;/strong&gt; function to make table.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;GoogleCredentials.get_application_default()&lt;/code&gt;&lt;/strong&gt; will read the credentials stored in my system. Either you need to export a variable mentioned in reference with service account key or you setup an google SDK which will store default credentials inside your home directory.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;ls ~/.config/gcloud/*.json
~/.config/gcloud/application_default_credentials.json
&lt;/pre&gt;&lt;/div&gt;


&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt; 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="ch"&gt;#!/usr/bin/env python&lt;/span&gt;

&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;googleapiclient&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;discovery&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;oauth2client.client&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;GoogleCredentials&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;tableCreate&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;TableObject&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;createTable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bigquery&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;tables&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;bigquery&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tables&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="c1"&gt;#insert utility make call to BQ API with payload \&lt;/span&gt;
    &lt;span class="c1"&gt;#(TableObject) contains schema and table-name information&lt;/span&gt;
    &lt;span class="n"&gt;tableStatusObject&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tables&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;insert&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="n"&gt;projectId&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;mimetic-slate&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; \
     &lt;span class="n"&gt;datasetId&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;BQ_Dataset&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;body&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;TableObject&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;execute&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="c1"&gt;# [END]&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="c1"&gt;#to get credentials from my laptop&lt;/span&gt;
    &lt;span class="n"&gt;credentials&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;GoogleCredentials&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_application_default&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="c1"&gt;# Construct the service object for interacting with the BigQuery API.&lt;/span&gt;
    &lt;span class="n"&gt;bigquery&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;discovery&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;build&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;bigquery&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;v2&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;credentials&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;credentials&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;createTable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bigquery&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="vm"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;__main__&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;main&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;BQ Table Creator !!&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;

&lt;p&gt;Run above to program to create table with name &lt;strong&gt;StreamTable&lt;/strong&gt; in bigquery dataset &lt;strong&gt;&lt;code&gt;BQ_Dataset&lt;/code&gt;&lt;/strong&gt;. Make sure you have created dataset already.&lt;/p&gt;
&lt;p&gt;You can verify the table created by visiting bigquery UI. Visit : &lt;a href="https://bigquery.cloud.google.com"&gt;https://bigquery.cloud.google.com&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;You can also verify table creation by running &lt;code&gt;bq&lt;/code&gt; CLI commands&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ bq ls BQ_Dataset    
    tableId     Type    Labels   Time Partitioning  
 ------------- ------- -------- -------------------
  StreamTable   TABLE                               
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="bq show " src="/images/bq-series/part2/bq-show-2.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Reference &lt;/strong&gt; :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Table Creation : &lt;/strong&gt; &lt;a href="https://developers.google.com/resources/api-libraries/documentation/bigquery/v2/python/latest/bigquery_v2.tables.html"&gt;https://developers.google.com/resources/api-libraries/documentation/bigquery/v2/python/latest/bigquery_v2.tables.html&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Google Application Default Credentials :&lt;/strong&gt; &lt;a href="https://developers.google.com/identity/protocols/application-default-credentials"&gt;https://developers.google.com/identity/protocols/application-default-credentials&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Build the service object :&lt;/strong&gt; &lt;a href="https://developers.google.com/api-client-library/python/start/get_started#build-the-service-object"&gt;https://developers.google.com/api-client-library/python/start/get_started#build-the-service-object&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h4&gt;4. Streaming into Bigquery&lt;/h4&gt;
&lt;p&gt;Now, we have table created and data queued into redis list, We are ready to stream right away by running a python script, lets call this script &lt;strong&gt;&lt;code&gt;bq-streamer.py&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt; 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="ch"&gt;#!/usr/bin/env python&lt;/span&gt;

&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;googleapiclient&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;discovery&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;oauth2client.client&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;GoogleCredentials&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;json&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nn"&gt;time&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nn"&gt;copy&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;redis&lt;/span&gt;


&lt;span class="n"&gt;batchCount&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;

&lt;span class="n"&gt;redisStream&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;redis&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Redis&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;host&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;127.0.0.1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
             &lt;span class="n"&gt;port&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;6379&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;password&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;streamObject&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;rows&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
      &lt;span class="c1"&gt;#{ &amp;quot;json&amp;quot;: {# Represents a single JSON object. } }&lt;/span&gt;
    &lt;span class="p"&gt;],&lt;/span&gt;
 &lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="c1"&gt;#[START Streaming batcher]&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;streamBuilder&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="c1"&gt;#Every API needs a refresh copy of dict&lt;/span&gt;
    &lt;span class="n"&gt;newStreamObject&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;copy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;deepcopy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;streamObject&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;currentCounter&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="n"&gt;currentCounter&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;batchCount&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;packet&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;redisStream&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;brpop&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;redisList&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="n"&gt;timeout&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;newStreamObject&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;rows&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;json&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;json&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loads&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;packet&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;})&lt;/span&gt;
        &lt;span class="n"&gt;currentCounter&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt;  &lt;span class="mi"&gt;1&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;newStreamObject&lt;/span&gt;
&lt;span class="c1"&gt;#[END]&lt;/span&gt;


&lt;span class="c1"&gt;# [START Streaming Utility]&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;streamUtils&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bigquery&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;tabledata&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;bigquery&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tabledata&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="c1"&gt;#Run infinitely&lt;/span&gt;
    &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;streamBuildBatch&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;streamBuilder&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

        &lt;span class="c1"&gt;#BQ API to insert bulk data into table&lt;/span&gt;
        &lt;span class="n"&gt;insertStatusObject&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tabledata&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;insertAll&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;projectId&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;mimetic-slate&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; \
        &lt;span class="n"&gt;datasetId&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;BQ_Dataset&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;tableId&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;StreamTable&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; \
        &lt;span class="n"&gt;body&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;streamBuildBatch&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;execute&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;


&lt;span class="c1"&gt;# [ MAIN]&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="n"&gt;credentials&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;GoogleCredentials&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_application_default&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="c1"&gt;# Construct the service object for interacting with the BigQuery API.&lt;/span&gt;
    &lt;span class="n"&gt;bigquery&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;discovery&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;build&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;bigquery&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;v2&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;credentials&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;credentials&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;#Stream utility&lt;/span&gt;
    &lt;span class="n"&gt;streamUtils&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bigquery&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# [END]&lt;/span&gt;


&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="vm"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;__main__&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;main&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;

&lt;p&gt;Above program is going to read redis running on &lt;code&gt;127.0.0.1:6379&lt;/code&gt; from list name &lt;strong&gt;&lt;code&gt;redisList&lt;/code&gt;&lt;/strong&gt; and build a dict object &lt;strong&gt;&lt;code&gt;streamObject&lt;/code&gt;&lt;/strong&gt; that is accepted by bq streaming API. We are calling &lt;strong&gt;&lt;code&gt;insertAll&lt;/code&gt;&lt;/strong&gt; utility to submit our streaming request to bigquery API.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;insertAll&lt;/code&gt;&lt;/strong&gt; takes &lt;strong&gt;&lt;code&gt;projectId, datasetId, tableId&lt;/code&gt;&lt;/strong&gt; as an argument and &lt;strong&gt;&lt;code&gt;body&lt;/code&gt;&lt;/strong&gt; which contains your data to be streamed.&lt;/p&gt;
&lt;p&gt;Script has been configured to &lt;a href="https://redis.io/commands/brpop"&gt;pop&lt;/a&gt; &lt;code&gt;100 entries&lt;/code&gt; from redis list and prepare it to be pushed into table. Run &lt;strong&gt;&lt;code&gt;bq-streamer.py&lt;/code&gt;&lt;/strong&gt; script to start streaming data into bigquery table.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;#when bulk data is prepared, JSON payload in body argument would look like
{
    &amp;quot;rows&amp;quot;: [
        {
            &amp;quot;json&amp;quot;: {
                &amp;quot;username&amp;quot;: &amp;quot;nicholaswagner&amp;quot;,
                &amp;quot;name&amp;quot;: &amp;quot;Laura Scott&amp;quot;,
                &amp;quot;birthdate&amp;quot;: &amp;quot;1970-11-30&amp;quot;,
                &amp;quot;sex&amp;quot;: &amp;quot;F&amp;quot;,
                &amp;quot;address&amp;quot;: &amp;quot;788 Faulkner Locks Suite 687\nSanfordside, FL 50804-6818&amp;quot;,
                &amp;quot;mail&amp;quot;: &amp;quot;austinnathaniel@yahoo.com&amp;quot;
            }
        },
        {
           &amp;quot;json&amp;quot;: {
               &amp;quot;username&amp;quot;: &amp;quot;david27&amp;quot;,
               &amp;quot;name&amp;quot;: &amp;quot;Aaron Silva&amp;quot;,
               &amp;quot;birthdate&amp;quot;: &amp;quot;2003-09-17&amp;quot;,
               &amp;quot;sex&amp;quot;: &amp;quot;M&amp;quot;,
               &amp;quot;address&amp;quot;: &amp;quot;57976 Collins Loaf Apt. 843\nMichaelfort, VA 79233&amp;quot;,
               &amp;quot;mail&amp;quot;: &amp;quot;dbeck@hotmail.com&amp;quot;
           }
        },
        .....
        more data
        .....
}
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;Reference&lt;/strong&gt; :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Streaming insertAll :&lt;/strong&gt; &lt;a href="https://developers.google.com/resources/api-libraries/documentation/bigquery/v2/python/latest/bigquery_v2.tabledata.html"&gt;https://developers.google.com/resources/api-libraries/documentation/bigquery/v2/python/latest/bigquery_v2.tabledata.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h4&gt;5. Verify the data in BigQuery Table&lt;/h4&gt;
&lt;p&gt;After running streaming, you will start seeing something similar as shown below when you click table info. Clicking on preview will not show you any streamed data, it will take a while to appear but it will be in buffer to be available for query instantly.&lt;/p&gt;
&lt;div style="text-align: center"&gt;&lt;u&gt;&lt;b&gt;Buffer Statistics&lt;/b&gt;&lt;/u&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="streaming-1" src="/images/bq-series/part2/stream.png"&gt;&lt;/p&gt;
&lt;div style="text-align: center"&gt;&lt;u&gt;&lt;b&gt;Table Preview&lt;/b&gt;&lt;/u&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="streaming-2" src="/images/bq-series/part2/preview-none.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;SQL Query in BQ Table&lt;/h3&gt;
&lt;p&gt;We are going to run a simple query to show the output that shows your streamed data.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;SELECT * FROM [mimetic-slate:BQ_Dataset.StreamTable]
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="text-align: center"&gt;&lt;u&gt;&lt;b&gt;Query Result&lt;/u&gt;&lt;/b&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="query" src="/images/bq-series/part2/query-2.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Github reference&lt;/strong&gt; : &lt;a href="https://github.com/sunnykrGupta/Bigquery-series"&gt;https://github.com/sunnykrGupta/Bigquery-series&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;That's all from this &lt;strong&gt;series Part-II&lt;/strong&gt;. Hope you will get basic understanding of Streaming in BigQuery from this post. Streaming is helpful in cases when you want your data to be instantly available for query, helps in scenario where have a requirement of building real time analysis.&lt;/p&gt;
&lt;p&gt;I would appreciate feedback via comments. In next blog which is part of this series, I will be covering &lt;strong&gt;Patching and Updating table schema in Bigquery&lt;/strong&gt; which is important when you want to add fields in table.&lt;/p&gt;
&lt;hr&gt;</content><category term="Google BigQuery"></category><category term="GCP"></category><category term="Redis"></category><category term="Google Cloud Platform"></category></entry><entry><title>Export &amp; Load Job with MongoDB - BigQuery Part-I</title><link href="https://sunnykrGupta.github.io/export-load-job-with-mongodb-bigquery-part-i.html" rel="alternate"></link><published>2017-10-16T01:12:31+05:30</published><updated>2017-10-16T01:12:31+05:30</updated><author><name>Sunny Kr Gupta</name></author><id>tag:sunnykrgupta.github.io,2017-10-16:/export-load-job-with-mongodb-bigquery-part-i.html</id><summary type="html">&lt;p&gt;To get into fundamentals of Google BigQuery and related jobs needed to get your data inside BigQuery system. We are going to use a mongoDB server to export our data and going to import into BigQuery tables. There are several other ways to import data into bigquery.&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;img alt="Google BigQuery" src="/images/bq-series/part1/bquery.svg"&gt;&lt;/p&gt;
&lt;p&gt;This blog is intended for audience who wanted to get into fundamentals of BigQuery (short for BQ) and related jobs needed to get your data inside BigQuery system.&lt;/p&gt;
&lt;h3&gt;&lt;a href="https://cloud.google.com/bigquery/what-is-bigquery"&gt;Google BigQuery&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;What ?&lt;/strong&gt; - BigQuery is Google's fully managed, petabyte scale, low cost analytics data warehouse.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Why ?&lt;/strong&gt; - BigQuery is NoOps—there is no infrastructure to manage and you don't need a database administrator—so you can focus on analyzing data to find meaningful insights, use familiar SQL, and take advantage of our pay-as-you-go model.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How ?&lt;/strong&gt; - Signup to &lt;a href="https://bigquery.cloud.google.com/"&gt;Google Cloud platform - GCP&lt;/a&gt; using your google account, start loading your data and leverage the power of this NoOps system.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Terminology in BigQuery&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Dataset&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;A dataset is contained within a specific project. Datasets enable you to organize and control access to your tables. A table must belong to a dataset, so you need to create at least one dataset before loading data into BigQuery.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tables&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;A BigQuery table contains individual records organized in rows, and a data type assigned to each column (also called a field).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Schema&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Each Every table is defined by a schema that describes field names, types, and other information. You can specify the schema of a table during the initial table creation request, or you can create a table without a schema and declare the schema in the query or load job that first populates the table. If you need to change the schema later, you can update the schema.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Loading Data into BigQuery&lt;/h3&gt;
&lt;p&gt;In this post, we are going to use a mongoDB server to export our data and going to import into BQ. There are several other ways to import data into BQ.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h4&gt;1. Export data from MongoDB&lt;/h4&gt;
&lt;p&gt;&lt;img alt="MongoDB" src="/images/bq-series/part1/mongo-db-blog.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;In this example, i have a database in mongoDB server with name &lt;code&gt;restaurantdb&lt;/code&gt; with collection name &lt;code&gt;restaurantCollection&lt;/code&gt;. We are going to export using &lt;code&gt;mongoexport&lt;/code&gt; binary available with mongodb server tools.&lt;/p&gt;
&lt;p&gt;&lt;img alt="MongoDB server" src="/images/bq-series/part1/mongoexport.png"&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ mongoexport -d restaurantdb -c restaurantCollection -o restaurant.json
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Once export is done, we can see content of &lt;code&gt;restaurant.json&lt;/code&gt; file&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ head -n &lt;span class="m"&gt;1&lt;/span&gt; restaurant.json

&lt;span class="o"&gt;{&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; : &lt;span class="o"&gt;{&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$oid&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;55f14312c7447c3da7051b26&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;}&lt;/span&gt;, &lt;span class="se"&gt;\&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;URL&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;http://www.just-eat.co.uk/restaurants-cn-chinese-cardiff/menu&amp;quot;&lt;/span&gt;, &lt;span class="se"&gt;\&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;address&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;228 City Road&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;.CN Chinese&amp;quot;&lt;/span&gt;, &lt;span class="se"&gt;\&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;outcode&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;CF24&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;postcode&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;3JH&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;type_of_food&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;Chinese&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;}&lt;/span&gt;

//pretty json
&lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;.CN Chinese&amp;quot;&lt;/span&gt;,
    &lt;span class="s2"&gt;&amp;quot;URL&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;http://www.just-eat.co.uk/restaurants-cn-chinese-cardiff/menu&amp;quot;&lt;/span&gt;,
    &lt;span class="s2"&gt;&amp;quot;outcode&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;CF24&amp;quot;&lt;/span&gt;,
    &lt;span class="s2"&gt;&amp;quot;postcode&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;3JH&amp;quot;&lt;/span&gt;,
    &lt;span class="s2"&gt;&amp;quot;address&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;228 City Road&amp;quot;&lt;/span&gt;,
    &lt;span class="s2"&gt;&amp;quot;_id&amp;quot;&lt;/span&gt;: &lt;span class="o"&gt;{&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$oid&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;55f14312c7447c3da7051b26&amp;quot;&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;,
    &lt;span class="s2"&gt;&amp;quot;type_of_food&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;Chinese&amp;quot;&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;hr&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h4&gt;2. Prepare schema for Table&lt;/h4&gt;
&lt;p&gt;Now we have our data ready in json format to be imported into BQ table. We need schema to design in order to import these records. Schema is skeleton of each field with datatype and the field not described in schema will not be imported. We have given all fields as &lt;strong&gt;NULLABLE&lt;/strong&gt; ie, if field didn't came in any records BQ will define null value.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ cat restaurantSchema.json
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;[
    {
        &amp;quot;name&amp;quot;: &amp;quot;name&amp;quot;,
        &amp;quot;type&amp;quot;: &amp;quot;STRING&amp;quot;,
        &amp;quot;mode&amp;quot;: &amp;quot;NULLABLE&amp;quot;
    },
    {
        &amp;quot;name&amp;quot;: &amp;quot;URL&amp;quot;,
        &amp;quot;type&amp;quot;: &amp;quot;STRING&amp;quot;,
        &amp;quot;mode&amp;quot;: &amp;quot;NULLABLE&amp;quot;
    },
    {
        &amp;quot;name&amp;quot;: &amp;quot;address&amp;quot;,
        &amp;quot;type&amp;quot;: &amp;quot;STRING&amp;quot;,
        &amp;quot;mode&amp;quot;: &amp;quot;NULLABLE&amp;quot;
    },
    {
        &amp;quot;name&amp;quot;: &amp;quot;outcode&amp;quot;,
        &amp;quot;type&amp;quot;: &amp;quot;STRING&amp;quot;,
        &amp;quot;mode&amp;quot;: &amp;quot;NULLABLE&amp;quot;
    },
    {
        &amp;quot;name&amp;quot;: &amp;quot;postcode&amp;quot;,
        &amp;quot;type&amp;quot;: &amp;quot;STRING&amp;quot;,
        &amp;quot;mode&amp;quot;: &amp;quot;NULLABLE&amp;quot;
    },
    {
        &amp;quot;name&amp;quot;: &amp;quot;type_of_food&amp;quot;,
        &amp;quot;type&amp;quot;: &amp;quot;STRING&amp;quot;,
        &amp;quot;mode&amp;quot;: &amp;quot;NULLABLE&amp;quot;
    }
]
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Reference for schema : &lt;a href="https://cloud.google.com/bigquery/docs/schemas"&gt;https://cloud.google.com/bigquery/docs/schemas&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h4&gt;3. Google Cloud SDK Installation&lt;/h4&gt;
&lt;p&gt;We have schema, data ready to be imported in BQ, but in order to talk to APIs of GCP services, we need Cloud SDK to be installed in our system. We are going to use &lt;code&gt;gcloud&lt;/code&gt; CLI tools in order to interact with our Cloud services running in GCP.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Installation : &lt;/strong&gt; &lt;a href="https://cloud.google.com/sdk/"&gt;https://cloud.google.com/sdk/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Once installation is done, run following command to verify account setup.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ gcloud auth list

$ gcloud config list
&lt;/pre&gt;&lt;/div&gt;


&lt;hr&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h4&gt;4. Create BigQuery Dataset&lt;/h4&gt;
&lt;p&gt;Go to your BigQuery in google console. &lt;a href="https://bigquery.cloud.google.com"&gt;https://bigquery.cloud.google.com&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Follow below instruction to create dataset.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Step 1:&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img alt="Create Dataset Step 1" src="/images/bq-series/part1/create-ds-1.png"&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Step 2:&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img alt="Create Dataset Step 2" src="/images/bq-series/part1/create-ds-2.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;To create dataset through &lt;code&gt;bq&lt;/code&gt; command line interface.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ bq mk -d --data_location&lt;span class="o"&gt;=&lt;/span&gt;US   BQ_Dataset

// Verify your dataset creation
$ bq ls

    datasetId
 ----------------
  BQ_Dataset
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;Read :&lt;/strong&gt; &lt;a href="https://cloud.google.com/bigquery/docs/datasets#create-dataset"&gt;https://cloud.google.com/bigquery/docs/datasets#create-dataset&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h4&gt;5. Load data into BigQuery&lt;/h4&gt;
&lt;p&gt;Now, my directory consists two files ie, data and schema.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ tree
.
├── restaurant.json
└── restaurantSchema.json

&lt;span class="m"&gt;0&lt;/span&gt; directories, &lt;span class="m"&gt;2&lt;/span&gt; files
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Run command to load data into BQ.  Once you submit load job, it will take seconds to minute depends on size of data you are importing into BQ table.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;Ex&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;bq&lt;/span&gt; &lt;span class="n"&gt;load&lt;/span&gt; &lt;span class="o"&gt;--&lt;/span&gt;&lt;span class="n"&gt;project_id&lt;/span&gt;&lt;span class="o"&gt;=&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;PROJECT&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;ID&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="o"&gt;--&lt;/span&gt;&lt;span class="n"&gt;source_format&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;NEWLINE_DELIMITED_JSON&lt;/span&gt; &lt;span class="o"&gt;\&lt;/span&gt;
    &lt;span class="n"&gt;mydataset&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;mytable&lt;/span&gt; &lt;span class="o"&gt;./&lt;/span&gt;&lt;span class="n"&gt;myfile&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;json&lt;/span&gt; &lt;span class="o"&gt;./&lt;/span&gt;&lt;span class="n"&gt;myschema&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;json&lt;/span&gt;

&lt;span class="n"&gt;$&lt;/span&gt; &lt;span class="n"&gt;bq&lt;/span&gt; &lt;span class="n"&gt;load&lt;/span&gt; &lt;span class="o"&gt;--&lt;/span&gt;&lt;span class="n"&gt;project_id&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;mimetic&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;slate&lt;/span&gt;   &lt;span class="o"&gt;\&lt;/span&gt;
    &lt;span class="o"&gt;--&lt;/span&gt;&lt;span class="n"&gt;source_format&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;NEWLINE_DELIMITED_JSON&lt;/span&gt; &lt;span class="o"&gt;--&lt;/span&gt;&lt;span class="n"&gt;max_bad_records&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt; &lt;span class="o"&gt;\&lt;/span&gt;
    &lt;span class="n"&gt;BQ_Dataset&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;Restaurant&lt;/span&gt; &lt;span class="o"&gt;./&lt;/span&gt;&lt;span class="n"&gt;restaurant&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;json&lt;/span&gt;  &lt;span class="o"&gt;./&lt;/span&gt;&lt;span class="n"&gt;restaurantSchema&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;json&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;code&gt;--max_bad_records 10&lt;/code&gt; are additional flags to allow 10 bad records while importing your job, exceeding this value will result in import failure.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Import through Cloud Storage&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="Cloud Storage" src="/images/bq-series/part1/google-cloud-storage.png"&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Another methods to import the data through &lt;code&gt;Cloud Storage&lt;/code&gt;, this method is lot faster compared to above one.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Read : &lt;/strong&gt; &lt;a href="https://cloud.google.com/storage/docs/creating-buckets"&gt;https://cloud.google.com/storage/docs/creating-buckets&lt;/a&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;//to create Cloud storage bucket for this example.
$ gsutil mb  gs://bq-storage

//to verify bucket creation
$ gsutil ls
gs://bq-storage/
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now , we will upload our data &lt;code&gt;restaurant.json&lt;/code&gt; to storage in bucket &lt;code&gt;gs://bq-storage/&lt;/code&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;//Run command to upload
$ gsutil cp restaurant.json gs://bq-storage/restaurant.json
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now, we can use storage path of object to import into BQ tables.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ bq load --project_id&lt;span class="o"&gt;=&lt;/span&gt;mimetic-slate  &lt;span class="se"&gt;\&lt;/span&gt;
    --source_format&lt;span class="o"&gt;=&lt;/span&gt;NEWLINE_DELIMITED_JSON --max_bad_records &lt;span class="m"&gt;10&lt;/span&gt;  &lt;span class="se"&gt;\&lt;/span&gt;
    BQ_Dataset.Restaurant &lt;span class="se"&gt;\&lt;/span&gt;
    gs://bq-storage/restaurant.json ./restaurantSchema.json
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;p&gt;After &lt;code&gt;bq load&lt;/code&gt; finished, run following command to verify the table creation.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ bq show BQ_Dataset.Restaurant
&lt;/pre&gt;&lt;/div&gt;


&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Output will be similar to this :&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img alt="bq show" src="/images/bq-series/part1/bq-show.png"&gt;&lt;/p&gt;
&lt;p&gt;You can also verify in bigQuery UI after hitting refresh. Visit to table and click &lt;em&gt;preview&lt;/em&gt;. You will start seeing records in table.&lt;/p&gt;
&lt;p&gt;&lt;img alt="bq show" src="/images/bq-series/part1/table-preview.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;More into bq CLI :&lt;/strong&gt; &lt;a href="https://cloud.google.com/storage/docs/creating-buckets"&gt;https://cloud.google.com/bigquery/bq-command-line-tool&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;More into gsutil CLI :&lt;/strong&gt; &lt;a href="https://cloud.google.com/storage/docs/quickstart-gsutil"&gt;https://cloud.google.com/storage/docs/quickstart-gsutil&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;SQL Query in BQ Table&lt;/h3&gt;
&lt;p&gt;We are going to run a simple query to show the output.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;SELECT
  name,
  address
FROM
  [mimetic-slate:BQ_Dataset.Restaurant]
WHERE
  type_of_food = &amp;#39;Thai&amp;#39;
GROUP BY
  name, address
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="query" src="/images/bq-series/part1/query.png"&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Github reference&lt;/strong&gt; : &lt;a href="https://github.com/sunnykrGupta/Bigquery-series"&gt;https://github.com/sunnykrGupta/Bigquery-series&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;BigQuery is a query service that allows you to run SQL-like queries against
multiple terabytes of data in a matter of seconds. The technology is one of the Google’s core technologies, like MapReduce and Bigtable, and has been used
by Google internally for various analytic tasks since 2006.&lt;/li&gt;
&lt;li&gt;While MapReduce is suitable for long-running batch processes such as data
mining, BigQuery is the best choice for ad hoc OLAP/BI queries that require
results as fast as possible.&lt;/li&gt;
&lt;li&gt;Wildcard can also be applied into Bigquery tables to expand your computations to multiple tables.&lt;/li&gt;
&lt;li&gt;BigQuery is the cloud-powered massively parallel
query database that provides extremely high full-scan query performance
and cost effectiveness compared to traditional data warehouse solutions
and appliances&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Next from here:&lt;/strong&gt; Play around with more &lt;a href="https://cloud.google.com/bigquery/docs/how-to"&gt;functionality available in BigQuery&lt;/a&gt; and dive into it for more computation hungry jobs.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;p&gt;That's all from this &lt;strong&gt;series Part-I&lt;/strong&gt;. Hope you get basic understanding of import jobs, storage and basic outline of BigQuery from this post. I have seen power of BigQuery in my workplace to crunch 100-120 TB of data and getting results in minute or two, its really incredibly awesome. I would appreciate a feedback via comments available below and claps on medium.&lt;/p&gt;
&lt;p&gt;In next blog which is part of this series, i will be covering more into &lt;em&gt;Streaming feature available in Bigquery&lt;/em&gt; to push data in BQ tables in real-time to make it available for instant query on changing dataset.&lt;/p&gt;
&lt;h5&gt;Medium Blog : &lt;a href="https://medium.com/@sunnykrgupta/export-load-job-with-mongodb-bigquery-part-i-64a00eb5266b"&gt;medium.com/@sunnykrgupta/export-load-job-with-mongodb-bigquery-part-i&lt;/a&gt;&lt;/h5&gt;</content><category term="Google BigQuery"></category><category term="GCP"></category><category term="MongoDB"></category><category term="Cloud Storage"></category><category term="Google Cloud Platform"></category></entry><entry><title>Installation of VNC server on Ubuntu</title><link href="https://sunnykrGupta.github.io/installation-of-vnc-server-on-ubuntu.html" rel="alternate"></link><published>2017-09-04T22:51:50+05:30</published><updated>2017-09-04T22:51:50+05:30</updated><author><name>Sunny Kr Gupta</name></author><id>tag:sunnykrgupta.github.io,2017-09-04:/installation-of-vnc-server-on-ubuntu.html</id><summary type="html">&lt;p&gt;This blog is intended for people who wanted to install GUI or desktop environment on linux &lt;a href="https://en.wikipedia.org/wiki/Server_(computing)"&gt;servers&lt;/a&gt; running on &lt;a href="https://en.wikipedia.org/wiki/Cloud_computing"&gt;cloud&lt;/a&gt; and connect.&lt;/p&gt;
&lt;p&gt;We are going to use VNC (Virtual Network Computing) protocol for accessing our remote desktop server.&lt;/p&gt;
&lt;h4&gt;What is VNC ?&lt;/h4&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Virtual_Network_Computing"&gt;Virtual network computing&lt;/a&gt;, or VNC, is a graphical desktop …&lt;/p&gt;</summary><content type="html">&lt;p&gt;This blog is intended for people who wanted to install GUI or desktop environment on linux &lt;a href="https://en.wikipedia.org/wiki/Server_(computing)"&gt;servers&lt;/a&gt; running on &lt;a href="https://en.wikipedia.org/wiki/Cloud_computing"&gt;cloud&lt;/a&gt; and connect.&lt;/p&gt;
&lt;p&gt;We are going to use VNC (Virtual Network Computing) protocol for accessing our remote desktop server.&lt;/p&gt;
&lt;h4&gt;What is VNC ?&lt;/h4&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Virtual_Network_Computing"&gt;Virtual network computing&lt;/a&gt;, or VNC, is a graphical desktop sharing system that allows you to control one computer remotely from another. A VNC server transfers keyboard and mouse events, and displays the remote host’s screen via a network connection, which allows you to operate a full &lt;a href="https://en.wikipedia.org/wiki/Desktop_environment"&gt;desktop environment&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Basically ubuntu server and ubuntu cloud editions does not contains GUI, which needs to be installed before installing VNC server. Please note that server and cloud editions are carefully designed to utilize less hardware resources ( minimal environment ), installing GUI might leads to high hardware utilization.&lt;/p&gt;
&lt;h4&gt;Why I needed desktop environment in remote server ?&lt;/h4&gt;
&lt;p&gt;Just to explain a use case, let me tell me you how I ended up using VNC in first place. I was working on a problem which relates with cloud latency testing. My Friend, &lt;a href="https://www.linkedin.com/in/neekneeraj/"&gt;Neeraj&lt;/a&gt; &lt;em&gt;(whose work revolves around core JS research &amp;amp; development)&lt;/em&gt; developed a javascript code that makes an cross origin &lt;a href="(https://en.wikipedia.org/wiki/Web_API)"&gt;HTTP API&lt;/a&gt; call to a &lt;a href="https://en.wikipedia.org/wiki/Cloud_load_balancing"&gt;loadbalancer&lt;/a&gt; near to the geographical location of browser and response will be delivered from loadbalancer in geographic proximity. To test this setup, executing JS code and to use &lt;a href="https://developer.mozilla.org/en/docs/Tools/Browser_Console"&gt;developer console&lt;/a&gt; to see what's happening under the network layer, we were in need of a browser engine in different geographical location. I could have used some online paid or free service to get browser rented, services like &lt;a href="https://www.browserstack.com/"&gt;browserstack&lt;/a&gt; or other alternatives but that has free minutes based trial restrictions.&lt;/p&gt;
&lt;hr&gt;
&lt;h3&gt;Install a Desktop and VNC Server on Ubtunu 14.04&lt;/h3&gt;
&lt;h4&gt;Step 1 - Install Ubuntu desktop&lt;/h4&gt;
&lt;p&gt;Start installing below &lt;em&gt;gnome packages&lt;/em&gt; which helps VNC to load properly . These packages are required for all editions including &lt;em&gt;ubuntu desktop&lt;/em&gt; .&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ sudo apt-get install --no-install-recommends ubuntu-desktop gnome-panel gnome-settings-daemon metacity nautilus gnome-terminal gnome-core
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Step 2 - Install vnc4server package.&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ sudo apt-get install vnc4server
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Step 3 - Make configuration changes in vncserver&lt;/h4&gt;
&lt;p&gt;Open &lt;code&gt;/usr/bin/vncserver&lt;/code&gt; file and edit as follows . Before editing, make a backup copy.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ sudo cp /usr/bin/vncserver /usr/bin/vncserver.bkp

$ sudo vim /usr/bin/vncserver

&lt;span class="c1"&gt;#Find this line &amp;quot;# exec /etc/X11/xinit/xinitrcnn&amp;quot;.&lt;/span&gt;
&lt;span class="c1"&gt;#and add these lines like below&lt;/span&gt;

    &lt;span class="s2"&gt;&amp;quot;# exec /etc/X11/xinit/xinitrcnn&amp;quot;&lt;/span&gt;.
       &lt;span class="s2"&gt;&amp;quot;gnome-panel &amp;amp;n&amp;quot;&lt;/span&gt;.
       &lt;span class="s2"&gt;&amp;quot;gnome-settings-daemon &amp;amp;n&amp;quot;&lt;/span&gt;.
       &lt;span class="s2"&gt;&amp;quot;metacity &amp;amp;n&amp;quot;&lt;/span&gt;.
       &lt;span class="s2"&gt;&amp;quot;nautilus &amp;amp;n&amp;quot;&lt;/span&gt;.
       &lt;span class="s2"&gt;&amp;quot;gnome-terminal &amp;amp;n&amp;quot;&lt;/span&gt;.
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Step 4 - Start your vncserver&lt;/h4&gt;
&lt;p&gt;Now type the command &lt;code&gt;vncserver&lt;/code&gt; to start VNC session. you will be prompted for creating new vnc password.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ vncserver
You will require a password to access your desktops through VNC Clients.
Password:******
Verify:******

xauth: file /root/.Xauthority does not exist
New &lt;span class="s1"&gt;&amp;#39;ubuntu-desktop:1 (root)&amp;#39;&lt;/span&gt; desktop is ubuntu-desktop:1

Starting applications specified in /root/.vnc/xstartup
Log file is /root/.vnc/ubuntu-desktop:1.log
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Step 5 - To check VNC server has started, follow&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ netstat -tulpn

Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        &lt;span class="m"&gt;0&lt;/span&gt;      &lt;span class="m"&gt;0&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt;.0.0.0:6001            &lt;span class="m"&gt;0&lt;/span&gt;.0.0.0:*               LISTEN      &lt;span class="m"&gt;28372&lt;/span&gt;/Xvnc4
tcp6       &lt;span class="m"&gt;0&lt;/span&gt;      &lt;span class="m"&gt;0&lt;/span&gt; :::5901                 :::*                    LISTEN      &lt;span class="m"&gt;28372&lt;/span&gt;/Xvnc4
&lt;/pre&gt;&lt;/div&gt;


&lt;blockquote&gt;
&lt;p&gt;VNC server is running and listening on &lt;strong&gt;5901 port&lt;/strong&gt;. Make sure your firewall allows &lt;strong&gt;inbound&lt;/strong&gt; TCP connection to this port.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;Step 6 - Configure your Firewall&lt;/h4&gt;
&lt;p&gt;If &lt;strong&gt;firewall&lt;/strong&gt; is active, you need to open ports for inbound communication. If no firewall is enabled, you can skip this section.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;#allow SSH&lt;/span&gt;
$ sudo ufw allow OpenSSH

&lt;span class="c1"&gt;#allowing single port 5901 port&lt;/span&gt;
$ sudo ufw allow &lt;span class="m"&gt;5901&lt;/span&gt;/tcp

&lt;span class="c1"&gt;#To allow series of port 5901 - 5910, follow&lt;/span&gt;
$ sudo ufw allow &lt;span class="m"&gt;5901&lt;/span&gt;:5910/tcp

&lt;span class="c1"&gt;#To check firewall rules&lt;/span&gt;
$ sudo  ufw status verbose

Status: active
Logging: on &lt;span class="o"&gt;(&lt;/span&gt;low&lt;span class="o"&gt;)&lt;/span&gt;
Default: deny &lt;span class="o"&gt;(&lt;/span&gt;incoming&lt;span class="o"&gt;)&lt;/span&gt;, allow &lt;span class="o"&gt;(&lt;/span&gt;outgoing&lt;span class="o"&gt;)&lt;/span&gt;, disabled &lt;span class="o"&gt;(&lt;/span&gt;routed&lt;span class="o"&gt;)&lt;/span&gt;
New profiles: skip

To                         Action      From
--                         ------      ----
&lt;span class="m"&gt;22&lt;/span&gt;/tcp &lt;span class="o"&gt;(&lt;/span&gt;OpenSSH&lt;span class="o"&gt;)&lt;/span&gt;           ALLOW IN    Anywhere
&lt;span class="m"&gt;5901&lt;/span&gt;:5910/tcp              ALLOW IN    Anywhere
&lt;span class="m"&gt;22&lt;/span&gt;/tcp &lt;span class="o"&gt;(&lt;/span&gt;OpenSSH &lt;span class="o"&gt;(&lt;/span&gt;v6&lt;span class="o"&gt;))&lt;/span&gt;      ALLOW IN    Anywhere &lt;span class="o"&gt;(&lt;/span&gt;v6&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="m"&gt;5901&lt;/span&gt;:5910/tcp &lt;span class="o"&gt;(&lt;/span&gt;v6&lt;span class="o"&gt;)&lt;/span&gt;         ALLOW IN    Anywhere &lt;span class="o"&gt;(&lt;/span&gt;v6&lt;span class="o"&gt;)&lt;/span&gt;WW
&lt;/pre&gt;&lt;/div&gt;


&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://www.digitalocean.com/community/tutorials/how-to-set-up-a-firewall-with-ufw-on-ubuntu-14-04"&gt;Good reads on configuring UFW firewall&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;Step 7 - Connect to VNC Server&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Use any remote desktop connect client that allow VNC protocol. Use &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/IP_address"&gt;IP address&lt;/a&gt;&lt;/em&gt; of server along with port where VNC server is listening.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div style="text-align: right"&gt;&lt;sub&gt;Connect -Remote Desktop Viewer&lt;/sub&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="VNC Connect" src="/images/vnc/vnc-connect.png" title="VNC Connect"&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Once connected to your VNC server, you will see screen of remote server where you installed desktop GUI.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div style="text-align: right"&gt;&lt;sub&gt;Launch Firefox from Terminal&lt;/sub&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="Launch Firefox" src="/images/vnc/vnc-launch-firefox.png" title="Launch Firefox"&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Browser screen running on remote server.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div style="text-align: right"&gt;&lt;sub&gt;Google UK&lt;/sub&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="Google UK" src="/images/vnc/vnc-google-uk.png" title="Google UK"&gt;&lt;/p&gt;
&lt;p&gt;That’s it, your VNC server is working.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Here I created my linux server in london, UK. I opened firefox through terminal to reach out to URL &lt;em&gt;google.com&lt;/em&gt;. It opened google.co.uk domain based on regional search engine. You can do lot of other stuffs on VNC protocol to get things done from remote location.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h5&gt;Medium Blog : &lt;a href="https://medium.com/@sunnykrgupta/installation-of-vnc-server-on-ubuntu-1cf035370bd3"&gt;medium.com/@sunnykrgupta/installation-of-vnc-server-on-ubuntu-1cf035370bd3&lt;/a&gt;&lt;/h5&gt;</content><category term="vnc"></category><category term="vncserver"></category><category term="ubuntu"></category><category term="desktop installation"></category></entry><entry><title>Managing fleet on Kubernetes</title><link href="https://sunnykrGupta.github.io/managing-fleet-on-kubernetes.html" rel="alternate"></link><published>2017-08-13T19:45:00+05:30</published><updated>2017-08-13T19:45:00+05:30</updated><author><name>Sunny Kr Gupta</name></author><id>tag:sunnykrgupta.github.io,2017-08-13:/managing-fleet-on-kubernetes.html</id><summary type="html">&lt;blockquote&gt;
&lt;p&gt;Couple of months ago, we were tackling challenges with scalability of system and were in pursuit of finding right orchestration tools which can help in scaling system quickly. This draft is outline of things we have tried and learned along the way, , most of things might sound familiar to you …&lt;/p&gt;&lt;/blockquote&gt;</summary><content type="html">&lt;blockquote&gt;
&lt;p&gt;Couple of months ago, we were tackling challenges with scalability of system and were in pursuit of finding right orchestration tools which can help in scaling system quickly. This draft is outline of things we have tried and learned along the way, , most of things might sound familiar to you. A Quick glance of things we came across while building fleet on Kubernetes.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We started exploring popular project managed by Google for orchestration management, &lt;strong&gt;&lt;a href="https://kubernetes.io/"&gt;Kubernetes&lt;/a&gt;&lt;/strong&gt; for DevOps. Starting with two weeks of learning curves, we get our working staging system in &lt;em&gt;kubes&lt;/em&gt; (kubernetes in short) and did small working setup to visualize the power of this orchestration framework.&lt;/p&gt;
&lt;hr&gt;
&lt;h4&gt;Microservices&lt;/h4&gt;
&lt;p&gt;Microservice architectures have been trending because its architectural style aims to tackle the problems of managing modern application by decoupling software solutions into smaller functional services that are expected to fail.&lt;/p&gt;
&lt;p&gt;This help in quick recovery from failure on smaller functional units in contrast to making recovery from big monolithic software systems. Microservices helps in making your release cycle faster even because you will be focusing on smaller changes in single app instead of pushing code changes in bigger software systems that has multiple dependencies.&lt;/p&gt;
&lt;hr&gt;
&lt;h4&gt;Containers&lt;/h4&gt;
&lt;p&gt;Microservice architectures got a big tide in 2013 when Docker inc. released Docker technology. &lt;strong&gt;Docker container&lt;/strong&gt; gave perfect alternatives to virtual machines and drove software packaging methods in a more developer friendly way. Docker container are comparatively smaller than virtual machines (VMs). Its shares underlying host OS resources, we can spin up hundreds of these small units in order of milliseconds. Their smaller size helps in faster packaging, testing and even deployments because of its portable nature.&lt;/p&gt;
&lt;p&gt;Docker’s container-based platform allows highly portable workloads. Docker containers can run on a developer’s local laptop, on physical or virtual machines in a data center, on cloud providers, or in a mixture of environments.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;We started with &lt;code&gt;Google Container Engine (GCE)&lt;/code&gt; to get things work quickly. We started with a cluster with few &lt;code&gt;10's of Nodes&lt;/code&gt;, each Node with configuration &lt;code&gt;12 vCore and 30 GB&lt;/code&gt; in &lt;strong&gt;&lt;a href="https://cloud.google.com/container-engine/docs/node-pools"&gt;default pool&lt;/a&gt;&lt;/strong&gt; to run stateless components.&lt;/p&gt;
&lt;p&gt;Before going in depth, we needs some &lt;em&gt;gears&lt;/em&gt; (concepts/tools/theory) to onboard into container ship and sail out for cruise.&lt;/p&gt;
&lt;p&gt;We are dividing gears that we need to know into two parts, ie, first will be &lt;code&gt;Docker&lt;/code&gt; and second will focused on &lt;code&gt;Kubernetes&lt;/code&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h4&gt;Part - I (Understanding Docker at Dock)&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Stateless and stateful components.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In computing, a stateless protocol is a communication protocol in which no information is retained by either sender or receiver. The sender transmits a packet to the receiver and does not expect an acknowledgment of receipt. A UDP connection-oriented session is a stateless connection because neither systems maintains information about the session during its life.&lt;/li&gt;
&lt;li&gt;In contrast, a protocol that requires keeping of the internal state on the server is known as a stateful protocol. A TCP connection-oriented session is a 'stateful' connection because both systems maintain information about the session itself during its life.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.redhat.com/en/containers"&gt;Understanding containerization concept&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Container provides operating system-level virtualization through a virtual environment that has its own process and network space, instead of creating a full-fledged &lt;a href="https://en.wikipedia.org/wiki/Virtual_machine"&gt;virtual machine&lt;/a&gt;. This enables the kernel of an operating system to allow the existence of multiple isolated user-space instances, instead of just one.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://docs.docker.com/engine/userguide/eng-image/dockerfile_best-practices/"&gt;Writing good Dockerfile for modules&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Dockerfile is set of instruction used by Docker to build an image. Containers are created using docker images, which can be built either by executing commands manually or automatically through Dockerfile. Docker achieves this by creating safe, &lt;strong&gt;LXC&lt;/strong&gt; (i.e. Linux Containers) based environments for applications called “docker containers”.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Writing optimized Dockerfile, understanding order of commands. Each command that we run in Dockerfile is executed as a layer and subsequent command will be build on top of previous layer. Each layer is managed in cache by Docker tool. Docker manages cache itself to reuse layer of previously build Docker images to save time &amp;amp; disk.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;I have three file in my directory named &amp;#39;flask&amp;#39; :
➜  flask

── app.py
── Dockerfile
── requirement.txt
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;##### cat app.py&lt;/span&gt;

&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;flask&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Flask&lt;/span&gt;
&lt;span class="c1"&gt;#from flask import render_template, request&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;json&lt;/span&gt;

&lt;span class="n"&gt;app&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Flask&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="vm"&gt;__name__&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="nd"&gt;@app.route&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;/&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;hello&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Welcome to Python Flask!&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="vm"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;__main__&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;app&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;host&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;0.0.0.0&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;port&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;5009&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;###### cat requirement.txt

Flask==0.10.1
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;###### cat Dockerfile

FROM frolvlad/alpine-python2

RUN mkdir /etc/flask

ADD app.py /etc/flask/app.py
ADD requirement.txt /etc/flask/requirement.txt


CMD [&amp;quot;python&amp;quot;, &amp;quot;/etc/flask/app.py&amp;quot;]
&lt;/pre&gt;&lt;/div&gt;


&lt;blockquote&gt;
&lt;p&gt;Lets build our docker image with name &lt;strong&gt;flask-v1&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;➜  docker build -t flask-v1 .

Sending build context to Docker daemon 4.096 kB

Step 1/5 : FROM frolvlad/alpine-python2
latest: Pulling from frolvlad/alpine-python2
627beaf3eaaf: Pull complete
79d39e719c2e: Pull complete
Digest: sha256:47e3f85dadf401d51c6f74a18d4f693c2157692292e6dae0a078f37499a183ee
Status: Downloaded newer image for frolvlad/alpine-python2:latest
 ---&amp;gt; 603e17608203

Step 2/5 : RUN mkdir /etc/flask
 ---&amp;gt; Running in 0d97b7a8986b
 ---&amp;gt; 9b2a858914e2
Removing intermediate container 0d97b7a8986b

Step 3/5 : ADD app.py /etc/flask/app.py
 ---&amp;gt; 1a4938be7722
Removing intermediate container f4d11b837e26

Step 4/5 : ADD requirement.txt /etc/flask/requirement.txt
 ---&amp;gt; 5d058851dd81
Removing intermediate container 08eef72a4051

Step 5/5 : CMD python /etc/flask/app.py
 ---&amp;gt; Running in 96490917e533
 ---&amp;gt; a081e6cbcf3c
Removing intermediate container 96490917e533

Successfully built a081e6cbcf3c
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now, we have made some modification in &lt;strong&gt;app.py&lt;/strong&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;##### cat app.py&lt;/span&gt;

&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;flask&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Flask&lt;/span&gt;
&lt;span class="c1"&gt;#from flask import render_template, request&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;json&lt;/span&gt;

&lt;span class="n"&gt;app&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Flask&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="vm"&gt;__name__&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="nd"&gt;@app.route&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;/&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;hello&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Welcome to Python Flask!&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;

&lt;span class="c1"&gt;#Added utility&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;utility&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;something&amp;quot;&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="vm"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;__main__&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;app&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;host&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;0.0.0.0&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;port&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;5009&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;blockquote&gt;
&lt;p&gt;Lets build our docker image with name &lt;strong&gt;flask-v2&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;➜  docker build -t flask-v2 .

Sending build context to Docker daemon 4.096 kB

Step 1/5 : FROM frolvlad/alpine-python2
 ---&amp;gt; 603e17608203

Step 2/5 : RUN mkdir /etc/flask
 ---&amp;gt; Using cache
 ---&amp;gt; 9b2a858914e2

Step 3/5 : ADD app.py /etc/flask/app.py
 ---&amp;gt; a7565514aab3
Removing intermediate container 360eb266a458

Step 4/5 : ADD requirement.txt /etc/flask/requirement.txt
 ---&amp;gt; 8441bca383f0
Removing intermediate container 15ebbb15d67d

Step 5/5 : CMD python /etc/flask/app.py
 ---&amp;gt; Running in 73e4bbcbe512
 ---&amp;gt; 0f4a5754f0d0
Removing intermediate container 73e4bbcbe512

Successfully built 0f4a5754f0d0
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;You will see only &lt;em&gt;step-2&lt;/em&gt; was taken from cache, rest of the instructions ran again because change has been detected by Docker on &lt;em&gt;step-3&lt;/em&gt; command ie, &lt;code&gt;ADD app.py&lt;/code&gt; , so all subsequent commands ran again to build layer on top of previous layer.&lt;/p&gt;
&lt;p&gt;Now we have made changes in &lt;code&gt;Dockerfile&lt;/code&gt;, some reorder of commands.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;# cat Dockerfile

FROM frolvlad/alpine-python2

RUN mkdir /etc/flask

# add requirement file first, then commands which contains some changes.
ADD requirement.txt /etc/flask/requirement.txt
ADD app.py /etc/flask/app.py

CMD [&amp;quot;python&amp;quot;, &amp;quot;/etc/flask/app.py&amp;quot;]
&lt;/pre&gt;&lt;/div&gt;


&lt;blockquote&gt;
&lt;p&gt;Lets build our docker image with name &lt;strong&gt;flask-v3&lt;/strong&gt; and lets see build console.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;➜  flask docker build -t flask-v3 .

Sending build context to Docker daemon 4.096 kB

Step 1/5 : FROM frolvlad/alpine-python2
 ---&amp;gt; 603e17608203

Step 2/5 : RUN mkdir /etc/flask
 ---&amp;gt; Using cache
 ---&amp;gt; 9b2a858914e2

Step 3/5 : ADD requirement.txt /etc/flask/requirement.txt
 ---&amp;gt; Using cache
 ---&amp;gt; db8fc95cebff

Step 4/5 : ADD app.py /etc/flask/app.py
 ---&amp;gt; 9fb8351616e1
Removing intermediate container 4e98534d5339

Step 5/5 : CMD python /etc/flask/app.py
 ---&amp;gt; Running in 139f8c4282d8
 ---&amp;gt; e3fae9852d94
Removing intermediate container 139f8c4282d8

Successfully built e3fae9852d94
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now, you can see only &lt;em&gt;step-2,3&lt;/em&gt; was taken from &lt;code&gt;cache&lt;/code&gt;, &lt;em&gt;step-4&lt;/em&gt; command ie, &lt;code&gt;ADD app.py&lt;/code&gt; build new layer because of change detected in &lt;code&gt;app.py&lt;/code&gt; file, and this build saved little bit of our time. This is really important in building big Docker images where we have bigger chain of command to build an app.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://docs.docker.com/engine/userguide/eng-image/dockerfile_best-practices/#each-container-should-have-only-one-concern"&gt;Running a single process inside a Docker container&lt;/a&gt;.&lt;ul&gt;
&lt;li&gt;“one process per container” is frequently a good rule of thumb, it is not a hard and fast rule. Use your best judgment to keep containers as clean and modular as possible - Docker&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Understanding remote Docker container registry for storing/pushing our locally built docker images, here we have used Google container registry (GCR) for docker image management.&lt;ul&gt;
&lt;li&gt;&lt;a href="https://cloud.google.com/container-registry/docs/pushing-and-pulling"&gt;Pushing and Pulling Images to GCR&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.docker.com/docker-cloud/builds/push-images/"&gt;Push images to Docker Cloud&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h4&gt;Part - II ( Understanding Kubernetes in Ocean )&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Learning basics of kubernetes &amp;amp; &lt;a href="https://kubernetes.io/docs/tutorials/kubernetes-basics/"&gt;work flow training&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Kubernetes is an open-source platform for automating deployment, scaling, and operations of application containers across clusters of hosts, providing container-centric infrastructure - Kubernetes.io&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://kubernetes.io/docs/concepts/workloads/pods/pod/#what-is-a-pod"&gt;What are Pods? How container run inside a pod?&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pods are the atomic unit on the Kubernetes platform. A Pod is a Kubernetes abstraction that represents a group of one or more application containers (such as Nginx or redis), and some shared resources for those containers.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div style="text-align: right"&gt;&lt;sub&gt;Pod Overview : Images by Kubernetes.io&lt;/sub&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="Pods overview image" src="/images/kubes/module_03_pods.svg" title="Pods Overview"&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://kubernetes.io/docs/concepts/nodes/node/#what-is-a-node"&gt;What are Nodes?&lt;/a&gt; &lt;sub&gt;(also known as worker or minion, a single machine)&lt;/sub&gt;&lt;ul&gt;
&lt;li&gt;A Pod always runs inside a Node. A Node is a worker machine in Kubernetes and may be either a virtual or a physical machine, depending on the cluster. Node is controlled by Kubernetes Master. Kubernetes manages scheduling of pods in Nodes running in a cluster.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div style="text-align: right"&gt;&lt;sub&gt;Node Overview : Images by Kubernetes.io&lt;/sub&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="Node overview image" src="/images/kubes/module_03_nodes.svg" title="Node Overview"&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/"&gt;What are deployments?&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;Deployments to create new resources, or replace existing ones by new ones by means of configuration defined. You can think of it as a supervisor of pods management.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;#### cat sample-deployment.yaml

apiVersion: apps/v1beta1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.7.9
      ports:
      - containerPort: 80
&lt;/pre&gt;&lt;/div&gt;


&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/"&gt;What is replication controller and Replica sets?&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A ReplicationController and Replica Sets ensures that a specified number of pod “replicas” are running at any one time. In other words, it makes sure that a pod or homogeneous set of pods are always up and available. If there are too many pods, it will kill some. If there are too few, it will start more.&lt;blockquote&gt;
&lt;p&gt;In above &lt;code&gt;yaml&lt;/code&gt; file, you can see &lt;code&gt;replicas&lt;/code&gt; keyword, this is being managed by replication utility.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://kubernetes.io/docs/concepts/overview/components/"&gt;What is Kubernetes master?&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The controlling services in a Kubernetes cluster are called the master, or control plane, components. For example, master components are responsible for making global decisions about the cluster (e.g., scheduling), and detecting and responding to cluster events (e.g., starting up a new pod when a replication controller’s ‘replicas’ field is unsatisfied). Kubernetes provides a REST API supporting primarily CRUD operations on (mostly) persistent resources, which serve as the hub of its control plane.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/architecture.md#architecture"&gt;Kubernetes Ecosystem consists of mutiple components.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://kubernetes.io/docs/concepts/services-networking/service/"&gt;What are services?&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A Kubernetes Service is an abstraction which defines a logical set of Pods and a policy by which to access them. The set of Pods targeted by a Service is (usually) determined by a Label Selector. Service keep on looking for pods which has specific labels assigned and keep tracks of those pods for request offloading.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div style="text-align: right"&gt;&lt;sub&gt;Service Overview : Images by Kubernetes.io&lt;/sub&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="Service overview image" src="/images/kubes/module_04_labels.svg" title="Service Overview"&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;###### cat sample-services.yaml

kind: Service
apiVersion: v1
metadata:
  name: my-service
spec:
  type: LoadBalancer
  loadBalancerIP: 10.10.10.1
  ports:
    # the port that this service should serve on
  - port: 80
    # port on which it should forward request ie, port pod is listening
    targetPort: 8080
  selector:
    # labels assigned to pods
    app: MyApp
&lt;/pre&gt;&lt;/div&gt;


&lt;ul&gt;
&lt;li&gt;&lt;a href="https://kubernetes.io/docs/user-guide/kubectl-cheatsheet/"&gt;How to debug or get cluster info from command-line?&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;code&gt;kubectl&lt;/code&gt; is a command line interface for running commands against Kubernetes clusters.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;-----General Commands

## To view Nodes in a cluster
$kubectl get nodes

NAME                                          STATUS    AGE
gke-test-cluster-default-pool-2d123aa1-012f   Ready     2d
gke-test-cluster-default-pool-2d123aa1-e23k   Ready     2d


## To view the Deployment we created run:
$    kubectl get deployments

NAME         DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
hello-node   1         1         1            1           3m


## To view the Pod created by the deployment run:
$    kubectl get pods

NAME                         READY     STATUS    RESTARTS   AGE
hello-node-714049816-ztzrb   1/1       Running   0          6m


## To view the Pod created by the deployment run:
$    kubectl get pods

NAME                         READY     STATUS    RESTARTS   AGE
hello-node-714049816-ztzrb   1/1       Running   0          6m

## To view detailed information of the Pod:
$ kubectl get pods -o wide
or
$ kubectl get pods --output=wide

NAME             READY     STATUS    RESTARTS   AGE       IP
dd-agent-0f75g   1/1       Running   0          23d       10.204.5.16


## To view the stdout / stderr from a Pod run:
$    kubectl logs &amp;lt;POD-NAME&amp;gt;

## To view metadata about the cluster run:
$    kubectl cluster-info
&lt;/pre&gt;&lt;/div&gt;


&lt;hr&gt;
&lt;h4&gt;How do we run containers in GCE ?&lt;/h4&gt;
&lt;p&gt;We have number of &lt;code&gt;deployments&lt;/code&gt; which manages scaling pods up/down depend on processing we need. Pods run containers inside Node available in a cluster. We need to follow proper versioning of modules to distinguish what is running inside your system and this helps in rollback releases in case of issues in production.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;How about services/APIs we need to expose ?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;There comes kubes &lt;code&gt;services&lt;/code&gt;. We have plenty of APIs we need to expose to outside world. To make it happen, we have couple of kube services exposed using tcp loadbalancer which has been assigned public IP. Internally, these services keeps on doing service discovery using &lt;code&gt;label selector&lt;/code&gt; to find pods and attach it to this service, pods having same label will be targeted by a service. Its same concept of how we manage loadbalancer on cloud, attach VMs to a loadbalancer to offload incoming traffic.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Resources running inside Kube ship knows each other very well. Each &lt;code&gt;services/pods&lt;/code&gt; can communicate by names assigned to each. Instead of using IPs (private) assigned to each of them, you can use names given as FQDN, its a good practise to use names instead of IPs because of dynamic nature of network resource allocation since resources get destroyed and created again in a container lifecycle management. Kube-DNS maintains all list of IPs internally assigned and helps finding resources by names.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h4&gt;&lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-ram-container/"&gt;How to decide what resources you should allocate to your pods resources&lt;/a&gt;?&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Convention
# RAM : Mi = MB, ie, 1024Mi is 1024 MB or 1GB
# CPU : m = milicpu , ie, 100m cpu is 100 milicpu, or say 0.1 CPU

apiVersion: v1
kind: Pod
metadata:
  name: cpu-ram-demo
spec:
  containers:
  - name: cpu-ram-demo-container
    image: gcr.io/google-samples/node-hello:1.0
    resources:
      requests:
        memory: &amp;quot;64Mi&amp;quot;
        cpu: &amp;quot;250m&amp;quot;
      limits:
        memory: &amp;quot;128Mi&amp;quot;
        cpu: &amp;quot;500m&amp;quot;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Each container has its own requirements of resources (ie, CPU, RAM, disk, network etc), there comes requests &amp;amp; limits in kubes. This helps in keeping your nodes healthy. Many times due to bad limits or not defining limits, your pods can go crazy at utilization, they might eat any resources, can lead to node starvation that results in Nodes becomes unhealthy and goes in &lt;code&gt;[Not Ready]&lt;/code&gt; state due to resource exhaustion. We faced this multiple times at early stage and now we have fine tuned each pods resources based on its hunger behaviour.&lt;/p&gt;
&lt;h5&gt;How to define Node resources in kubernetes cluster?&lt;/h5&gt;
&lt;p&gt;Depends on container type &lt;sub&gt;(which you are running inside a pod)&lt;/sub&gt;, you can define different Node pools. Suppose you have modules named &lt;code&gt;Core.X, Core.Y and Core.Z&lt;/code&gt; , all of them needs &lt;code&gt;2 core, 2 GB&lt;/code&gt; each to run, then you can have &lt;strong&gt;Standard Node Pool&lt;/strong&gt; to run them. In this case, i will allocate following config for my Node pool.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Name : Standard Pool&lt;/li&gt;
&lt;li&gt;Pool Size : 2&lt;/li&gt;
&lt;li&gt;Node Config: 4 Core, 4 GB&lt;/li&gt;
&lt;li&gt;Node Pool Resource : 8 Core, 8 GB&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Utilization&lt;/code&gt; : 6 Core, 6 GB (75 % used Core &amp;amp; RAM)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now, lets say i have high memory eater modules. let call them &lt;code&gt;Mem.X, Mem.Y and Mem.Z&lt;/code&gt; , all of them needs &lt;code&gt;0.5 core, 4 GB&lt;/code&gt; each to run, then you need &lt;strong&gt;High memory Node Pool&lt;/strong&gt; to run them. In this case, i will allocate different config for my Node pool.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Name : HighMem Pool&lt;/li&gt;
&lt;li&gt;Pool Size : 2&lt;/li&gt;
&lt;li&gt;Node Config : 1 Core, 8 GB&lt;/li&gt;
&lt;li&gt;Node Pool Resource : 2 Core, 16 GB&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Utilization&lt;/code&gt; : 1.5 Core, 12 GB (75 % used Core &amp;amp; RAM)&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;So, based on your &lt;a href="https://cloud.google.com/container-engine/docs/node-pools"&gt;Node pool type&lt;/a&gt;, you can deploy your pods in different Node pools by using &lt;code&gt;nodeSelector&lt;/code&gt; in kubes.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;-------------- Node selector example

apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    env: test
spec:
  containers:
  - name: nginx-v1
    image: nginx
  nodeSelector:
    #give label assigned to your node pool
    cloud.google.com/gke-nodepool: high-mem-pool
&lt;/pre&gt;&lt;/div&gt;


&lt;hr&gt;
&lt;h4&gt;How we monitor Kubernetes ?&lt;/h4&gt;
&lt;p&gt;We can run custom monitoring setup to keep an eye on Nodes. You can run &lt;a href="https://github.com/kubernetes/heapster"&gt;heapster&lt;/a&gt;, ie. responsible for compute resource usage analysis and monitoring of container clusters, hooked with &lt;a href="https://github.com/influxdata/influxdb"&gt;influxdb&lt;/a&gt; that consumes reporting pushed by heapster and can be visualized in &lt;a href="https://grafana.com/"&gt;grafana&lt;/a&gt;.&lt;/p&gt;
&lt;div style="text-align: right"&gt;&lt;sub&gt;Monitoring in Grafana&lt;/sub&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="Grafana monitoring" src="/images/kubes/grafana.png" title="Grafana monitoring"&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Note :&lt;/strong&gt; Some configuration in GCE should be taken care, like &lt;code&gt;autoupgrade kubernetes version&lt;/code&gt;. If you are running RabbitMQ, Redis or any other message queue as service that needs uptime, better you turn off autoupgrade because kubernetes new version release will schedule all your node for maintenance, however it rolles updates one by one but could affect your production system. Else, if you are fully stateless, you can keep default or skip this warning!&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;----- Autoupgrade off

# https://cloud.google.com/container-engine/docs/node-auto-upgrade

gcloud beta container node-pools update &amp;lt;NODEPOOL&amp;gt; --cluster &amp;lt;CLUSTER&amp;gt; --zone &amp;lt;ZONE&amp;gt; --no-enable-autoupgrade
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Pretty much all above understanding are based on what I learned in last six months of kubernetes running in production. Container management is easy to adapt and lot of new observation is yet to be discovered as we go along the way.&lt;/p&gt;
&lt;p&gt;Looking at deployments today, Kubernetes is absolutely fantastic in Auto-pilot and doing self-healing jobs itself. We are running more than &lt;code&gt;1000 pods&lt;/code&gt; in cluster together and processing &lt;code&gt;10's of Billions of API calls per month&lt;/code&gt; and are pushing more to handle.&lt;/p&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;Conclusion : &lt;code&gt;Kubernetes&lt;/code&gt; lifted lot of &lt;code&gt;server management&lt;/code&gt; and helped in faster depployments &amp;amp; scaling system. Adaptability is much quicker, most of security and other concerns is being managed by Google. Kubernetes aims to offer a better orchestration management system on top of clustered infrastrcuture. Development on Kubernetes has been happening at storm-speed, and the &lt;a href="https://kubernetes.io/community/"&gt;community of Kubernauts&lt;/a&gt; has grown bigger.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h5&gt;Medium Blog : &lt;a href="https://medium.com/@sunnykrgupta/managing-fleet-on-kubernetes-8cac6483b64"&gt;medium.com/@sunnykrgupta/managing-fleet-on-kubernetes-8cac6483b64&lt;/a&gt;&lt;/h5&gt;</content><category term="kubernetes"></category><category term="docker"></category><category term="gce"></category><category term="gcr"></category><category term="container"></category></entry><entry><title>A Practical Guide to Geopy</title><link href="https://sunnykrGupta.github.io/a-practical-guide-to-geopy.html" rel="alternate"></link><published>2015-07-19T01:31:31+05:30</published><updated>2015-07-19T01:31:31+05:30</updated><author><name>Sunny Kr Gupta</name></author><id>tag:sunnykrgupta.github.io,2015-07-19:/a-practical-guide-to-geopy.html</id><summary type="html">&lt;p&gt;During my final major academic project, i was working with &lt;a href="http://twitter.com"&gt;Twitter&lt;/a&gt; data (ie. tweets). I did some research and found out approximately only 1% of all Tweets published on Twitter are geolocated (ie. have location information). This is a very small portion of the Tweets, and i needed almost every …&lt;/p&gt;</summary><content type="html">&lt;p&gt;During my final major academic project, i was working with &lt;a href="http://twitter.com"&gt;Twitter&lt;/a&gt; data (ie. tweets). I did some research and found out approximately only 1% of all Tweets published on Twitter are geolocated (ie. have location information). This is a very small portion of the Tweets, and i needed almost every tweets location to segregate data country-wise.
  - How to resolve a string location to determine country ("Banagher", "Ipoh" etc )
  - How to resolve a Coordinate into country. ('4.581' - '101.082' etc)&lt;/p&gt;
&lt;p&gt;There comes a &lt;a href="http://geopy.readthedocs.org/"&gt;Geopy&lt;/a&gt; to rescue us to this problem.&lt;/p&gt;
&lt;h4&gt;Geopy&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Geopy makes it easy for Python developers to locate the coordinates of addresses, cities, countries, and landmarks across the globe using third-party geocoders and other data sources.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Geocoding is the process of converting addresses (like "1600 Amphitheatre Parkway, Mountain View, CA") into geographic coordinates (like latitude 37.423021 and longitude 122.083739) or reverse.&lt;/p&gt;
&lt;h5&gt;Available Geocoder APIs&lt;/h5&gt;
&lt;p&gt;There are several Geocoding service provided by different Map APIs, populars are listed below:
 - &lt;a href="https://developers.google.com/maps/documentation/geolocation/intro"&gt;Google Maps Geocoding V3 API&lt;/a&gt; (2500 per-day)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://www.geonames.org/about.html"&gt;Geonames&lt;/a&gt; (30000 per-day | 2000 per-hour)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://wiki.openstreetmap.org/wiki/Nominatim"&gt;Nominatim - Open Street Map&lt;/a&gt; (refer Usage Docs)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://tech.yandex.com/maps/doc/geocoder/desc/concepts/About-docpage/"&gt;Yandex Map API&lt;/a&gt;  (25,000 per-day)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://msdn.microsoft.com/en-us/library/ff428643.aspx"&gt;Bing  Map API (Microsoft)&lt;/a&gt; (refer Usage Docs)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://developer.yahoo.com/boss/geo/"&gt;Yahoo BOSS Finder&lt;/a&gt; (refer Usage Docs)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Note : Geocoder APIs may have request limts on per-day or per-IP or others. Increasing those limts can result in blacklisting. At the time of writing this post, limits are mentioned in brackets above.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;Installation&lt;/h4&gt;
&lt;hr&gt;
&lt;p&gt;Open your favorite Terminal and run these commands. Install using &lt;code&gt;pip&lt;/code&gt; with:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ pip install geopy
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Geocoding&lt;/h4&gt;
&lt;p&gt;To query a location using string using &lt;code&gt;Google MAP V3&lt;/code&gt;. To acquire key you need to register your app on google developer console.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;geopy.geocoders&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;GoogleV3&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;json&lt;/span&gt;
&lt;span class="n"&gt;geolocator&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;GoogleV3&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;api_key&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;AIzaSyCxk0i1WQokYRgUxAZieq&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;location&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;geolocator&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;geocode&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Washington&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;language&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;en&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;location&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;json&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dumps&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;location&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;raw&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;indent&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;location&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;address&lt;/span&gt;
&lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;No location!&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;location&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Gocoders have different service api classes, here i have used &lt;code&gt;GoogleV3&lt;/code&gt;, then an object is created named &lt;code&gt;geolocator&lt;/code&gt; to query and saved results of my query string "Washington" in &lt;code&gt;location&lt;/code&gt;. If query string doesnt contain valid place name or city, it throws &lt;code&gt;None&lt;/code&gt;. Now lets see what we got after this script:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;quot;geometry&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;location_type&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;APPROXIMATE&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;bounds&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
            &lt;span class="nt"&gt;&amp;quot;northeast&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
                &lt;span class="nt"&gt;&amp;quot;lat&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;38.995548&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nt"&gt;&amp;quot;lng&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;-76.909393&lt;/span&gt;
            &lt;span class="p"&gt;},&lt;/span&gt;
            &lt;span class="nt"&gt;&amp;quot;southwest&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
                &lt;span class="nt"&gt;&amp;quot;lat&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;38.8031495&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nt"&gt;&amp;quot;lng&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;-77.11974&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
        &lt;span class="p"&gt;},&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;location&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
            &lt;span class="nt"&gt;&amp;quot;lat&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;38.9071923&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nt"&gt;&amp;quot;lng&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;-77.0368707&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
        &lt;span class="p"&gt;},&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;quot;address_components&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
        &lt;span class="p"&gt;{&lt;/span&gt;
            &lt;span class="nt"&gt;&amp;quot;long_name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Washington&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="nt"&gt;&amp;quot;types&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;locality&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;political&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
            &lt;span class="nt"&gt;&amp;quot;short_name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;D.C.&amp;quot;&lt;/span&gt;
        &lt;span class="p"&gt;},&lt;/span&gt;
        &lt;span class="p"&gt;{&lt;/span&gt;
            &lt;span class="nt"&gt;&amp;quot;long_name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;District of Columbia&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="nt"&gt;&amp;quot;types&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;administrative_area_level_1&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;political&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
            &lt;span class="nt"&gt;&amp;quot;short_name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;DC&amp;quot;&lt;/span&gt;
        &lt;span class="p"&gt;},&lt;/span&gt;
        &lt;span class="p"&gt;{&lt;/span&gt;
            &lt;span class="nt"&gt;&amp;quot;long_name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;United States&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="nt"&gt;&amp;quot;types&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;country&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;political&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
            &lt;span class="nt"&gt;&amp;quot;short_name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;US&amp;quot;&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;quot;place_id&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;ChIJW-T2Wt7Gt4kRKl2I1CJFUsI&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;quot;formatted_address&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Washington, DC, USA&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;quot;types&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;locality&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;political&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="err"&gt;Washington,&lt;/span&gt; &lt;span class="err"&gt;DC,&lt;/span&gt; &lt;span class="err"&gt;USA&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;So, you can see its a format of data returned from Google Map API, so we need to extract the information we are looking for from this resulted data. Easy huh! You can see lattitude. longitude , Country name, District name, and other place information.
&lt;/br&gt;
You can query a string of coordinate too, in above code replace the geolocator lines with follows and run:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;location&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;geolocator&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reverse&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;52.509669, 13.376294&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;You can see the raw JSON output as result. See below, if you want formatted address, use &lt;code&gt;location.address&lt;/code&gt; , &lt;code&gt;location.latitude&lt;/code&gt;, &lt;code&gt;location.longitude&lt;/code&gt; gives you coordinates of place and &lt;code&gt;location.raw&lt;/code&gt; gives you the result like we saw above in JSON format, that contains lots of other information.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;location&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;address&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;Potsdamer&lt;/span&gt; &lt;span class="n"&gt;Platz&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Mitte&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Berlin&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10117&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Deutschland&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;European&lt;/span&gt; &lt;span class="n"&gt;Union&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;location&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;latitude&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;location&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;longitude&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;52.5094982&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;13.3765983&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Similarly we can use other Geocoder APIs too, &lt;code&gt;Nominatim&lt;/code&gt; as openstreet map, &lt;code&gt;Yandex&lt;/code&gt; for Yandex Map, &lt;code&gt;GeoNames&lt;/code&gt; as Geonames Geocoder etc.&lt;/p&gt;
&lt;p&gt;Geopy code for &lt;code&gt;GeoNames&lt;/code&gt; and &lt;code&gt;Yandex&lt;/code&gt; :&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;geopy.geocoders&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;GeoNames&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;json&lt;/span&gt;
&lt;span class="n"&gt;geolocator&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;GeoNames&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;username&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Ur_user_NAME&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# Register at Geonames&lt;/span&gt;
&lt;span class="n"&gt;location&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;geolocator&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;geocode&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;冥府&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;timeout&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;location&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;json&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dump&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;location&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;raw&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;location&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;address&lt;/span&gt;
&lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;No location!&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;location&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;geopy.geocoders&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Yandex&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;json&lt;/span&gt;
&lt;span class="n"&gt;geolocator&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Yandex&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lang&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;en_US&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;location&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;geolocator&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;geocode&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;بغداد، العراق&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;timeout&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;location&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;json&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dumps&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;location&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;raw&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;indent&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;location&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;address&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;location&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;latitude&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot; -&amp;gt; &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;location&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;longitude&lt;/span&gt;
&lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;location&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;So simple right! for using other Geocoder Map APIs , refer to full documentation of &lt;a href="http://geopy.readthedocs.org/en/latest/#"&gt;Geopy&lt;/a&gt;. I hope you understands the working of Geocoder APIs and power of Geopy.
&lt;/br&gt;Comment below if you come across any problem and give feedback. Thanks you all! Have a good day!&lt;/p&gt;</content><category term="Geopy"></category><category term="Geocoder"></category><category term="Python"></category><category term="GoogleV3"></category><category term="Yandex"></category><category term="Geonames"></category></entry><entry><title>Get Familiar with Python</title><link href="https://sunnykrGupta.github.io/get-familiar-with-python.html" rel="alternate"></link><published>2014-05-26T17:23:31+05:30</published><updated>2014-05-26T17:23:31+05:30</updated><author><name>Sunny Kr Gupta</name></author><id>tag:sunnykrgupta.github.io,2014-05-26:/get-familiar-with-python.html</id><summary type="html">&lt;p&gt;Python is a powerful scripting language.&lt;/p&gt;
&lt;h4&gt;Guide's to Python :&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Learn about &lt;a href="http://en.wikipedia.org/wiki/REST"&gt;REST architecture&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Know and get familiar with Python. [2][2.1][10]&lt;/li&gt;
&lt;li&gt;Setup virtualenv [3]&lt;/li&gt;
&lt;li&gt;Get the feel of handling backend. Complete the django tutorial [4] from   official site.&lt;/li&gt;
&lt;li&gt;Know a bit more about django architecture [5][8]&lt;/li&gt;
&lt;li&gt;Write …&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;p&gt;Python is a powerful scripting language.&lt;/p&gt;
&lt;h4&gt;Guide's to Python :&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Learn about &lt;a href="http://en.wikipedia.org/wiki/REST"&gt;REST architecture&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Know and get familiar with Python. [2][2.1][10]&lt;/li&gt;
&lt;li&gt;Setup virtualenv [3]&lt;/li&gt;
&lt;li&gt;Get the feel of handling backend. Complete the django tutorial [4] from   official site.&lt;/li&gt;
&lt;li&gt;Know a bit more about django architecture [5][8]&lt;/li&gt;
&lt;li&gt;Write you first api using tastypie[6][7]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Footnotes:&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;[1]: http://en.wikipedia.org/wiki/REST&lt;/p&gt;
&lt;p&gt;[2]: &lt;a href="http://www.youtube.com/watch?v=u1sVfGEBKWQ"&gt;What makes python awesome&lt;/a&gt;.
[2.1]: &lt;a href="http://www.python.org/dev/peps/pep-0008"&gt;Python Style Guide&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;[3]: http://stackoverflow.com/questions/4806448/how-do-i-install-from-a-local-cache-with-pip&lt;/p&gt;
&lt;p&gt;[4]: https://docs.djangoproject.com/en/1.5/intro/tutorial01/&lt;/p&gt;
&lt;p&gt;[5]: http://www.youtube.com/watch?v=t_ziKY1ayCo (~3hr video by James Bennett)&lt;/p&gt;
&lt;p&gt;[6]: http://django-tastypie.readthedocs.org/en/latest/&lt;/p&gt;
&lt;p&gt;[7]: http://pyvideo.org/video/673/restful-apis-with-tastypie&lt;/p&gt;
&lt;p&gt;[8]: &lt;a href="http://bit.ly/16z2gQq"&gt;Two scoops for Django Book&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[9]: &lt;a href="http://bit.ly/197rnvU"&gt;Pro python Book&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[10]: http://docs.python.org/2/tutorial/index.html&lt;/p&gt;
&lt;p&gt;[11]: https://github.com/twoscoops/django-twoscoops-project&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;Additional Resources:&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Django best practises [8]&lt;/li&gt;
&lt;li&gt;Pro Python Book [9]&lt;/li&gt;
&lt;li&gt;Pycon Videos: [http://pyvideo.org/category/33/pycon-us-2013]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Community and updates:&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;http://www.pythonweekly.com/&lt;/li&gt;
&lt;li&gt;http://mail.python.org/mailman/listinfo/bangpypers&lt;/li&gt;
&lt;li&gt;https://groups.google.com/forum/#!forum/django-users&lt;/li&gt;
&lt;li&gt;http://mail.python.org/mailman/listinfo/ncr-python.in&lt;/li&gt;
&lt;/ul&gt;</content><category term="python"></category></entry><entry><title>Octopress - Blog Frameworks</title><link href="https://sunnykrGupta.github.io/octopress-blog-frameworks.html" rel="alternate"></link><published>2014-05-26T17:23:31+05:30</published><updated>2014-05-26T17:23:31+05:30</updated><author><name>Sunny Kr Gupta</name></author><id>tag:sunnykrgupta.github.io,2014-05-26:/octopress-blog-frameworks.html</id><summary type="html">&lt;p&gt;Hello, Everyone! This is blog post for helping you setup your own blog using Octopress framework running over Jekyll.
I am Summer Intern at &lt;a href="http://www.ophio.co.in"&gt;Ophio&lt;/a&gt;, a New York based company. I have been given a task to set up my blog. My mentor Mr.&lt;a href="https://github.com/theskumar"&gt;Saurabh&lt;/a&gt; is a Senior Back-end Developer …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Hello, Everyone! This is blog post for helping you setup your own blog using Octopress framework running over Jekyll.
I am Summer Intern at &lt;a href="http://www.ophio.co.in"&gt;Ophio&lt;/a&gt;, a New York based company. I have been given a task to set up my blog. My mentor Mr.&lt;a href="https://github.com/theskumar"&gt;Saurabh&lt;/a&gt; is a Senior Back-end Developer. He developed many cool stuffs and active contibutor to open-source technology.&lt;/p&gt;
&lt;h3&gt;Deployment of Octopress : Blog Frameworks for Hackers&lt;/h3&gt;
&lt;p&gt;Octopress is a static blogging framework built on top of Jekyll. It uses scripts to build static files to be deployed to a server. You can design your blog and deploy on &lt;a href="https://pages.github.com/"&gt;Github-Pages&lt;/a&gt; very quickly.&lt;/p&gt;
&lt;h4&gt;Installation's :&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Git&lt;/li&gt;
&lt;li&gt;Jekyll&lt;/li&gt;
&lt;li&gt;Ruby Gem&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;How to setup Octopress?? &lt;a href="http://octopress.org/docs/setup/"&gt;Go ahead&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Variety of theme available &lt;a href="https://github.com/imathis/octopress/wiki/3rd-Party-Octopress-Themes"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Responsive Blog" src="images/main-screenshot.png" title=" Octopress Image"&gt;&lt;/p&gt;</content><category term="Octopress"></category><category term="ruby"></category></entry><entry><title>Pelican Python Blogging Framework</title><link href="https://sunnykrGupta.github.io/pelican-python-blogging-framework.html" rel="alternate"></link><published>2014-05-26T17:23:31+05:30</published><updated>2014-05-26T17:23:31+05:30</updated><author><name>Sunny Kr Gupta</name></author><id>tag:sunnykrgupta.github.io,2014-05-26:/pelican-python-blogging-framework.html</id><summary type="html">&lt;p&gt;Hello, Everyone! This is 2nd blog post about Pelican framework in python.&lt;/p&gt;
&lt;h4&gt;Deployment of Pelican : Blog Frameworks&lt;/h4&gt;
&lt;p&gt;Pelican is a static site generator, written in Python. It strikes as
a very interesting method as you can write your content purely in your favourite editor (in Markdown Flavour) and commit your …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Hello, Everyone! This is 2nd blog post about Pelican framework in python.&lt;/p&gt;
&lt;h4&gt;Deployment of Pelican : Blog Frameworks&lt;/h4&gt;
&lt;p&gt;Pelican is a static site generator, written in Python. It strikes as
a very interesting method as you can write your content purely in your favourite editor (in Markdown Flavour) and commit your post or style changes using Git. You can design your blog and deploy on &lt;a href="https://pages.github.com/"&gt;Github-Pages&lt;/a&gt; very quickly.&lt;/p&gt;
&lt;h4&gt;Installation's :&lt;/h4&gt;
&lt;p&gt;How to setup Pelican?? &lt;a href="http://docs.getpelican.com/en/stable/quickstart.html"&gt;Go ahead&lt;/a&gt; and detailed explanation of kicking a small webpage and deploying using github-pages, &lt;a href="http://seanazlin.com/creating-a-blog-on-GitHub-dot-io-with-Python.html"&gt;Creating-a-blog-on-GitHub-dot-io-with-Python&lt;/a&gt; by Sean Azlin. I got most of the help in building this blog from this.&lt;/p&gt;
&lt;p&gt;Variety of &lt;a href="https://github.com/getpelican/pelican-themes"&gt;themes&lt;/a&gt; available.&lt;/p&gt;
&lt;h4&gt;Note's :&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;Follow Markdown syntax to write post.&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Deploy your page by following and check your gitpage-url :&lt;/p&gt;
&lt;p&gt;$ ghp-import output&lt;/p&gt;
&lt;p&gt;$ git checkout master&lt;/p&gt;
&lt;p&gt;$ git merge gh-pages&lt;/p&gt;
&lt;p&gt;$ git push --all&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img alt="Responsive Blog" src="https://sunnykrGupta.github.io/images/Pelican.jpg" title="Pelican Image"&gt;&lt;/p&gt;</content><category term="pelican"></category><category term="python"></category></entry></feed>