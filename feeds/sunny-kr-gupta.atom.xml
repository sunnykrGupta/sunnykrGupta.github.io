<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Daemon Blog - Sunny Kr Gupta</title><link href="https://sunnykrGupta.github.io/" rel="alternate"></link><link href="https://sunnykrGupta.github.io/feeds/sunny-kr-gupta.atom.xml" rel="self"></link><id>https://sunnykrGupta.github.io/</id><updated>2017-11-05T11:12:31+05:30</updated><entry><title>Patch and Update Table - BigQuery Part-III</title><link href="https://sunnykrGupta.github.io/patch-and-update-table-bigquery-part-iii.html" rel="alternate"></link><published>2017-11-05T11:12:31+05:30</published><updated>2017-11-05T11:12:31+05:30</updated><author><name>Sunny Kr Gupta</name></author><id>tag:sunnykrgupta.github.io,2017-11-05:/patch-and-update-table-bigquery-part-iii.html</id><summary type="html">&lt;p&gt;Table updation is important when you have data ready inside the table and suddenly you have a requirement of adding more fields in the table to do analysis. In this post, we are going to dive into Streaming feature of BigQuery.&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;img alt="Google BigQuery" src="/images/bq-series/part3/bigquery.png"&gt;&lt;/p&gt;
&lt;p&gt;This post is 3rd part of 3-post series. In the earlier post, we understood the streaming in BigQuery &lt;strong&gt;&lt;a href="https://sunnykrgupta.github.io/streaming-with-redis-bigquery-part-ii.html"&gt;Streaming with Redis - BigQuery Part-II&lt;/a&gt;&lt;/strong&gt;. In this post, we are going to learn patching and updating table schemas.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Update/Patch Table in BigQuery&lt;/h3&gt;
&lt;p&gt;Table updation is important when you have data ready inside the table and suddenly you have a requirement of adding more fields in the table to do analysis.&lt;/p&gt;
&lt;p&gt;Here, we are going to add a field and see the changes in the table after update operation.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h4&gt;1. Add field into schema&lt;/h4&gt;
&lt;p&gt;We are going to use same table &lt;strong&gt;&lt;code&gt;StreamTable&lt;/code&gt;&lt;/strong&gt; which we used earlier for streaming and introducing one more field as &lt;strong&gt;&lt;code&gt;UniqueSocialNumber&lt;/code&gt;&lt;/strong&gt; with datatype &lt;strong&gt;&lt;code&gt;INTEGER&lt;/code&gt;&lt;/strong&gt;. Lets add these changes to &lt;strong&gt;&lt;code&gt;schema.py&lt;/code&gt;&lt;/strong&gt; which will be used by our main program written in later steps.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;cat schema.py

&lt;span class="c"&gt;#Add field schema&lt;/span&gt;
&lt;span class="nv"&gt;TableObject&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;tableReference&amp;quot;&lt;/span&gt;: &lt;span class="o"&gt;{&lt;/span&gt;
      &lt;span class="s2"&gt;&amp;quot;projectId&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;mimetic-slate&amp;quot;&lt;/span&gt;,
      &lt;span class="s2"&gt;&amp;quot;tableId&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;StreamTable&amp;quot;&lt;/span&gt;,
      &lt;span class="s2"&gt;&amp;quot;datasetId&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;BQ_Dataset&amp;quot;&lt;/span&gt;,
    &lt;span class="o"&gt;}&lt;/span&gt;,

    &lt;span class="s2"&gt;&amp;quot;schema&amp;quot;&lt;/span&gt;: &lt;span class="o"&gt;{&lt;/span&gt;
      &lt;span class="s2"&gt;&amp;quot;fields&amp;quot;&lt;/span&gt;: &lt;span class="o"&gt;[&lt;/span&gt;
          &lt;span class="o"&gt;{&lt;/span&gt;
              &lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;username&amp;quot;&lt;/span&gt;,
              &lt;span class="s2"&gt;&amp;quot;type&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;STRING&amp;quot;&lt;/span&gt;,
              &lt;span class="s2"&gt;&amp;quot;mode&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;NULLABLE&amp;quot;&lt;/span&gt;
          &lt;span class="o"&gt;}&lt;/span&gt;,
          &lt;span class="o"&gt;{&lt;/span&gt;
              &lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;,
              &lt;span class="s2"&gt;&amp;quot;type&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;STRING&amp;quot;&lt;/span&gt;,
              &lt;span class="s2"&gt;&amp;quot;mode&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;NULLABLE&amp;quot;&lt;/span&gt;
          &lt;span class="o"&gt;}&lt;/span&gt;,
          &lt;span class="o"&gt;{&lt;/span&gt;
              &lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;birthdate&amp;quot;&lt;/span&gt;,
              &lt;span class="s2"&gt;&amp;quot;type&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;STRING&amp;quot;&lt;/span&gt;,
              &lt;span class="s2"&gt;&amp;quot;mode&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;NULLABLE&amp;quot;&lt;/span&gt;
          &lt;span class="o"&gt;}&lt;/span&gt;,
          &lt;span class="o"&gt;{&lt;/span&gt;
              &lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;sex&amp;quot;&lt;/span&gt;,
              &lt;span class="s2"&gt;&amp;quot;type&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;STRING&amp;quot;&lt;/span&gt;,
              &lt;span class="s2"&gt;&amp;quot;mode&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;NULLABLE&amp;quot;&lt;/span&gt;
          &lt;span class="o"&gt;}&lt;/span&gt;,
          &lt;span class="o"&gt;{&lt;/span&gt;
              &lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;address&amp;quot;&lt;/span&gt;,
              &lt;span class="s2"&gt;&amp;quot;type&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;STRING&amp;quot;&lt;/span&gt;,
              &lt;span class="s2"&gt;&amp;quot;mode&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;NULLABLE&amp;quot;&lt;/span&gt;
          &lt;span class="o"&gt;}&lt;/span&gt;,
          &lt;span class="o"&gt;{&lt;/span&gt;
              &lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;mail&amp;quot;&lt;/span&gt;,
              &lt;span class="s2"&gt;&amp;quot;type&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;STRING&amp;quot;&lt;/span&gt;,
              &lt;span class="s2"&gt;&amp;quot;mode&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;NULLABLE&amp;quot;&lt;/span&gt;
          &lt;span class="o"&gt;}&lt;/span&gt;,
          &lt;span class="o"&gt;{&lt;/span&gt;
              &lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;UniqueSocialNumber&amp;quot;&lt;/span&gt;,   &lt;span class="c"&gt;#New Field&lt;/span&gt;
              &lt;span class="s2"&gt;&amp;quot;type&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;INTEGER&amp;quot;&lt;/span&gt;,
              &lt;span class="s2"&gt;&amp;quot;mode&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;NULLABLE&amp;quot;&lt;/span&gt;
          &lt;span class="o"&gt;}&lt;/span&gt;
      &lt;span class="o"&gt;]&lt;/span&gt;,
  &lt;span class="o"&gt;}&lt;/span&gt;,
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h4&gt;2. Patch/Update API in BigQuery&lt;/h4&gt;
&lt;p&gt;We have &lt;code&gt;schema.py&lt;/code&gt; script ready and below is our main program &lt;strong&gt;&lt;code&gt;tablePatch.py&lt;/code&gt;&lt;/strong&gt; that will execute the table patch API call to bigquery.&lt;/p&gt;
&lt;p&gt;We have two methods available in BigQuery to make updates in table.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;tables.patch&lt;/code&gt;&lt;/strong&gt; - This method only replaces fields that are provided in the resources&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;tables.update&lt;/code&gt;&lt;/strong&gt; - This method replaces the entire table resource.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Patch Example :&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="err"&gt;$&lt;/span&gt; &lt;span class="n"&gt;cat&lt;/span&gt; &lt;span class="n"&gt;tablePatch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;py&lt;/span&gt;

&lt;span class="c"&gt;#!/usr/bin/env python&lt;/span&gt;

&lt;span class="c"&gt;#https://developers.google.com/api-client-library/python/&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;googleapiclient&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;discovery&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;oauth2client.client&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;GoogleCredentials&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;schema&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;TableObject&lt;/span&gt;


&lt;span class="c"&gt;# [START Table Creater ]&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;PatchTable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bigquery&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;tables&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;bigquery&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tables&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="n"&gt;tableStatusObject&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tables&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;patch&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;projectId&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;TableObject&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;tableReference&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;projectId&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; \
                &lt;span class="n"&gt;datasetId&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;TableObject&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;tableReference&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;datasetId&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; \
                &lt;span class="n"&gt;tableId&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;TableObject&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;tableReference&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;tableId&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; \
                &lt;span class="n"&gt;body&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;TableObject&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;execute&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;&lt;/span&gt;&lt;span class="se"&gt;\n\n&lt;/span&gt;&lt;span class="s"&gt;Table Patched&amp;quot;&lt;/span&gt;
&lt;span class="c"&gt;# [END]&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="c"&gt;#to get credentials from my laptop&lt;/span&gt;
    &lt;span class="n"&gt;credentials&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;GoogleCredentials&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_application_default&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="c"&gt;# Construct the service object for interacting with the BigQuery API.&lt;/span&gt;
    &lt;span class="n"&gt;bigquery&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;discovery&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;build&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;bigquery&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;v2&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;credentials&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;credentials&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;PatchTable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bigquery&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;https://cloud.google.com/bigquery/docs/tables#update-schema&lt;/span&gt;
&lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;__main__&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;main&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;BQ Table Patch !!&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Update Example :&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;#instead of calling patch(), we call update() to apply updates
tables.update(projectId=TableObject[&amp;#39;tableReference&amp;#39;][&amp;#39;projectId&amp;#39;],\
                datasetId=TableObject[&amp;#39;tableReference&amp;#39;][&amp;#39;datasetId&amp;#39;],\
                tableId=TableObject[&amp;#39;tableReference&amp;#39;][&amp;#39;tableId&amp;#39;], \
                body=TableObject).execute()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;After applying changes we can verify changes in schema of bigquery table.&lt;/p&gt;
&lt;p&gt;Visit UI :&lt;a href="https://bigquery.cloud.google.com"&gt;https://bigquery.cloud.google.com&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;You can also verify table schema by running &lt;strong&gt;&lt;code&gt;bq&lt;/code&gt;&lt;/strong&gt; CLI commands&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;bq show BQ_Dataset.StreamTable
Table mimetic-slate:BQ_Dataset.StreamTable

   Last modified                Schema               Total Rows   Total Bytes   Expiration   Time Partitioning   Labels  
 ----------------- -------------------------------- ------------ ------------- ------------ ------------------- --------
  &lt;span class="m"&gt;04&lt;/span&gt; Nov 15:48:43   &lt;span class="p"&gt;|&lt;/span&gt;-username: string              &lt;span class="m"&gt;451&lt;/span&gt;          &lt;span class="m"&gt;50837&lt;/span&gt;                                                  
                    &lt;span class="p"&gt;|&lt;/span&gt;-name:string                                                                                      
                    &lt;span class="p"&gt;|&lt;/span&gt;-birthdate:string                                                                                 
                    &lt;span class="p"&gt;|&lt;/span&gt;-sex:string                                                                                       
                    &lt;span class="p"&gt;|&lt;/span&gt;-address:string                                                                                   
                    &lt;span class="p"&gt;|&lt;/span&gt;-mail:string                                                                                      
                    &lt;span class="p"&gt;|&lt;/span&gt;-UniqueSocialNumber: integer
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Reference &lt;/strong&gt; :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Table Patch and Update :&lt;/strong&gt; &lt;a href="https://developers.google.com/resources/api-libraries/documentation/bigquery/v2/python/latest/bigquery_v2.tables.html"&gt;https://developers.google.com/resources/api-libraries/documentation/bigquery/v2/python/latest/bigquery_v2.tables.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h4&gt;3. Preview of data in bigquery table&lt;/h4&gt;
&lt;p&gt;Click on table preview to see new field. It will show &lt;strong&gt;&lt;code&gt;null&lt;/code&gt;&lt;/strong&gt; with old records. You can start streaming data which contains this newly added field to be written in table going forward.&lt;/p&gt;
&lt;div style="text-align: center"&gt;&lt;u&gt;&lt;b&gt;Table Preview&lt;/b&gt;&lt;/u&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="New Field" src="/images/bq-series/part3/new-field.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Github reference&lt;/strong&gt; : &lt;a href="https://github.com/sunnykrGupta/Bigquery-series"&gt;https://github.com/sunnykrGupta/Bigquery-series&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;This was introduction to updation in BigQuery table, thus concludes our 3-post series.&lt;/p&gt;
&lt;p&gt;From this series, we covered basic features of Google BigQuery serverless product. There are other similar product available on other clouds like AWS redshift, Azure SQL Data Warehouse which serve infinite computing resources like BigQuery.&lt;/p&gt;
&lt;p&gt;Please comment your thoughts/feedback about this series.&lt;/p&gt;
&lt;hr&gt;</content><category term="Google BigQuery"></category><category term="GCP"></category><category term="Google Cloud Platform"></category></entry><entry><title>Streaming with Redis - BigQuery Part-II</title><link href="https://sunnykrGupta.github.io/streaming-with-redis-bigquery-part-ii.html" rel="alternate"></link><published>2017-11-01T17:12:31+05:30</published><updated>2017-11-01T17:12:31+05:30</updated><author><name>Sunny Kr Gupta</name></author><id>tag:sunnykrgupta.github.io,2017-11-01:/streaming-with-redis-bigquery-part-ii.html</id><summary type="html">&lt;p&gt;Streaming helps in pushing our data into BigQuery (short for BQ) and helps in making data available for query without delay of running load jobs. In this post, we are going to dive into Streaming feature of BigQuery.&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;img alt="Google BigQuery" src="/images/bq-series/part2/bq-stream.png"&gt;&lt;/p&gt;
&lt;p&gt;This post is 2nd part of 3-post series. In the earlier post, we understood the fundamentals of BigQuery Load Jobs &lt;strong&gt;&lt;a href="https://sunnykrgupta.github.io/export-load-job-with-mongodb-bigquery-part-i.html"&gt;Export &amp;amp; Load Job with MongoDB - BigQuery Part-I&lt;/a&gt;&lt;/strong&gt;. In this post, we are going to dive into Streaming feature of BigQuery.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a href="https://cloud.google.com/bigquery/streaming-data-into-bigquery"&gt;Streaming&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Why ?&lt;/strong&gt; - Streaming helps in pushing our data into BigQuery (short for BQ) and helps in making data available for query without delay of running load jobs.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;There are some trade-offs to choose Streaming. A few are belowsÂ :&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;We need to follow a few quotas like http body size, maximum rows / request etc while making streaming API calls.&lt;/li&gt;
&lt;li&gt;Written data in tables are not instantly available for copy or for export jobs in bigquery, it will take upto 90 minutes to be made available while load based tables are available instantly.&lt;/li&gt;
&lt;li&gt;At the time of writing this post, charges incurred in streaming whereas load jobs were free.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Keeping above in mind, we need to choose streaming vs load jobs in BigQuery.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Quotas :&lt;/strong&gt; &lt;a href="https://cloud.google.com/bigquery/quotas#streaminginserts"&gt;https://cloud.google.com/bigquery/quotas#streaminginserts&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Streaming Data into BigQuery&lt;/h3&gt;
&lt;p&gt;In this article, we are going to use a &lt;a href="https://redis.io"&gt;redis server&lt;/a&gt; as a message broker to hold our data.&lt;/p&gt;
&lt;p&gt;We are going to prepare data and the skeleton of data is going to be basic information of any person (username, name, birthdate, sex, address, email). As per this information, we need schema and table in bigquery to be created in advance before streaming. Post table creation, we are going to run streaming program to ingest our data in bulk which will be read from redis and same will be written to bigquery table in real time.
We are going to use &lt;code&gt;python&lt;/code&gt; as our programming language.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h4&gt;1. Prepare data in Redis&lt;/h4&gt;
&lt;p&gt;&lt;img alt="Redis" src="/images/bq-series/part2/redis.png"&gt;&lt;/p&gt;
&lt;p&gt;We are going to write a small python script to preapare data in &lt;a href="https://redis.io/topics/data-types"&gt;redis List&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Redis Installation :&lt;/strong&gt; &lt;a href="https://redis.io/download"&gt;https://redis.io/download&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;If you have docker running, run redis inside container with simple command&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;docker run -d --name redis-streaming -p 6379:6379 redis
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Script is going to execute &lt;a href="https://redis.io/commands/lpush"&gt;LPUSH command&lt;/a&gt; in redis to insert data into list named as &lt;code&gt;redisList&lt;/code&gt; . With the help of &lt;code&gt;Faker&lt;/code&gt; library we are going to generate some fake profile as our data.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Faker :&lt;/strong&gt; &lt;a href="https://github.com/joke2k/faker"&gt;https://github.com/joke2k/faker&lt;/a&gt;&lt;/p&gt;
&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt; 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="c"&gt;#!/usr/bin/env python&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;redis&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;json&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nn"&gt;time&lt;/span&gt;
&lt;span class="c"&gt;#Faker : https://github.com/joke2k/faker&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;faker&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Faker&lt;/span&gt;

&lt;span class="n"&gt;streamRedis&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;redis&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Redis&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;host&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;127.0.0.1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
             &lt;span class="n"&gt;port&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;6379&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;password&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;fake&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Faker&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;profile_generator&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;fake&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;simple_profile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sex&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;streamRedis&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;lpush&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;redisList&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;json&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dumps&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;profile_generator&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="c"&gt;#sleep 200ms&lt;/span&gt;
        &lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sleep&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;__main__&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;main&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h4&gt;2. Inspect data and prepare schema for Table&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Our data looks like :&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;{
    &amp;#39;username&amp;#39;: u&amp;#39;tarawade&amp;#39;,
    &amp;#39;name&amp;#39;: u&amp;#39;Jennifer Lewis&amp;#39;,
    &amp;#39;birthdate&amp;#39;: &amp;#39;2005-06-14&amp;#39;,
    &amp;#39;sex&amp;#39;: &amp;#39;F&amp;#39;,
    &amp;#39;address&amp;#39;: u&amp;#39;7134 Robinson Club Apt. 530\nPort Andreachester, GA 19011-6162&amp;#39;,
    &amp;#39;mail&amp;#39;: u&amp;#39;tmorgan@yahoo.com&amp;#39;
}
&lt;/pre&gt;&lt;/div&gt;


&lt;blockquote&gt;
&lt;p&gt;For the above formatted data, below schema is going to work :&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;[
    {
        &amp;quot;name&amp;quot;: &amp;quot;username&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;STRING&amp;quot;, &amp;quot;mode&amp;quot;: &amp;quot;NULLABLE&amp;quot;
    },
    {
        &amp;quot;name&amp;quot;: &amp;quot;name&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;STRING&amp;quot;, &amp;quot;mode&amp;quot;: &amp;quot;NULLABLE&amp;quot;
    },
    {
        &amp;quot;name&amp;quot;: &amp;quot;birthdate&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;STRING&amp;quot;, &amp;quot;mode&amp;quot;: &amp;quot;NULLABLE&amp;quot;
    },
    {
        &amp;quot;name&amp;quot;: &amp;quot;sex&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;STRING&amp;quot;, &amp;quot;mode&amp;quot;: &amp;quot;NULLABLE&amp;quot;
    },
    {
        &amp;quot;name&amp;quot;: &amp;quot;address&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;STRING&amp;quot;, &amp;quot;mode&amp;quot;: &amp;quot;NULLABLE&amp;quot;
    },
    {
        &amp;quot;name&amp;quot;: &amp;quot;mail&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;STRING&amp;quot;, &amp;quot;mode&amp;quot;: &amp;quot;NULLABLE&amp;quot;
    }
]
&lt;/pre&gt;&lt;/div&gt;


&lt;hr&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h4&gt;3. Create a table in BigQuery&lt;/h4&gt;
&lt;p&gt;We are going to create two python file, ie, &lt;strong&gt;&lt;code&gt;createConfig.py&lt;/code&gt;&lt;/strong&gt; that will keep schema configuration and &lt;strong&gt;&lt;code&gt;tableCreate.py&lt;/code&gt;&lt;/strong&gt; that will execute the table creation API call to bigquery. We are going to use &lt;strong&gt;&lt;code&gt;Google Application Default Credentials&lt;/code&gt;&lt;/strong&gt; to authorize our python application to talk to bigquery APIs.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;cat createConfig.py

&lt;span class="nv"&gt;TableObject&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;tableReference&amp;quot;&lt;/span&gt;: &lt;span class="o"&gt;{&lt;/span&gt;
      &lt;span class="s2"&gt;&amp;quot;projectId&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;mimetic-slate&amp;quot;&lt;/span&gt;,
      &lt;span class="s2"&gt;&amp;quot;tableId&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;StreamTable&amp;quot;&lt;/span&gt;,
      &lt;span class="s2"&gt;&amp;quot;datasetId&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;BQ_Dataset&amp;quot;&lt;/span&gt;,
    &lt;span class="o"&gt;}&lt;/span&gt;,

    &lt;span class="s2"&gt;&amp;quot;schema&amp;quot;&lt;/span&gt;: &lt;span class="o"&gt;{&lt;/span&gt;
      &lt;span class="s2"&gt;&amp;quot;fields&amp;quot;&lt;/span&gt;: &lt;span class="o"&gt;[&lt;/span&gt;
          &lt;span class="o"&gt;{&lt;/span&gt;
              &lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;username&amp;quot;&lt;/span&gt;,
              &lt;span class="s2"&gt;&amp;quot;type&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;STRING&amp;quot;&lt;/span&gt;,
              &lt;span class="s2"&gt;&amp;quot;mode&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;NULLABLE&amp;quot;&lt;/span&gt;
          &lt;span class="o"&gt;}&lt;/span&gt;,
          &lt;span class="o"&gt;{&lt;/span&gt;
              &lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;,
              &lt;span class="s2"&gt;&amp;quot;type&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;STRING&amp;quot;&lt;/span&gt;,
              &lt;span class="s2"&gt;&amp;quot;mode&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;NULLABLE&amp;quot;&lt;/span&gt;
          &lt;span class="o"&gt;}&lt;/span&gt;,
          &lt;span class="o"&gt;{&lt;/span&gt;
              &lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;birthdate&amp;quot;&lt;/span&gt;,
              &lt;span class="s2"&gt;&amp;quot;type&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;STRING&amp;quot;&lt;/span&gt;,
              &lt;span class="s2"&gt;&amp;quot;mode&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;NULLABLE&amp;quot;&lt;/span&gt;
          &lt;span class="o"&gt;}&lt;/span&gt;,
          &lt;span class="o"&gt;{&lt;/span&gt;
              &lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;sex&amp;quot;&lt;/span&gt;,
              &lt;span class="s2"&gt;&amp;quot;type&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;STRING&amp;quot;&lt;/span&gt;,
              &lt;span class="s2"&gt;&amp;quot;mode&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;NULLABLE&amp;quot;&lt;/span&gt;
          &lt;span class="o"&gt;}&lt;/span&gt;,
          &lt;span class="o"&gt;{&lt;/span&gt;
              &lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;address&amp;quot;&lt;/span&gt;,
              &lt;span class="s2"&gt;&amp;quot;type&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;STRING&amp;quot;&lt;/span&gt;,
              &lt;span class="s2"&gt;&amp;quot;mode&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;NULLABLE&amp;quot;&lt;/span&gt;
          &lt;span class="o"&gt;}&lt;/span&gt;,
          &lt;span class="o"&gt;{&lt;/span&gt;
              &lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;mail&amp;quot;&lt;/span&gt;,
              &lt;span class="s2"&gt;&amp;quot;type&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;STRING&amp;quot;&lt;/span&gt;,
              &lt;span class="s2"&gt;&amp;quot;mode&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;NULLABLE&amp;quot;&lt;/span&gt;
          &lt;span class="o"&gt;}&lt;/span&gt;
      &lt;span class="o"&gt;]&lt;/span&gt;,
  &lt;span class="o"&gt;}&lt;/span&gt;,
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We are going to use &lt;strong&gt; &lt;a href="https://developers.google.com/api-client-library/python/"&gt;google-api-python-client&lt;/a&gt;&lt;/strong&gt; library for interacting to our bigquery APIs.&lt;/p&gt;
&lt;p&gt;We are building service object by calling our API name and version supported by API. In this case we are using &lt;strong&gt;&lt;code&gt;bigquery&lt;/code&gt;&lt;/strong&gt; with version &lt;strong&gt;&lt;code&gt;v2&lt;/code&gt;&lt;/strong&gt;. This service object will be used to make tables related operation.  As of now we are going to use &lt;strong&gt;&lt;code&gt;insert&lt;/code&gt;&lt;/strong&gt; function to make table.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;GoogleCredentials.get_application_default()&lt;/code&gt;&lt;/strong&gt; will read the credentials stored in my system. Either you need to export a variable mentioned in reference with service account key or you setup an google SDK which will store default credentials inside your home directory.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;ls ~/.config/gcloud/*.json
~/.config/gcloud/application_default_credentials.json
&lt;/pre&gt;&lt;/div&gt;


&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt; 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="c"&gt;#!/usr/bin/env python&lt;/span&gt;

&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;googleapiclient&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;discovery&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;oauth2client.client&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;GoogleCredentials&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;tableCreate&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;TableObject&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;createTable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bigquery&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;tables&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;bigquery&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tables&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="c"&gt;#insert utility make call to BQ API with payload \&lt;/span&gt;
    &lt;span class="c"&gt;#(TableObject) contains schema and table-name information&lt;/span&gt;
    &lt;span class="n"&gt;tableStatusObject&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tables&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;insert&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="n"&gt;projectId&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;mimetic-slate&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; \
     &lt;span class="n"&gt;datasetId&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;BQ_Dataset&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;body&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;TableObject&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;execute&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="c"&gt;# [END]&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="c"&gt;#to get credentials from my laptop&lt;/span&gt;
    &lt;span class="n"&gt;credentials&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;GoogleCredentials&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_application_default&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="c"&gt;# Construct the service object for interacting with the BigQuery API.&lt;/span&gt;
    &lt;span class="n"&gt;bigquery&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;discovery&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;build&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;bigquery&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;v2&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;credentials&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;credentials&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;createTable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bigquery&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;__main__&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;main&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;BQ Table Creator !!&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;

&lt;p&gt;Run above to program to create table with name &lt;strong&gt;StreamTable&lt;/strong&gt; in bigquery dataset &lt;strong&gt;&lt;code&gt;BQ_Dataset&lt;/code&gt;&lt;/strong&gt;. Make sure you have created dataset already.&lt;/p&gt;
&lt;p&gt;You can verify the table created by visiting bigquery UI. Visit : &lt;a href="https://bigquery.cloud.google.com"&gt;https://bigquery.cloud.google.com&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;You can also verify table creation by running &lt;code&gt;bq&lt;/code&gt; CLI commands&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;bq ls BQ_Dataset    
    tableId     Type    Labels   Time Partitioning  
 ------------- ------- -------- -------------------
  StreamTable   TABLE                               
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="bq show " src="/images/bq-series/part2/bq-show-2.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Reference &lt;/strong&gt; :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Table Creation : &lt;/strong&gt; &lt;a href="https://developers.google.com/resources/api-libraries/documentation/bigquery/v2/python/latest/bigquery_v2.tables.html"&gt;https://developers.google.com/resources/api-libraries/documentation/bigquery/v2/python/latest/bigquery_v2.tables.html&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Google Application Default Credentials :&lt;/strong&gt; &lt;a href="https://developers.google.com/identity/protocols/application-default-credentials"&gt;https://developers.google.com/identity/protocols/application-default-credentials&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Build the service object :&lt;/strong&gt; &lt;a href="https://developers.google.com/api-client-library/python/start/get_started#build-the-service-object"&gt;https://developers.google.com/api-client-library/python/start/get_started#build-the-service-object&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h4&gt;4. Streaming into Bigquery&lt;/h4&gt;
&lt;p&gt;Now, we have table created and data queued into redis list, We are ready to stream right away by running a python script, lets call this script &lt;strong&gt;&lt;code&gt;bq-streamer.py&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt; 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="c"&gt;#!/usr/bin/env python&lt;/span&gt;

&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;googleapiclient&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;discovery&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;oauth2client.client&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;GoogleCredentials&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;json&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nn"&gt;time&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nn"&gt;copy&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;redis&lt;/span&gt;


&lt;span class="n"&gt;batchCount&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;

&lt;span class="n"&gt;redisStream&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;redis&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Redis&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;host&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;127.0.0.1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
             &lt;span class="n"&gt;port&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;6379&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;password&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;streamObject&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="s"&gt;&amp;quot;rows&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
      &lt;span class="c"&gt;#{ &amp;quot;json&amp;quot;: {# Represents a single JSON object. } }&lt;/span&gt;
    &lt;span class="p"&gt;],&lt;/span&gt;
 &lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="c"&gt;#[START Streaming batcher]&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;streamBuilder&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="c"&gt;#Every API needs a refresh copy of dict&lt;/span&gt;
    &lt;span class="n"&gt;newStreamObject&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;copy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;deepcopy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;streamObject&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;currentCounter&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="n"&gt;currentCounter&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;batchCount&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;packet&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;redisStream&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;brpop&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;redisList&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="n"&gt;timeout&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;newStreamObject&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;rows&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;json&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;json&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loads&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;packet&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;})&lt;/span&gt;
        &lt;span class="n"&gt;currentCounter&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt;  &lt;span class="mi"&gt;1&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;newStreamObject&lt;/span&gt;
&lt;span class="c"&gt;#[END]&lt;/span&gt;


&lt;span class="c"&gt;# [START Streaming Utility]&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;streamUtils&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bigquery&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;tabledata&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;bigquery&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tabledata&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="c"&gt;#Run infinitely&lt;/span&gt;
    &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;streamBuildBatch&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;streamBuilder&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

        &lt;span class="c"&gt;#BQ API to insert bulk data into table&lt;/span&gt;
        &lt;span class="n"&gt;insertStatusObject&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tabledata&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;insertAll&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;projectId&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;mimetic-slate&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; \
        &lt;span class="n"&gt;datasetId&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;BQ_Dataset&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;tableId&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;StreamTable&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; \
        &lt;span class="n"&gt;body&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;streamBuildBatch&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;execute&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;


&lt;span class="c"&gt;# [ MAIN]&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="n"&gt;credentials&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;GoogleCredentials&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_application_default&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="c"&gt;# Construct the service object for interacting with the BigQuery API.&lt;/span&gt;
    &lt;span class="n"&gt;bigquery&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;discovery&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;build&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;bigquery&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;v2&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;credentials&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;credentials&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c"&gt;#Stream utility&lt;/span&gt;
    &lt;span class="n"&gt;streamUtils&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bigquery&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c"&gt;# [END]&lt;/span&gt;


&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;__main__&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;main&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;

&lt;p&gt;Above program is going to read redis running on &lt;code&gt;127.0.0.1:6379&lt;/code&gt; from list name &lt;strong&gt;&lt;code&gt;redisList&lt;/code&gt;&lt;/strong&gt; and build a dict object &lt;strong&gt;&lt;code&gt;streamObject&lt;/code&gt;&lt;/strong&gt; that is accepted by bq streaming API. We are calling &lt;strong&gt;&lt;code&gt;insertAll&lt;/code&gt;&lt;/strong&gt; utility to submit our streaming request to bigquery API.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;insertAll&lt;/code&gt;&lt;/strong&gt; takes &lt;strong&gt;&lt;code&gt;projectId, datasetId, tableId&lt;/code&gt;&lt;/strong&gt; as an argument and &lt;strong&gt;&lt;code&gt;body&lt;/code&gt;&lt;/strong&gt; which contains your data to be streamed.&lt;/p&gt;
&lt;p&gt;Script has been configured to &lt;a href="https://redis.io/commands/brpop"&gt;pop&lt;/a&gt; &lt;code&gt;100 entries&lt;/code&gt; from redis list and prepare it to be pushed into table. Run &lt;strong&gt;&lt;code&gt;bq-streamer.py&lt;/code&gt;&lt;/strong&gt; script to start streaming data into bigquery table.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;#when bulk data is prepared, JSON payload in body argument would look like
{
    &amp;quot;rows&amp;quot;: [
        {
            &amp;quot;json&amp;quot;: {
                &amp;quot;username&amp;quot;: &amp;quot;nicholaswagner&amp;quot;,
                &amp;quot;name&amp;quot;: &amp;quot;Laura Scott&amp;quot;,
                &amp;quot;birthdate&amp;quot;: &amp;quot;1970-11-30&amp;quot;,
                &amp;quot;sex&amp;quot;: &amp;quot;F&amp;quot;,
                &amp;quot;address&amp;quot;: &amp;quot;788 Faulkner Locks Suite 687\nSanfordside, FL 50804-6818&amp;quot;,
                &amp;quot;mail&amp;quot;: &amp;quot;austinnathaniel@yahoo.com&amp;quot;
            }
        },
        {
           &amp;quot;json&amp;quot;: {
               &amp;quot;username&amp;quot;: &amp;quot;david27&amp;quot;,
               &amp;quot;name&amp;quot;: &amp;quot;Aaron Silva&amp;quot;,
               &amp;quot;birthdate&amp;quot;: &amp;quot;2003-09-17&amp;quot;,
               &amp;quot;sex&amp;quot;: &amp;quot;M&amp;quot;,
               &amp;quot;address&amp;quot;: &amp;quot;57976 Collins Loaf Apt. 843\nMichaelfort, VA 79233&amp;quot;,
               &amp;quot;mail&amp;quot;: &amp;quot;dbeck@hotmail.com&amp;quot;
           }
        },
        .....
        more data
        .....
}
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;Reference&lt;/strong&gt; :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Streaming insertAll :&lt;/strong&gt; &lt;a href="https://developers.google.com/resources/api-libraries/documentation/bigquery/v2/python/latest/bigquery_v2.tabledata.html"&gt;https://developers.google.com/resources/api-libraries/documentation/bigquery/v2/python/latest/bigquery_v2.tabledata.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h4&gt;5. Verify the data in BigQuery Table&lt;/h4&gt;
&lt;p&gt;After running streaming, you will start seeing something similar as shown below when you click table info. Clicking on preview will not show you any streamed data, it will take a while to appear but it will be in buffer to be available for query instantly.&lt;/p&gt;
&lt;div style="text-align: center"&gt;&lt;u&gt;&lt;b&gt;Buffer Statistics&lt;/b&gt;&lt;/u&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="streaming-1" src="/images/bq-series/part2/stream.png"&gt;&lt;/p&gt;
&lt;div style="text-align: center"&gt;&lt;u&gt;&lt;b&gt;Table Preview&lt;/b&gt;&lt;/u&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="streaming-2" src="/images/bq-series/part2/preview-none.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;SQL Query in BQ Table&lt;/h3&gt;
&lt;p&gt;We are going to run a simple query to show the output that shows your streamed data.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;SELECT * FROM [mimetic-slate:BQ_Dataset.StreamTable]
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="text-align: center"&gt;&lt;u&gt;&lt;b&gt;Query Result&lt;/u&gt;&lt;/b&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="query" src="/images/bq-series/part2/query-2.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Github reference&lt;/strong&gt; : &lt;a href="https://github.com/sunnykrGupta/Bigquery-series"&gt;https://github.com/sunnykrGupta/Bigquery-series&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;That's all from this &lt;strong&gt;series Part-II&lt;/strong&gt;. Hope you will get basic understanding of Streaming in BigQuery from this post. Streaming is helpful in cases when you want your data to be instantly available for query, helps in scenario where have a requirement of building real time analysis.&lt;/p&gt;
&lt;p&gt;I would appreciate feedback via comments. In next blog which is part of this series, I will be covering &lt;strong&gt;Patching and Updating table schema in Bigquery&lt;/strong&gt; which is important when you want to add fields in table.&lt;/p&gt;
&lt;hr&gt;</content><category term="Google BigQuery"></category><category term="GCP"></category><category term="Redis"></category><category term="Google Cloud Platform"></category></entry><entry><title>Export &amp; Load Job with MongoDB - BigQuery Part-I</title><link href="https://sunnykrGupta.github.io/export-load-job-with-mongodb-bigquery-part-i.html" rel="alternate"></link><published>2017-10-16T01:12:31+05:30</published><updated>2017-10-16T01:12:31+05:30</updated><author><name>Sunny Kr Gupta</name></author><id>tag:sunnykrgupta.github.io,2017-10-16:/export-load-job-with-mongodb-bigquery-part-i.html</id><summary type="html">&lt;p&gt;To get into fundamentals of Google BigQuery and related jobs needed to get your data inside BigQuery system. We are going to use a mongoDB server to export our data and going to import into BigQuery tables. There are several other ways to import data into bigquery.&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;img alt="Google BigQuery" src="/images/bq-series/part1/bquery.svg"&gt;&lt;/p&gt;
&lt;p&gt;This blog is intended for audience who wanted to get into fundamentals of BigQuery (short for BQ) and related jobs needed to get your data inside BigQuery system.&lt;/p&gt;
&lt;h3&gt;&lt;a href="https://cloud.google.com/bigquery/what-is-bigquery"&gt;Google BigQuery&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;What ?&lt;/strong&gt; - BigQuery is Google's fully managed, petabyte scale, low cost analytics data warehouse.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Why ?&lt;/strong&gt; - BigQuery is NoOpsâthere is no infrastructure to manage and you don't need a database administratorâso you can focus on analyzing data to find meaningful insights, use familiar SQL, and take advantage of our pay-as-you-go model.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How ?&lt;/strong&gt; - Signup to &lt;a href="https://bigquery.cloud.google.com/"&gt;Google Cloud platform - GCP&lt;/a&gt; using your google account, start loading your data and leverage the power of this NoOps system.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Terminology in BigQuery&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Dataset&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;A dataset is contained within a specific project. Datasets enable you to organize and control access to your tables. A table must belong to a dataset, so you need to create at least one dataset before loading data into BigQuery.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tables&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;A BigQuery table contains individual records organized in rows, and a data type assigned to each column (also called a field).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Schema&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Each Every table is defined by a schema that describes field names, types, and other information. You can specify the schema of a table during the initial table creation request, or you can create a table without a schema and declare the schema in the query or load job that first populates the table. If you need to change the schema later, you can update the schema.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Loading Data into BigQuery&lt;/h3&gt;
&lt;p&gt;In this post, we are going to use a mongoDB server to export our data and going to import into BQ. There are several other ways to import data into BQ.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h4&gt;1. Export data from MongoDB&lt;/h4&gt;
&lt;p&gt;&lt;img alt="MongoDB" src="/images/bq-series/part1/mongo-db-blog.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;In this example, i have a database in mongoDB server with name &lt;code&gt;restaurantdb&lt;/code&gt; with collection name &lt;code&gt;restaurantCollection&lt;/code&gt;. We are going to export using &lt;code&gt;mongoexport&lt;/code&gt; binary available with mongodb server tools.&lt;/p&gt;
&lt;p&gt;&lt;img alt="MongoDB server" src="/images/bq-series/part1/mongoexport.png"&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;mongoexport -d restaurantdb -c restaurantCollection -o restaurant.json
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Once export is done, we can see content of &lt;code&gt;restaurant.json&lt;/code&gt; file&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;head -n &lt;span class="m"&gt;1&lt;/span&gt; restaurant.json

&lt;span class="o"&gt;{&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; : &lt;span class="o"&gt;{&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$oid&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;55f14312c7447c3da7051b26&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;}&lt;/span&gt;, &lt;span class="se"&gt;\&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;URL&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;http://www.just-eat.co.uk/restaurants-cn-chinese-cardiff/menu&amp;quot;&lt;/span&gt;, &lt;span class="se"&gt;\&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;address&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;228 City Road&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;.CN Chinese&amp;quot;&lt;/span&gt;, &lt;span class="se"&gt;\&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;outcode&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;CF24&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;postcode&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;3JH&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;type_of_food&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;Chinese&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;}&lt;/span&gt;

//pretty json
&lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;.CN Chinese&amp;quot;&lt;/span&gt;,
    &lt;span class="s2"&gt;&amp;quot;URL&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;http://www.just-eat.co.uk/restaurants-cn-chinese-cardiff/menu&amp;quot;&lt;/span&gt;,
    &lt;span class="s2"&gt;&amp;quot;outcode&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;CF24&amp;quot;&lt;/span&gt;,
    &lt;span class="s2"&gt;&amp;quot;postcode&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;3JH&amp;quot;&lt;/span&gt;,
    &lt;span class="s2"&gt;&amp;quot;address&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;228 City Road&amp;quot;&lt;/span&gt;,
    &lt;span class="s2"&gt;&amp;quot;_id&amp;quot;&lt;/span&gt;: &lt;span class="o"&gt;{&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$oid&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;55f14312c7447c3da7051b26&amp;quot;&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;,
    &lt;span class="s2"&gt;&amp;quot;type_of_food&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;Chinese&amp;quot;&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;hr&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h4&gt;2. Prepare schema for Table&lt;/h4&gt;
&lt;p&gt;Now we have our data ready in json format to be imported into BQ table. We need schema to design in order to import these records. Schema is skeleton of each field with datatype and the field not described in schema will not be imported. We have given all fields as &lt;strong&gt;NULLABLE&lt;/strong&gt; ie, if field didn't came in any records BQ will define null value.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;cat restaurantSchema.json
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;[
    {
        &amp;quot;name&amp;quot;: &amp;quot;name&amp;quot;,
        &amp;quot;type&amp;quot;: &amp;quot;STRING&amp;quot;,
        &amp;quot;mode&amp;quot;: &amp;quot;NULLABLE&amp;quot;
    },
    {
        &amp;quot;name&amp;quot;: &amp;quot;URL&amp;quot;,
        &amp;quot;type&amp;quot;: &amp;quot;STRING&amp;quot;,
        &amp;quot;mode&amp;quot;: &amp;quot;NULLABLE&amp;quot;
    },
    {
        &amp;quot;name&amp;quot;: &amp;quot;address&amp;quot;,
        &amp;quot;type&amp;quot;: &amp;quot;STRING&amp;quot;,
        &amp;quot;mode&amp;quot;: &amp;quot;NULLABLE&amp;quot;
    },
    {
        &amp;quot;name&amp;quot;: &amp;quot;outcode&amp;quot;,
        &amp;quot;type&amp;quot;: &amp;quot;STRING&amp;quot;,
        &amp;quot;mode&amp;quot;: &amp;quot;NULLABLE&amp;quot;
    },
    {
        &amp;quot;name&amp;quot;: &amp;quot;postcode&amp;quot;,
        &amp;quot;type&amp;quot;: &amp;quot;STRING&amp;quot;,
        &amp;quot;mode&amp;quot;: &amp;quot;NULLABLE&amp;quot;
    },
    {
        &amp;quot;name&amp;quot;: &amp;quot;type_of_food&amp;quot;,
        &amp;quot;type&amp;quot;: &amp;quot;STRING&amp;quot;,
        &amp;quot;mode&amp;quot;: &amp;quot;NULLABLE&amp;quot;
    }
]
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Reference for schema : &lt;a href="https://cloud.google.com/bigquery/docs/schemas"&gt;https://cloud.google.com/bigquery/docs/schemas&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h4&gt;3. Google Cloud SDK Installation&lt;/h4&gt;
&lt;p&gt;We have schema, data ready to be imported in BQ, but in order to talk to APIs of GCP services, we need Cloud SDK to be installed in our system. We are going to use &lt;code&gt;gcloud&lt;/code&gt; CLI tools in order to interact with our Cloud services running in GCP.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Installation : &lt;/strong&gt; &lt;a href="https://cloud.google.com/sdk/"&gt;https://cloud.google.com/sdk/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Once installation is done, run following command to verify account setup.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;gcloud auth list

&lt;span class="nv"&gt;$ &lt;/span&gt;gcloud config list
&lt;/pre&gt;&lt;/div&gt;


&lt;hr&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h4&gt;4. Create BigQuery Dataset&lt;/h4&gt;
&lt;p&gt;Go to your BigQuery in google console. &lt;a href="https://bigquery.cloud.google.com"&gt;https://bigquery.cloud.google.com&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Follow below instruction to create dataset.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Step 1:&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img alt="Create Dataset Step 1" src="/images/bq-series/part1/create-ds-1.png"&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Step 2:&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img alt="Create Dataset Step 2" src="/images/bq-series/part1/create-ds-2.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;To create dataset through &lt;code&gt;bq&lt;/code&gt; command line interface.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;bq mk -d --data_location&lt;span class="o"&gt;=&lt;/span&gt;US   BQ_Dataset

// Verify your dataset creation
&lt;span class="nv"&gt;$ &lt;/span&gt;bq ls

    datasetId
 ----------------
  BQ_Dataset
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;Read :&lt;/strong&gt; &lt;a href="https://cloud.google.com/bigquery/docs/datasets#create-dataset"&gt;https://cloud.google.com/bigquery/docs/datasets#create-dataset&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h4&gt;5. Load data into BigQuery&lt;/h4&gt;
&lt;p&gt;Now, my directory consists two files ie, data and schema.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;tree
.
âââ restaurant.json
âââ restaurantSchema.json

&lt;span class="m"&gt;0&lt;/span&gt; directories, &lt;span class="m"&gt;2&lt;/span&gt; files
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Run command to load data into BQ.  Once you submit load job, it will take seconds to minute depends on size of data you are importing into BQ table.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;Ex&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;bq&lt;/span&gt; &lt;span class="n"&gt;load&lt;/span&gt; &lt;span class="o"&gt;--&lt;/span&gt;&lt;span class="n"&gt;project_id&lt;/span&gt;&lt;span class="o"&gt;=&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;PROJECT&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;ID&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="o"&gt;--&lt;/span&gt;&lt;span class="n"&gt;source_format&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;NEWLINE_DELIMITED_JSON&lt;/span&gt; &lt;span class="o"&gt;\&lt;/span&gt;
    &lt;span class="n"&gt;mydataset&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;mytable&lt;/span&gt; &lt;span class="o"&gt;./&lt;/span&gt;&lt;span class="n"&gt;myfile&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;json&lt;/span&gt; &lt;span class="o"&gt;./&lt;/span&gt;&lt;span class="n"&gt;myschema&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;json&lt;/span&gt;

&lt;span class="n"&gt;$&lt;/span&gt; &lt;span class="n"&gt;bq&lt;/span&gt; &lt;span class="n"&gt;load&lt;/span&gt; &lt;span class="o"&gt;--&lt;/span&gt;&lt;span class="n"&gt;project_id&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;mimetic&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;slate&lt;/span&gt;   &lt;span class="o"&gt;\&lt;/span&gt;
    &lt;span class="o"&gt;--&lt;/span&gt;&lt;span class="n"&gt;source_format&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;NEWLINE_DELIMITED_JSON&lt;/span&gt; &lt;span class="o"&gt;--&lt;/span&gt;&lt;span class="n"&gt;max_bad_records&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt; &lt;span class="o"&gt;\&lt;/span&gt;
    &lt;span class="n"&gt;BQ_Dataset&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;Restaurant&lt;/span&gt; &lt;span class="o"&gt;./&lt;/span&gt;&lt;span class="n"&gt;restaurant&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;json&lt;/span&gt;  &lt;span class="o"&gt;./&lt;/span&gt;&lt;span class="n"&gt;restaurantSchema&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;json&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;code&gt;--max_bad_records 10&lt;/code&gt; are additional flags to allow 10 bad records while importing your job, exceeding this value will result in import failure.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Import through Cloud Storage&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="Cloud Storage" src="/images/bq-series/part1/google-cloud-storage.png"&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Another methods to import the data through &lt;code&gt;Cloud Storage&lt;/code&gt;, this method is lot faster compared to above one.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Read : &lt;/strong&gt; &lt;a href="https://cloud.google.com/storage/docs/creating-buckets"&gt;https://cloud.google.com/storage/docs/creating-buckets&lt;/a&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;//to create Cloud storage bucket for this example.
$ gsutil mb  gs://bq-storage

//to verify bucket creation
$ gsutil ls
gs://bq-storage/
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now , we will upload our data &lt;code&gt;restaurant.json&lt;/code&gt; to storage in bucket &lt;code&gt;gs://bq-storage/&lt;/code&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;//Run command to upload
$ gsutil cp restaurant.json gs://bq-storage/restaurant.json
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now, we can use storage path of object to import into BQ tables.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;bq load --project_id&lt;span class="o"&gt;=&lt;/span&gt;mimetic-slate  &lt;span class="se"&gt;\&lt;/span&gt;
    --source_format&lt;span class="o"&gt;=&lt;/span&gt;NEWLINE_DELIMITED_JSON --max_bad_records &lt;span class="m"&gt;10&lt;/span&gt;  &lt;span class="se"&gt;\&lt;/span&gt;
    BQ_Dataset.Restaurant &lt;span class="se"&gt;\&lt;/span&gt;
    gs://bq-storage/restaurant.json ./restaurantSchema.json
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;p&gt;After &lt;code&gt;bq load&lt;/code&gt; finished, run following command to verify the table creation.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;bq show BQ_Dataset.Restaurant
&lt;/pre&gt;&lt;/div&gt;


&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Output will be similar to this :&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img alt="bq show" src="/images/bq-series/part1/bq-show.png"&gt;&lt;/p&gt;
&lt;p&gt;You can also verify in bigQuery UI after hitting refresh. Visit to table and click &lt;em&gt;preview&lt;/em&gt;. You will start seeing records in table.&lt;/p&gt;
&lt;p&gt;&lt;img alt="bq show" src="/images/bq-series/part1/table-preview.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;More into bq CLI :&lt;/strong&gt; &lt;a href="https://cloud.google.com/storage/docs/creating-buckets"&gt;https://cloud.google.com/bigquery/bq-command-line-tool&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;More into gsutil CLI :&lt;/strong&gt; &lt;a href="https://cloud.google.com/storage/docs/quickstart-gsutil"&gt;https://cloud.google.com/storage/docs/quickstart-gsutil&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;SQL Query in BQ Table&lt;/h3&gt;
&lt;p&gt;We are going to run a simple query to show the output.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;SELECT
  name,
  address
FROM
  [mimetic-slate:BQ_Dataset.Restaurant]
WHERE
  type_of_food = &amp;#39;Thai&amp;#39;
GROUP BY
  name, address
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="query" src="/images/bq-series/part1/query.png"&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Github reference&lt;/strong&gt; : &lt;a href="https://github.com/sunnykrGupta/Bigquery-series"&gt;https://github.com/sunnykrGupta/Bigquery-series&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;BigQuery is a query service that allows you to run SQL-like queries against
multiple terabytes of data in a matter of seconds. The technology is one of the Googleâs core technologies, like MapReduce and Bigtable, and has been used
by Google internally for various analytic tasks since 2006.&lt;/li&gt;
&lt;li&gt;While MapReduce is suitable for long-running batch processes such as data
mining, BigQuery is the best choice for ad hoc OLAP/BI queries that require
results as fast as possible.&lt;/li&gt;
&lt;li&gt;Wildcard can also be applied into Bigquery tables to expand your computations to multiple tables.&lt;/li&gt;
&lt;li&gt;BigQuery is the cloud-powered massively parallel
query database that provides extremely high full-scan query performance
and cost effectiveness compared to traditional data warehouse solutions
and appliances&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Next from here:&lt;/strong&gt; Play around with more &lt;a href="https://cloud.google.com/bigquery/docs/how-to"&gt;functionality available in BigQuery&lt;/a&gt; and dive into it for more computation hungry jobs.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;p&gt;That's all from this &lt;strong&gt;series Part-I&lt;/strong&gt;. Hope you get basic understanding of import jobs, storage and basic outline of BigQuery from this post. I have seen power of BigQuery in my workplace to crunch 100-120 TB of data and getting results in minute or two, its really incredibly awesome. I would appreciate a feedback via comments available below and claps on medium.&lt;/p&gt;
&lt;p&gt;In next blog which is part of this series, i will be covering more into &lt;em&gt;Streaming feature available in Bigquery&lt;/em&gt; to push data in BQ tables in real-time to make it available for instant query on changing dataset.&lt;/p&gt;
&lt;h5&gt;Medium Blog : &lt;a href="https://medium.com/@sunnykrgupta/export-load-job-with-mongodb-bigquery-part-i-64a00eb5266b"&gt;medium.com/@sunnykrgupta/export-load-job-with-mongodb-bigquery-part-i&lt;/a&gt;&lt;/h5&gt;</content><category term="Google BigQuery"></category><category term="GCP"></category><category term="MongoDB"></category><category term="Cloud Storage"></category><category term="Google Cloud Platform"></category></entry><entry><title>Installation of VNC server on Ubuntu</title><link href="https://sunnykrGupta.github.io/installation-of-vnc-server-on-ubuntu.html" rel="alternate"></link><published>2017-09-04T22:51:50+05:30</published><updated>2017-09-04T22:51:50+05:30</updated><author><name>Sunny Kr Gupta</name></author><id>tag:sunnykrgupta.github.io,2017-09-04:/installation-of-vnc-server-on-ubuntu.html</id><summary type="html">&lt;p&gt;This blog is intended for people who wanted to install GUI or desktop environment on linux &lt;a href="https://en.wikipedia.org/wiki/Server_(computing)"&gt;servers&lt;/a&gt; running on &lt;a href="https://en.wikipedia.org/wiki/Cloud_computing"&gt;cloud&lt;/a&gt; and connect.&lt;/p&gt;
&lt;p&gt;We are going to use VNC (Virtual Network Computing) protocol for accessing our remote desktop server.&lt;/p&gt;
&lt;h4&gt;What is VNC ?&lt;/h4&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Virtual_Network_Computing"&gt;Virtual network computing&lt;/a&gt;, or VNC, is a graphical desktop â¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;This blog is intended for people who wanted to install GUI or desktop environment on linux &lt;a href="https://en.wikipedia.org/wiki/Server_(computing)"&gt;servers&lt;/a&gt; running on &lt;a href="https://en.wikipedia.org/wiki/Cloud_computing"&gt;cloud&lt;/a&gt; and connect.&lt;/p&gt;
&lt;p&gt;We are going to use VNC (Virtual Network Computing) protocol for accessing our remote desktop server.&lt;/p&gt;
&lt;h4&gt;What is VNC ?&lt;/h4&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Virtual_Network_Computing"&gt;Virtual network computing&lt;/a&gt;, or VNC, is a graphical desktop sharing system that allows you to control one computer remotely from another. A VNC server transfers keyboard and mouse events, and displays the remote hostâs screen via a network connection, which allows you to operate a full &lt;a href="https://en.wikipedia.org/wiki/Desktop_environment"&gt;desktop environment&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Basically ubuntu server and ubuntu cloud editions does not contains GUI, which needs to be installed before installing VNC server. Please note that server and cloud editions are carefully designed to utilize less hardware resources ( minimal environment ), installing GUI might leads to high hardware utilization.&lt;/p&gt;
&lt;h4&gt;Why I needed desktop environment in remote server ?&lt;/h4&gt;
&lt;p&gt;Just to explain a use case, let me tell me you how I ended up using VNC in first place. I was working on a problem which relates with cloud latency testing. My Friend, &lt;a href="https://www.linkedin.com/in/neekneeraj/"&gt;Neeraj&lt;/a&gt; &lt;em&gt;(whose work revolves around core JS research &amp;amp; development)&lt;/em&gt; developed a javascript code that makes an cross origin &lt;a href="(https://en.wikipedia.org/wiki/Web_API)"&gt;HTTP API&lt;/a&gt; call to a &lt;a href="https://en.wikipedia.org/wiki/Cloud_load_balancing"&gt;loadbalancer&lt;/a&gt; near to the geographical location of browser and response will be delivered from loadbalancer in geographic proximity. To test this setup, executing JS code and to use &lt;a href="https://developer.mozilla.org/en/docs/Tools/Browser_Console"&gt;developer console&lt;/a&gt; to see what's happening under the network layer, we were in need of a browser engine in different geographical location. I could have used some online paid or free service to get browser rented, services like &lt;a href="https://www.browserstack.com/"&gt;browserstack&lt;/a&gt; or other alternatives but that has free minutes based trial restrictions.&lt;/p&gt;
&lt;hr&gt;
&lt;h3&gt;Install a Desktop and VNC Server on Ubtunu 14.04&lt;/h3&gt;
&lt;h4&gt;Step 1 - Install Ubuntu desktop&lt;/h4&gt;
&lt;p&gt;Start installing below &lt;em&gt;gnome packages&lt;/em&gt; which helps VNC to load properly . These packages are required for all editions including &lt;em&gt;ubuntu desktop&lt;/em&gt; .&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;sudo apt-get install --no-install-recommends ubuntu-desktop gnome-panel gnome-settings-daemon metacity nautilus gnome-terminal gnome-core
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Step 2 - Install vnc4server package.&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;sudo apt-get install vnc4server
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Step 3 - Make configuration changes in vncserver&lt;/h4&gt;
&lt;p&gt;Open &lt;code&gt;/usr/bin/vncserver&lt;/code&gt; file and edit as follows . Before editing, make a backup copy.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;sudo cp /usr/bin/vncserver /usr/bin/vncserver.bkp

&lt;span class="nv"&gt;$ &lt;/span&gt;sudo vim /usr/bin/vncserver

&lt;span class="c"&gt;#Find this line &amp;quot;# exec /etc/X11/xinit/xinitrcnn&amp;quot;.&lt;/span&gt;
&lt;span class="c"&gt;#and add these lines like below&lt;/span&gt;

    &lt;span class="s2"&gt;&amp;quot;# exec /etc/X11/xinit/xinitrcnn&amp;quot;&lt;/span&gt;.
       &lt;span class="s2"&gt;&amp;quot;gnome-panel &amp;amp;n&amp;quot;&lt;/span&gt;.
       &lt;span class="s2"&gt;&amp;quot;gnome-settings-daemon &amp;amp;n&amp;quot;&lt;/span&gt;.
       &lt;span class="s2"&gt;&amp;quot;metacity &amp;amp;n&amp;quot;&lt;/span&gt;.
       &lt;span class="s2"&gt;&amp;quot;nautilus &amp;amp;n&amp;quot;&lt;/span&gt;.
       &lt;span class="s2"&gt;&amp;quot;gnome-terminal &amp;amp;n&amp;quot;&lt;/span&gt;.
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Step 4 - Start your vncserver&lt;/h4&gt;
&lt;p&gt;Now type the command &lt;code&gt;vncserver&lt;/code&gt; to start VNC session. you will be prompted for creating new vnc password.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;vncserver
You will require a password to access your desktops through VNC Clients.
Password:******
Verify:******

xauth: file /root/.Xauthority does not exist
New &lt;span class="s1"&gt;&amp;#39;ubuntu-desktop:1 (root)&amp;#39;&lt;/span&gt; desktop is ubuntu-desktop:1

Starting applications specified in /root/.vnc/xstartup
Log file is /root/.vnc/ubuntu-desktop:1.log
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Step 5 - To check VNC server has started, follow&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;netstat -tulpn

Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        &lt;span class="m"&gt;0&lt;/span&gt;      &lt;span class="m"&gt;0&lt;/span&gt; 0.0.0.0:6001            0.0.0.0:*               LISTEN      28372/Xvnc4
tcp6       &lt;span class="m"&gt;0&lt;/span&gt;      &lt;span class="m"&gt;0&lt;/span&gt; :::5901                 :::*                    LISTEN      28372/Xvnc4
&lt;/pre&gt;&lt;/div&gt;


&lt;blockquote&gt;
&lt;p&gt;VNC server is running and listening on &lt;strong&gt;5901 port&lt;/strong&gt;. Make sure your firewall allows &lt;strong&gt;inbound&lt;/strong&gt; TCP connection to this port.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;Step 6 - Configure your Firewall&lt;/h4&gt;
&lt;p&gt;If &lt;strong&gt;firewall&lt;/strong&gt; is active, you need to open ports for inbound communication. If no firewall is enabled, you can skip this section.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="c"&gt;#allow SSH&lt;/span&gt;
&lt;span class="nv"&gt;$ &lt;/span&gt;sudo ufw allow OpenSSH

&lt;span class="c"&gt;#allowing single port 5901 port&lt;/span&gt;
&lt;span class="nv"&gt;$ &lt;/span&gt;sudo ufw allow 5901/tcp

&lt;span class="c"&gt;#To allow series of port 5901 - 5910, follow&lt;/span&gt;
&lt;span class="nv"&gt;$ &lt;/span&gt;sudo ufw allow 5901:5910/tcp

&lt;span class="c"&gt;#To check firewall rules&lt;/span&gt;
&lt;span class="nv"&gt;$ &lt;/span&gt;sudo  ufw status verbose

Status: active
Logging: on &lt;span class="o"&gt;(&lt;/span&gt;low&lt;span class="o"&gt;)&lt;/span&gt;
Default: deny &lt;span class="o"&gt;(&lt;/span&gt;incoming&lt;span class="o"&gt;)&lt;/span&gt;, allow &lt;span class="o"&gt;(&lt;/span&gt;outgoing&lt;span class="o"&gt;)&lt;/span&gt;, disabled &lt;span class="o"&gt;(&lt;/span&gt;routed&lt;span class="o"&gt;)&lt;/span&gt;
New profiles: skip

To                         Action      From
--                         ------      ----
22/tcp &lt;span class="o"&gt;(&lt;/span&gt;OpenSSH&lt;span class="o"&gt;)&lt;/span&gt;           ALLOW IN    Anywhere
5901:5910/tcp              ALLOW IN    Anywhere
22/tcp &lt;span class="o"&gt;(&lt;/span&gt;OpenSSH &lt;span class="o"&gt;(&lt;/span&gt;v6&lt;span class="o"&gt;))&lt;/span&gt;      ALLOW IN    Anywhere &lt;span class="o"&gt;(&lt;/span&gt;v6&lt;span class="o"&gt;)&lt;/span&gt;
5901:5910/tcp &lt;span class="o"&gt;(&lt;/span&gt;v6&lt;span class="o"&gt;)&lt;/span&gt;         ALLOW IN    Anywhere &lt;span class="o"&gt;(&lt;/span&gt;v6&lt;span class="o"&gt;)&lt;/span&gt;WW
&lt;/pre&gt;&lt;/div&gt;


&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://www.digitalocean.com/community/tutorials/how-to-set-up-a-firewall-with-ufw-on-ubuntu-14-04"&gt;Good reads on configuring UFW firewall&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;Step 7 - Connect to VNC Server&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Use any remote desktop connect client that allow VNC protocol. Use &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/IP_address"&gt;IP address&lt;/a&gt;&lt;/em&gt; of server along with port where VNC server is listening.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div style="text-align: right"&gt;&lt;sub&gt;Connect -Remote Desktop Viewer&lt;/sub&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="VNC Connect" src="/images/vnc/vnc-connect.png" title="VNC Connect"&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Once connected to your VNC server, you will see screen of remote server where you installed desktop GUI.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div style="text-align: right"&gt;&lt;sub&gt;Launch Firefox from Terminal&lt;/sub&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="Launch Firefox" src="/images/vnc/vnc-launch-firefox.png" title="Launch Firefox"&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Browser screen running on remote server.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div style="text-align: right"&gt;&lt;sub&gt;Google UK&lt;/sub&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="Google UK" src="/images/vnc/vnc-google-uk.png" title="Google UK"&gt;&lt;/p&gt;
&lt;p&gt;Thatâs it, your VNC server is working.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Here I created my linux server in london, UK. I opened firefox through terminal to reach out to URL &lt;em&gt;google.com&lt;/em&gt;. It opened google.co.uk domain based on regional search engine. You can do lot of other stuffs on VNC protocol to get things done from remote location.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h5&gt;Medium Blog : &lt;a href="https://medium.com/@sunnykrgupta/installation-of-vnc-server-on-ubuntu-1cf035370bd3"&gt;medium.com/@sunnykrgupta/installation-of-vnc-server-on-ubuntu-1cf035370bd3&lt;/a&gt;&lt;/h5&gt;</content><category term="vnc"></category><category term="vncserver"></category><category term="ubuntu"></category><category term="desktop installation"></category></entry><entry><title>Managing fleet on Kubernetes</title><link href="https://sunnykrGupta.github.io/managing-fleet-on-kubernetes.html" rel="alternate"></link><published>2017-08-13T19:45:00+05:30</published><updated>2017-08-13T19:45:00+05:30</updated><author><name>Sunny Kr Gupta</name></author><id>tag:sunnykrgupta.github.io,2017-08-13:/managing-fleet-on-kubernetes.html</id><summary type="html">&lt;blockquote&gt;
&lt;p&gt;Couple of months ago, we were tackling challenges with scalability of system and were in pursuit of finding right orchestration tools which can help in scaling system quickly. This draft is outline of things we have tried and learned along the way, , most of things might sound familiar to you â¦&lt;/p&gt;&lt;/blockquote&gt;</summary><content type="html">&lt;blockquote&gt;
&lt;p&gt;Couple of months ago, we were tackling challenges with scalability of system and were in pursuit of finding right orchestration tools which can help in scaling system quickly. This draft is outline of things we have tried and learned along the way, , most of things might sound familiar to you. A Quick glance of things we came across while building fleet on Kubernetes.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We started exploring popular project managed by Google for orchestration management, &lt;strong&gt;&lt;a href="https://kubernetes.io/"&gt;Kubernetes&lt;/a&gt;&lt;/strong&gt; for DevOps. Starting with two weeks of learning curves, we get our working staging system in &lt;em&gt;kubes&lt;/em&gt; (kubernetes in short) and did small working setup to visualize the power of this orchestration framework.&lt;/p&gt;
&lt;hr&gt;
&lt;h4&gt;Microservices&lt;/h4&gt;
&lt;p&gt;Microservice architectures have been trending because its architectural style aims to tackle the problems of managing modern application by decoupling software solutions into smaller functional services that are expected to fail.&lt;/p&gt;
&lt;p&gt;This help in quick recovery from failure on smaller functional units in contrast to making recovery from big monolithic software systems. Microservices helps in making your release cycle faster even because you will be focusing on smaller changes in single app instead of pushing code changes in bigger software systems that has multiple dependencies.&lt;/p&gt;
&lt;hr&gt;
&lt;h4&gt;Containers&lt;/h4&gt;
&lt;p&gt;Microservice architectures got a big tide in 2013 when Docker inc. released Docker technology. &lt;strong&gt;Docker container&lt;/strong&gt; gave perfect alternatives to virtual machines and drove software packaging methods in a more developer friendly way. Docker container are comparatively smaller than virtual machines (VMs). Its shares underlying host OS resources, we can spin up hundreds of these small units in order of milliseconds. Their smaller size helps in faster packaging, testing and even deployments because of its portable nature.&lt;/p&gt;
&lt;p&gt;Dockerâs container-based platform allows highly portable workloads. Docker containers can run on a developerâs local laptop, on physical or virtual machines in a data center, on cloud providers, or in a mixture of environments.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;We started with &lt;code&gt;Google Container Engine (GCE)&lt;/code&gt; to get things work quickly. We started with a cluster with few &lt;code&gt;10's of Nodes&lt;/code&gt;, each Node with configuration &lt;code&gt;12 vCore and 30 GB&lt;/code&gt; in &lt;strong&gt;&lt;a href="https://cloud.google.com/container-engine/docs/node-pools"&gt;default pool&lt;/a&gt;&lt;/strong&gt; to run stateless components.&lt;/p&gt;
&lt;p&gt;Before going in depth, we needs some &lt;em&gt;gears&lt;/em&gt; (concepts/tools/theory) to onboard into container ship and sail out for cruise.&lt;/p&gt;
&lt;p&gt;We are dividing gears that we need to know into two parts, ie, first will be &lt;code&gt;Docker&lt;/code&gt; and second will focused on &lt;code&gt;Kubernetes&lt;/code&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h4&gt;Part - I (Understanding Docker at Dock)&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Stateless and stateful components.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In computing, a stateless protocol is a communication protocol in which no information is retained by either sender or receiver. The sender transmits a packet to the receiver and does not expect an acknowledgment of receipt. A UDP connection-oriented session is a stateless connection because neither systems maintains information about the session during its life.&lt;/li&gt;
&lt;li&gt;In contrast, a protocol that requires keeping of the internal state on the server is known as a stateful protocol. A TCP connection-oriented session is a 'stateful' connection because both systems maintain information about the session itself during its life.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.redhat.com/en/containers"&gt;Understanding containerization concept&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Container provides operating system-level virtualization through a virtual environment that has its own process and network space, instead of creating a full-fledged &lt;a href="https://en.wikipedia.org/wiki/Virtual_machine"&gt;virtual machine&lt;/a&gt;. This enables the kernel of an operating system to allow the existence of multiple isolated user-space instances, instead of just one.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://docs.docker.com/engine/userguide/eng-image/dockerfile_best-practices/"&gt;Writing good Dockerfile for modules&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Dockerfile is set of instruction used by Docker to build an image. Containers are created using docker images, which can be built either by executing commands manually or automatically through Dockerfile. Docker achieves this by creating safe, &lt;strong&gt;LXC&lt;/strong&gt; (i.e. Linux Containers) based environments for applications called âdocker containersâ.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Writing optimized Dockerfile, understanding order of commands. Each command that we run in Dockerfile is executed as a layer and subsequent command will be build on top of previous layer. Each layer is managed in cache by Docker tool. Docker manages cache itself to reuse layer of previously build Docker images to save time &amp;amp; disk.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;I have three file in my directory named &amp;#39;flask&amp;#39; :
â  flask

ââ app.py
ââ Dockerfile
ââ requirement.txt
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="c"&gt;##### cat app.py&lt;/span&gt;

&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;flask&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Flask&lt;/span&gt;
&lt;span class="c"&gt;#from flask import render_template, request&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;json&lt;/span&gt;

&lt;span class="n"&gt;app&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Flask&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;__name__&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="nd"&gt;@app.route&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;/&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;hello&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Welcome to Python Flask!&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;__main__&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;app&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;host&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;0.0.0.0&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;port&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;5009&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;###### cat requirement.txt

Flask==0.10.1
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;###### cat Dockerfile

FROM frolvlad/alpine-python2

RUN mkdir /etc/flask

ADD app.py /etc/flask/app.py
ADD requirement.txt /etc/flask/requirement.txt


CMD [&amp;quot;python&amp;quot;, &amp;quot;/etc/flask/app.py&amp;quot;]
&lt;/pre&gt;&lt;/div&gt;


&lt;blockquote&gt;
&lt;p&gt;Lets build our docker image with name &lt;strong&gt;flask-v1&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;â  docker build -t flask-v1 .

Sending build context to Docker daemon 4.096 kB

Step 1/5 : FROM frolvlad/alpine-python2
latest: Pulling from frolvlad/alpine-python2
627beaf3eaaf: Pull complete
79d39e719c2e: Pull complete
Digest: sha256:47e3f85dadf401d51c6f74a18d4f693c2157692292e6dae0a078f37499a183ee
Status: Downloaded newer image for frolvlad/alpine-python2:latest
 ---&amp;gt; 603e17608203

Step 2/5 : RUN mkdir /etc/flask
 ---&amp;gt; Running in 0d97b7a8986b
 ---&amp;gt; 9b2a858914e2
Removing intermediate container 0d97b7a8986b

Step 3/5 : ADD app.py /etc/flask/app.py
 ---&amp;gt; 1a4938be7722
Removing intermediate container f4d11b837e26

Step 4/5 : ADD requirement.txt /etc/flask/requirement.txt
 ---&amp;gt; 5d058851dd81
Removing intermediate container 08eef72a4051

Step 5/5 : CMD python /etc/flask/app.py
 ---&amp;gt; Running in 96490917e533
 ---&amp;gt; a081e6cbcf3c
Removing intermediate container 96490917e533

Successfully built a081e6cbcf3c
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now, we have made some modification in &lt;strong&gt;app.py&lt;/strong&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="c"&gt;##### cat app.py&lt;/span&gt;

&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;flask&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Flask&lt;/span&gt;
&lt;span class="c"&gt;#from flask import render_template, request&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;json&lt;/span&gt;

&lt;span class="n"&gt;app&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Flask&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;__name__&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="nd"&gt;@app.route&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;/&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;hello&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Welcome to Python Flask!&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&lt;/span&gt;

&lt;span class="c"&gt;#Added utility&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;utility&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;something&amp;quot;&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;__main__&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;app&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;host&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;0.0.0.0&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;port&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;5009&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;blockquote&gt;
&lt;p&gt;Lets build our docker image with name &lt;strong&gt;flask-v2&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;â  docker build -t flask-v2 .

Sending build context to Docker daemon 4.096 kB

Step 1/5 : FROM frolvlad/alpine-python2
 ---&amp;gt; 603e17608203

Step 2/5 : RUN mkdir /etc/flask
 ---&amp;gt; Using cache
 ---&amp;gt; 9b2a858914e2

Step 3/5 : ADD app.py /etc/flask/app.py
 ---&amp;gt; a7565514aab3
Removing intermediate container 360eb266a458

Step 4/5 : ADD requirement.txt /etc/flask/requirement.txt
 ---&amp;gt; 8441bca383f0
Removing intermediate container 15ebbb15d67d

Step 5/5 : CMD python /etc/flask/app.py
 ---&amp;gt; Running in 73e4bbcbe512
 ---&amp;gt; 0f4a5754f0d0
Removing intermediate container 73e4bbcbe512

Successfully built 0f4a5754f0d0
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;You will see only &lt;em&gt;step-2&lt;/em&gt; was taken from cache, rest of the instructions ran again because change has been detected by Docker on &lt;em&gt;step-3&lt;/em&gt; command ie, &lt;code&gt;ADD app.py&lt;/code&gt; , so all subsequent commands ran again to build layer on top of previous layer.&lt;/p&gt;
&lt;p&gt;Now we have made changes in &lt;code&gt;Dockerfile&lt;/code&gt;, some reorder of commands.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;# cat Dockerfile

FROM frolvlad/alpine-python2

RUN mkdir /etc/flask

# add requirement file first, then commands which contains some changes.
ADD requirement.txt /etc/flask/requirement.txt
ADD app.py /etc/flask/app.py

CMD [&amp;quot;python&amp;quot;, &amp;quot;/etc/flask/app.py&amp;quot;]
&lt;/pre&gt;&lt;/div&gt;


&lt;blockquote&gt;
&lt;p&gt;Lets build our docker image with name &lt;strong&gt;flask-v3&lt;/strong&gt; and lets see build console.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;â  flask docker build -t flask-v3 .

Sending build context to Docker daemon 4.096 kB

Step 1/5 : FROM frolvlad/alpine-python2
 ---&amp;gt; 603e17608203

Step 2/5 : RUN mkdir /etc/flask
 ---&amp;gt; Using cache
 ---&amp;gt; 9b2a858914e2

Step 3/5 : ADD requirement.txt /etc/flask/requirement.txt
 ---&amp;gt; Using cache
 ---&amp;gt; db8fc95cebff

Step 4/5 : ADD app.py /etc/flask/app.py
 ---&amp;gt; 9fb8351616e1
Removing intermediate container 4e98534d5339

Step 5/5 : CMD python /etc/flask/app.py
 ---&amp;gt; Running in 139f8c4282d8
 ---&amp;gt; e3fae9852d94
Removing intermediate container 139f8c4282d8

Successfully built e3fae9852d94
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now, you can see only &lt;em&gt;step-2,3&lt;/em&gt; was taken from &lt;code&gt;cache&lt;/code&gt;, &lt;em&gt;step-4&lt;/em&gt; command ie, &lt;code&gt;ADD app.py&lt;/code&gt; build new layer because of change detected in &lt;code&gt;app.py&lt;/code&gt; file, and this build saved little bit of our time. This is really important in building big Docker images where we have bigger chain of command to build an app.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://docs.docker.com/engine/userguide/eng-image/dockerfile_best-practices/#each-container-should-have-only-one-concern"&gt;Running a single process inside a Docker container&lt;/a&gt;.&lt;ul&gt;
&lt;li&gt;âone process per containerâ is frequently a good rule of thumb, it is not a hard and fast rule. Use your best judgment to keep containers as clean and modular as possible - Docker&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Understanding remote Docker container registry for storing/pushing our locally built docker images, here we have used Google container registry (GCR) for docker image management.&lt;ul&gt;
&lt;li&gt;&lt;a href="https://cloud.google.com/container-registry/docs/pushing-and-pulling"&gt;Pushing and Pulling Images to GCR&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.docker.com/docker-cloud/builds/push-images/"&gt;Push images to Docker Cloud&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h4&gt;Part - II ( Understanding Kubernetes in Ocean )&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Learning basics of kubernetes &amp;amp; &lt;a href="https://kubernetes.io/docs/tutorials/kubernetes-basics/"&gt;work flow training&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Kubernetes is an open-source platform for automating deployment, scaling, and operations of application containers across clusters of hosts, providing container-centric infrastructure - Kubernetes.io&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://kubernetes.io/docs/concepts/workloads/pods/pod/#what-is-a-pod"&gt;What are Pods? How container run inside a pod?&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pods are the atomic unit on the Kubernetes platform. A Pod is a Kubernetes abstraction that represents a group of one or more application containers (such as Nginx or redis), and some shared resources for those containers.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div style="text-align: right"&gt;&lt;sub&gt;Pod Overview : Images by Kubernetes.io&lt;/sub&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="Pods overview image" src="/images/kubes/module_03_pods.svg" title="Pods Overview"&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://kubernetes.io/docs/concepts/nodes/node/#what-is-a-node"&gt;What are Nodes?&lt;/a&gt; &lt;sub&gt;(also known as worker or minion, a single machine)&lt;/sub&gt;&lt;ul&gt;
&lt;li&gt;A Pod always runs inside a Node. A Node is a worker machine in Kubernetes and may be either a virtual or a physical machine, depending on the cluster. Node is controlled by Kubernetes Master. Kubernetes manages scheduling of pods in Nodes running in a cluster.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div style="text-align: right"&gt;&lt;sub&gt;Node Overview : Images by Kubernetes.io&lt;/sub&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="Node overview image" src="/images/kubes/module_03_nodes.svg" title="Node Overview"&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/"&gt;What are deployments?&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;Deployments to create new resources, or replace existing ones by new ones by means of configuration defined. You can think of it as a supervisor of pods management.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;#### cat sample-deployment.yaml

apiVersion: apps/v1beta1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.7.9
      ports:
      - containerPort: 80
&lt;/pre&gt;&lt;/div&gt;


&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/"&gt;What is replication controller and Replica sets?&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A ReplicationController and Replica Sets ensures that a specified number of pod âreplicasâ are running at any one time. In other words, it makes sure that a pod or homogeneous set of pods are always up and available. If there are too many pods, it will kill some. If there are too few, it will start more.&lt;blockquote&gt;
&lt;p&gt;In above &lt;code&gt;yaml&lt;/code&gt; file, you can see &lt;code&gt;replicas&lt;/code&gt; keyword, this is being managed by replication utility.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://kubernetes.io/docs/concepts/overview/components/"&gt;What is Kubernetes master?&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The controlling services in a Kubernetes cluster are called the master, or control plane, components. For example, master components are responsible for making global decisions about the cluster (e.g., scheduling), and detecting and responding to cluster events (e.g., starting up a new pod when a replication controllerâs âreplicasâ field is unsatisfied). Kubernetes provides a REST API supporting primarily CRUD operations on (mostly) persistent resources, which serve as the hub of its control plane.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/architecture.md#architecture"&gt;Kubernetes Ecosystem consists of mutiple components.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://kubernetes.io/docs/concepts/services-networking/service/"&gt;What are services?&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A Kubernetes Service is an abstraction which defines a logical set of Pods and a policy by which to access them. The set of Pods targeted by a Service is (usually) determined by a Label Selector. Service keep on looking for pods which has specific labels assigned and keep tracks of those pods for request offloading.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div style="text-align: right"&gt;&lt;sub&gt;Service Overview : Images by Kubernetes.io&lt;/sub&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="Service overview image" src="/images/kubes/module_04_labels.svg" title="Service Overview"&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;###### cat sample-services.yaml

kind: Service
apiVersion: v1
metadata:
  name: my-service
spec:
  type: LoadBalancer
  loadBalancerIP: 10.10.10.1
  ports:
    # the port that this service should serve on
  - port: 80
    # port on which it should forward request ie, port pod is listening
    targetPort: 8080
  selector:
    # labels assigned to pods
    app: MyApp
&lt;/pre&gt;&lt;/div&gt;


&lt;ul&gt;
&lt;li&gt;&lt;a href="https://kubernetes.io/docs/user-guide/kubectl-cheatsheet/"&gt;How to debug or get cluster info from command-line?&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;code&gt;kubectl&lt;/code&gt; is a command line interface for running commands against Kubernetes clusters.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="x"&gt;-----General Commands&lt;/span&gt;

&lt;span class="cp"&gt;##&lt;/span&gt;&lt;span class="c"&gt; To view Nodes in a cluster&lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;
&lt;span class="p"&gt;$&lt;/span&gt;&lt;span class="nv"&gt;kubectl&lt;/span&gt;&lt;span class="x"&gt; get nodes&lt;/span&gt;

&lt;span class="x"&gt;NAME                                          STATUS    AGE&lt;/span&gt;
&lt;span class="x"&gt;gke-test-cluster-default-pool-2d123aa1-012f   Ready     2d&lt;/span&gt;
&lt;span class="x"&gt;gke-test-cluster-default-pool-2d123aa1-e23k   Ready     2d&lt;/span&gt;


&lt;span class="cp"&gt;##&lt;/span&gt;&lt;span class="c"&gt; To view the Deployment we created run:&lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;
&lt;span class="p"&gt;$&lt;/span&gt;&lt;span class="x"&gt;    kubectl get deployments&lt;/span&gt;

&lt;span class="x"&gt;NAME         DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE&lt;/span&gt;
&lt;span class="x"&gt;hello-node   1         1         1            1           3m&lt;/span&gt;


&lt;span class="cp"&gt;##&lt;/span&gt;&lt;span class="c"&gt; To view the Pod created by the deployment run:&lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;
&lt;span class="p"&gt;$&lt;/span&gt;&lt;span class="x"&gt;    kubectl get pods&lt;/span&gt;

&lt;span class="x"&gt;NAME                         READY     STATUS    RESTARTS   AGE&lt;/span&gt;
&lt;span class="x"&gt;hello-node-714049816-ztzrb   1/1       Running   0          6m&lt;/span&gt;


&lt;span class="cp"&gt;##&lt;/span&gt;&lt;span class="c"&gt; To view the Pod created by the deployment run:&lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;
&lt;span class="p"&gt;$&lt;/span&gt;&lt;span class="x"&gt;    kubectl get pods&lt;/span&gt;

&lt;span class="x"&gt;NAME                         READY     STATUS    RESTARTS   AGE&lt;/span&gt;
&lt;span class="x"&gt;hello-node-714049816-ztzrb   1/1       Running   0          6m&lt;/span&gt;

&lt;span class="cp"&gt;##&lt;/span&gt;&lt;span class="c"&gt; To view detailed information of the Pod:&lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;
&lt;span class="p"&gt;$&lt;/span&gt;&lt;span class="x"&gt; kubectl get pods -o wide&lt;/span&gt;
&lt;span class="x"&gt;or&lt;/span&gt;
&lt;span class="p"&gt;$&lt;/span&gt;&lt;span class="x"&gt; kubectl get pods --output=wide&lt;/span&gt;

&lt;span class="x"&gt;NAME             READY     STATUS    RESTARTS   AGE       IP&lt;/span&gt;
&lt;span class="x"&gt;dd-agent-0f75g   1/1       Running   0          23d       10.204.5.16&lt;/span&gt;


&lt;span class="cp"&gt;##&lt;/span&gt;&lt;span class="c"&gt; To view the stdout / stderr from a Pod run:&lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;
&lt;span class="p"&gt;$&lt;/span&gt;&lt;span class="x"&gt;    kubectl logs &amp;lt;POD-NAME&amp;gt;&lt;/span&gt;

&lt;span class="cp"&gt;##&lt;/span&gt;&lt;span class="c"&gt; To view metadata about the cluster run:&lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;
&lt;span class="p"&gt;$&lt;/span&gt;&lt;span class="x"&gt;    kubectl cluster-info&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;hr&gt;
&lt;h4&gt;How do we run containers in GCE ?&lt;/h4&gt;
&lt;p&gt;We have number of &lt;code&gt;deployments&lt;/code&gt; which manages scaling pods up/down depend on processing we need. Pods run containers inside Node available in a cluster. We need to follow proper versioning of modules to distinguish what is running inside your system and this helps in rollback releases in case of issues in production.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;How about services/APIs we need to expose ?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;There comes kubes &lt;code&gt;services&lt;/code&gt;. We have plenty of APIs we need to expose to outside world. To make it happen, we have couple of kube services exposed using tcp loadbalancer which has been assigned public IP. Internally, these services keeps on doing service discovery using &lt;code&gt;label selector&lt;/code&gt; to find pods and attach it to this service, pods having same label will be targeted by a service. Its same concept of how we manage loadbalancer on cloud, attach VMs to a loadbalancer to offload incoming traffic.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Resources running inside Kube ship knows each other very well. Each &lt;code&gt;services/pods&lt;/code&gt; can communicate by names assigned to each. Instead of using IPs (private) assigned to each of them, you can use names given as FQDN, its a good practise to use names instead of IPs because of dynamic nature of network resource allocation since resources get destroyed and created again in a container lifecycle management. Kube-DNS maintains all list of IPs internally assigned and helps finding resources by names.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h4&gt;&lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-ram-container/"&gt;How to decide what resources you should allocate to your pods resources&lt;/a&gt;?&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;Convention
# RAM : Mi = MB, ie, 1024Mi is 1024 MB or 1GB
# CPU : m = milicpu , ie, 100m cpu is 100 milicpu, or say 0.1 CPU

apiVersion: v1
kind: Pod
metadata:
  name: cpu-ram-demo
spec:
  containers:
  - name: cpu-ram-demo-container
    image: gcr.io/google-samples/node-hello:1.0
    resources:
      requests:
        memory: &amp;quot;64Mi&amp;quot;
        cpu: &amp;quot;250m&amp;quot;
      limits:
        memory: &amp;quot;128Mi&amp;quot;
        cpu: &amp;quot;500m&amp;quot;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Each container has its own requirements of resources (ie, CPU, RAM, disk, network etc), there comes requests &amp;amp; limits in kubes. This helps in keeping your nodes healthy. Many times due to bad limits or not defining limits, your pods can go crazy at utilization, they might eat any resources, can lead to node starvation that results in Nodes becomes unhealthy and goes in &lt;code&gt;[Not Ready]&lt;/code&gt; state due to resource exhaustion. We faced this multiple times at early stage and now we have fine tuned each pods resources based on its hunger behaviour.&lt;/p&gt;
&lt;h5&gt;How to define Node resources in kubernetes cluster?&lt;/h5&gt;
&lt;p&gt;Depends on container type &lt;sub&gt;(which you are running inside a pod)&lt;/sub&gt;, you can define different Node pools. Suppose you have modules named &lt;code&gt;Core.X, Core.Y and Core.Z&lt;/code&gt; , all of them needs &lt;code&gt;2 core, 2 GB&lt;/code&gt; each to run, then you can have &lt;strong&gt;Standard Node Pool&lt;/strong&gt; to run them. In this case, i will allocate following config for my Node pool.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Name : Standard Pool&lt;/li&gt;
&lt;li&gt;Pool Size : 2&lt;/li&gt;
&lt;li&gt;Node Config: 4 Core, 4 GB&lt;/li&gt;
&lt;li&gt;Node Pool Resource : 8 Core, 8 GB&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Utilization&lt;/code&gt; : 6 Core, 6 GB (75 % used Core &amp;amp; RAM)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now, lets say i have high memory eater modules. let call them &lt;code&gt;Mem.X, Mem.Y and Mem.Z&lt;/code&gt; , all of them needs &lt;code&gt;0.5 core, 4 GB&lt;/code&gt; each to run, then you need &lt;strong&gt;High memory Node Pool&lt;/strong&gt; to run them. In this case, i will allocate different config for my Node pool.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Name : HighMem Pool&lt;/li&gt;
&lt;li&gt;Pool Size : 2&lt;/li&gt;
&lt;li&gt;Node Config : 1 Core, 8 GB&lt;/li&gt;
&lt;li&gt;Node Pool Resource : 2 Core, 16 GB&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Utilization&lt;/code&gt; : 1.5 Core, 12 GB (75 % used Core &amp;amp; RAM)&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;So, based on your &lt;a href="https://cloud.google.com/container-engine/docs/node-pools"&gt;Node pool type&lt;/a&gt;, you can deploy your pods in different Node pools by using &lt;code&gt;nodeSelector&lt;/code&gt; in kubes.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;-------------- Node selector example

apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    env: test
spec:
  containers:
  - name: nginx-v1
    image: nginx
  nodeSelector:
    #give label assigned to your node pool
    cloud.google.com/gke-nodepool: high-mem-pool
&lt;/pre&gt;&lt;/div&gt;


&lt;hr&gt;
&lt;h4&gt;How we monitor Kubernetes ?&lt;/h4&gt;
&lt;p&gt;We can run custom monitoring setup to keep an eye on Nodes. You can run &lt;a href="https://github.com/kubernetes/heapster"&gt;heapster&lt;/a&gt;, ie. responsible for compute resource usage analysis and monitoring of container clusters, hooked with &lt;a href="https://github.com/influxdata/influxdb"&gt;influxdb&lt;/a&gt; that consumes reporting pushed by heapster and can be visualized in &lt;a href="https://grafana.com/"&gt;grafana&lt;/a&gt;.&lt;/p&gt;
&lt;div style="text-align: right"&gt;&lt;sub&gt;Monitoring in Grafana&lt;/sub&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="Grafana monitoring" src="/images/kubes/grafana.png" title="Grafana monitoring"&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Note :&lt;/strong&gt; Some configuration in GCE should be taken care, like &lt;code&gt;autoupgrade kubernetes version&lt;/code&gt;. If you are running RabbitMQ, Redis or any other message queue as service that needs uptime, better you turn off autoupgrade because kubernetes new version release will schedule all your node for maintenance, however it rolles updates one by one but could affect your production system. Else, if you are fully stateless, you can keep default or skip this warning!&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;----- Autoupgrade off

# https://cloud.google.com/container-engine/docs/node-auto-upgrade

gcloud beta container node-pools update &amp;lt;NODEPOOL&amp;gt; --cluster &amp;lt;CLUSTER&amp;gt; --zone &amp;lt;ZONE&amp;gt; --no-enable-autoupgrade
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Pretty much all above understanding are based on what I learned in last six months of kubernetes running in production. Container management is easy to adapt and lot of new observation is yet to be discovered as we go along the way.&lt;/p&gt;
&lt;p&gt;Looking at deployments today, Kubernetes is absolutely fantastic in Auto-pilot and doing self-healing jobs itself. We are running more than &lt;code&gt;1000 pods&lt;/code&gt; in cluster together and processing &lt;code&gt;10's of Billions of API calls per month&lt;/code&gt; and are pushing more to handle.&lt;/p&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;Conclusion : &lt;code&gt;Kubernetes&lt;/code&gt; lifted lot of &lt;code&gt;server management&lt;/code&gt; and helped in faster depployments &amp;amp; scaling system. Adaptability is much quicker, most of security and other concerns is being managed by Google. Kubernetes aims to offer a better orchestration management system on top of clustered infrastrcuture. Development on Kubernetes has been happening at storm-speed, and the &lt;a href="https://kubernetes.io/community/"&gt;community of Kubernauts&lt;/a&gt; has grown bigger.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h5&gt;Medium Blog : &lt;a href="https://medium.com/@sunnykrgupta/managing-fleet-on-kubernetes-8cac6483b64"&gt;medium.com/@sunnykrgupta/managing-fleet-on-kubernetes-8cac6483b64&lt;/a&gt;&lt;/h5&gt;</content><category term="kubernetes"></category><category term="docker"></category><category term="gce"></category><category term="gcr"></category><category term="container"></category></entry><entry><title>A Practical Guide to Geopy</title><link href="https://sunnykrGupta.github.io/a-practical-guide-to-geopy.html" rel="alternate"></link><published>2015-07-19T01:31:31+05:30</published><updated>2015-07-19T01:31:31+05:30</updated><author><name>Sunny Kr Gupta</name></author><id>tag:sunnykrgupta.github.io,2015-07-19:/a-practical-guide-to-geopy.html</id><summary type="html">&lt;p&gt;During my final major academic project, i was working with &lt;a href="http://twitter.com"&gt;Twitter&lt;/a&gt; data (ie. tweets). I did some research and found out approximately only 1% of all Tweets published on Twitter are geolocated (ie. have location information). This is a very small portion of the Tweets, and i needed almost every â¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;During my final major academic project, i was working with &lt;a href="http://twitter.com"&gt;Twitter&lt;/a&gt; data (ie. tweets). I did some research and found out approximately only 1% of all Tweets published on Twitter are geolocated (ie. have location information). This is a very small portion of the Tweets, and i needed almost every tweets location to segregate data country-wise.
  - How to resolve a string location to determine country ("Banagher", "Ipoh" etc )
  - How to resolve a Coordinate into country. ('4.581' - '101.082' etc)&lt;/p&gt;
&lt;p&gt;There comes a &lt;a href="http://geopy.readthedocs.org/"&gt;Geopy&lt;/a&gt; to rescue us to this problem.&lt;/p&gt;
&lt;h4&gt;Geopy&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Geopy makes it easy for Python developers to locate the coordinates of addresses, cities, countries, and landmarks across the globe using third-party geocoders and other data sources.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Geocoding is the process of converting addresses (like "1600 Amphitheatre Parkway, Mountain View, CA") into geographic coordinates (like latitude 37.423021 and longitude 122.083739) or reverse.&lt;/p&gt;
&lt;h5&gt;Available Geocoder APIs&lt;/h5&gt;
&lt;p&gt;There are several Geocoding service provided by different Map APIs, populars are listed below:
 - &lt;a href="https://developers.google.com/maps/documentation/geolocation/intro"&gt;Google Maps Geocoding V3 API&lt;/a&gt; (2500 per-day)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://www.geonames.org/about.html"&gt;Geonames&lt;/a&gt; (30000 per-day | 2000 per-hour)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://wiki.openstreetmap.org/wiki/Nominatim"&gt;Nominatim - Open Street Map&lt;/a&gt; (refer Usage Docs)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://tech.yandex.com/maps/doc/geocoder/desc/concepts/About-docpage/"&gt;Yandex Map API&lt;/a&gt;  (25,000 per-day)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://msdn.microsoft.com/en-us/library/ff428643.aspx"&gt;Bing  Map API (Microsoft)&lt;/a&gt; (refer Usage Docs)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://developer.yahoo.com/boss/geo/"&gt;Yahoo BOSS Finder&lt;/a&gt; (refer Usage Docs)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Note : Geocoder APIs may have request limts on per-day or per-IP or others. Increasing those limts can result in blacklisting. At the time of writing this post, limits are mentioned in brackets above.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;Installation&lt;/h4&gt;
&lt;hr&gt;
&lt;p&gt;Open your favorite Terminal and run these commands. Install using &lt;code&gt;pip&lt;/code&gt; with:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;pip install geopy
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Geocoding&lt;/h4&gt;
&lt;p&gt;To query a location using string using &lt;code&gt;Google MAP V3&lt;/code&gt;. To acquire key you need to register your app on google developer console.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;geopy.geocoders&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;GoogleV3&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;json&lt;/span&gt;
&lt;span class="n"&gt;geolocator&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;GoogleV3&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;api_key&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;AIzaSyCxk0i1WQokYRgUxAZieq&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;location&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;geolocator&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;geocode&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Washington&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;language&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;en&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;location&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;json&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dumps&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;location&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;raw&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;indent&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;location&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;address&lt;/span&gt;
&lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;No location!&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;location&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Gocoders have different service api classes, here i have used &lt;code&gt;GoogleV3&lt;/code&gt;, then an object is created named &lt;code&gt;geolocator&lt;/code&gt; to query and saved results of my query string "Washington" in &lt;code&gt;location&lt;/code&gt;. If query string doesnt contain valid place name or city, it throws &lt;code&gt;None&lt;/code&gt;. Now lets see what we got after this script:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;quot;geometry&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;location_type&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;APPROXIMATE&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;bounds&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
            &lt;span class="nt"&gt;&amp;quot;northeast&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
                &lt;span class="nt"&gt;&amp;quot;lat&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;38.995548&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nt"&gt;&amp;quot;lng&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;-76.909393&lt;/span&gt;
            &lt;span class="p"&gt;},&lt;/span&gt;
            &lt;span class="nt"&gt;&amp;quot;southwest&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
                &lt;span class="nt"&gt;&amp;quot;lat&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;38.8031495&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nt"&gt;&amp;quot;lng&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;-77.11974&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
        &lt;span class="p"&gt;},&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;location&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
            &lt;span class="nt"&gt;&amp;quot;lat&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;38.9071923&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nt"&gt;&amp;quot;lng&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;-77.0368707&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
        &lt;span class="p"&gt;},&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;quot;address_components&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
        &lt;span class="p"&gt;{&lt;/span&gt;
            &lt;span class="nt"&gt;&amp;quot;long_name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Washington&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="nt"&gt;&amp;quot;types&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;locality&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;political&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
            &lt;span class="nt"&gt;&amp;quot;short_name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;D.C.&amp;quot;&lt;/span&gt;
        &lt;span class="p"&gt;},&lt;/span&gt;
        &lt;span class="p"&gt;{&lt;/span&gt;
            &lt;span class="nt"&gt;&amp;quot;long_name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;District of Columbia&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="nt"&gt;&amp;quot;types&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;administrative_area_level_1&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;political&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
            &lt;span class="nt"&gt;&amp;quot;short_name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;DC&amp;quot;&lt;/span&gt;
        &lt;span class="p"&gt;},&lt;/span&gt;
        &lt;span class="p"&gt;{&lt;/span&gt;
            &lt;span class="nt"&gt;&amp;quot;long_name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;United States&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="nt"&gt;&amp;quot;types&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;country&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;political&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
            &lt;span class="nt"&gt;&amp;quot;short_name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;US&amp;quot;&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;quot;place_id&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;ChIJW-T2Wt7Gt4kRKl2I1CJFUsI&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;quot;formatted_address&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Washington, DC, USA&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;quot;types&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;locality&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;political&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="err"&gt;Washington,&lt;/span&gt; &lt;span class="err"&gt;DC,&lt;/span&gt; &lt;span class="err"&gt;USA&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;So, you can see its a format of data returned from Google Map API, so we need to extract the information we are looking for from this resulted data. Easy huh! You can see lattitude. longitude , Country name, District name, and other place information.
&lt;/br&gt;
You can query a string of coordinate too, in above code replace the geolocator lines with follows and run:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;location&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;geolocator&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reverse&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;52.509669, 13.376294&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;You can see the raw JSON output as result. See below, if you want formatted address, use &lt;code&gt;location.address&lt;/code&gt; , &lt;code&gt;location.latitude&lt;/code&gt;, &lt;code&gt;location.longitude&lt;/code&gt; gives you coordinates of place and &lt;code&gt;location.raw&lt;/code&gt; gives you the result like we saw above in JSON format, that contains lots of other information.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;location&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;address&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;Potsdamer&lt;/span&gt; &lt;span class="n"&gt;Platz&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Mitte&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Berlin&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10117&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Deutschland&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;European&lt;/span&gt; &lt;span class="n"&gt;Union&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;location&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;latitude&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;location&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;longitude&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;52.5094982&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;13.3765983&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Similarly we can use other Geocoder APIs too, &lt;code&gt;Nominatim&lt;/code&gt; as openstreet map, &lt;code&gt;Yandex&lt;/code&gt; for Yandex Map, &lt;code&gt;GeoNames&lt;/code&gt; as Geonames Geocoder etc.&lt;/p&gt;
&lt;p&gt;Geopy code for &lt;code&gt;GeoNames&lt;/code&gt; and &lt;code&gt;Yandex&lt;/code&gt; :&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;geopy.geocoders&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;GeoNames&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;json&lt;/span&gt;
&lt;span class="n"&gt;geolocator&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;GeoNames&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;username&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Ur_user_NAME&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c"&gt;# Register at Geonames&lt;/span&gt;
&lt;span class="n"&gt;location&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;geolocator&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;geocode&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;å¥åº&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;timeout&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;location&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;json&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dump&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;location&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;raw&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;location&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;address&lt;/span&gt;
&lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;No location!&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;location&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;geopy.geocoders&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Yandex&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;json&lt;/span&gt;
&lt;span class="n"&gt;geolocator&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Yandex&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lang&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;en_US&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;location&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;geolocator&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;geocode&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Ø¨ØºØ¯Ø§Ø¯Ø Ø§ÙØ¹Ø±Ø§Ù&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;timeout&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;location&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;json&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dumps&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;location&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;raw&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;indent&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;location&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;address&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;location&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;latitude&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot; -&amp;gt; &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;location&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;longitude&lt;/span&gt;
&lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;location&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;So simple right! for using other Geocoder Map APIs , refer to full documentation of &lt;a href="http://geopy.readthedocs.org/en/latest/#"&gt;Geopy&lt;/a&gt;. I hope you understands the working of Geocoder APIs and power of Geopy.
&lt;/br&gt;Comment below if you come across any problem and give feedback. Thanks you all! Have a good day!&lt;/p&gt;</content><category term="Geopy"></category><category term="Geocoder"></category><category term="Python"></category><category term="GoogleV3"></category><category term="Yandex"></category><category term="Geonames"></category></entry><entry><title>Get Familiar with Python</title><link href="https://sunnykrGupta.github.io/get-familiar-with-python.html" rel="alternate"></link><published>2014-05-26T17:23:31+05:30</published><updated>2014-05-26T17:23:31+05:30</updated><author><name>Sunny Kr Gupta</name></author><id>tag:sunnykrgupta.github.io,2014-05-26:/get-familiar-with-python.html</id><summary type="html">&lt;p&gt;Python is a powerful scripting language.&lt;/p&gt;
&lt;h4&gt;Guide's to Python :&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Learn about &lt;a href="http://en.wikipedia.org/wiki/REST"&gt;REST architecture&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Know and get familiar with Python. [2][2.1][10]&lt;/li&gt;
&lt;li&gt;Setup virtualenv [3]&lt;/li&gt;
&lt;li&gt;Get the feel of handling backend. Complete the django tutorial [4] from   official site.&lt;/li&gt;
&lt;li&gt;Know a bit more about django architecture [5][8]&lt;/li&gt;
&lt;li&gt;Write â¦&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;p&gt;Python is a powerful scripting language.&lt;/p&gt;
&lt;h4&gt;Guide's to Python :&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Learn about &lt;a href="http://en.wikipedia.org/wiki/REST"&gt;REST architecture&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Know and get familiar with Python. [2][2.1][10]&lt;/li&gt;
&lt;li&gt;Setup virtualenv [3]&lt;/li&gt;
&lt;li&gt;Get the feel of handling backend. Complete the django tutorial [4] from   official site.&lt;/li&gt;
&lt;li&gt;Know a bit more about django architecture [5][8]&lt;/li&gt;
&lt;li&gt;Write you first api using tastypie[6][7]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Footnotes:&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;[1]: http://en.wikipedia.org/wiki/REST&lt;/p&gt;
&lt;p&gt;[2]: &lt;a href="http://www.youtube.com/watch?v=u1sVfGEBKWQ"&gt;What makes python awesome&lt;/a&gt;.
[2.1]: &lt;a href="http://www.python.org/dev/peps/pep-0008"&gt;Python Style Guide&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;[3]: http://stackoverflow.com/questions/4806448/how-do-i-install-from-a-local-cache-with-pip&lt;/p&gt;
&lt;p&gt;[4]: https://docs.djangoproject.com/en/1.5/intro/tutorial01/&lt;/p&gt;
&lt;p&gt;[5]: http://www.youtube.com/watch?v=t_ziKY1ayCo (~3hr video by James Bennett)&lt;/p&gt;
&lt;p&gt;[6]: http://django-tastypie.readthedocs.org/en/latest/&lt;/p&gt;
&lt;p&gt;[7]: http://pyvideo.org/video/673/restful-apis-with-tastypie&lt;/p&gt;
&lt;p&gt;[8]: &lt;a href="http://bit.ly/16z2gQq"&gt;Two scoops for Django Book&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[9]: &lt;a href="http://bit.ly/197rnvU"&gt;Pro python Book&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[10]: http://docs.python.org/2/tutorial/index.html&lt;/p&gt;
&lt;p&gt;[11]: https://github.com/twoscoops/django-twoscoops-project&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;Additional Resources:&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Django best practises [8]&lt;/li&gt;
&lt;li&gt;Pro Python Book [9]&lt;/li&gt;
&lt;li&gt;Pycon Videos: [http://pyvideo.org/category/33/pycon-us-2013]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Community and updates:&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;http://www.pythonweekly.com/&lt;/li&gt;
&lt;li&gt;http://mail.python.org/mailman/listinfo/bangpypers&lt;/li&gt;
&lt;li&gt;https://groups.google.com/forum/#!forum/django-users&lt;/li&gt;
&lt;li&gt;http://mail.python.org/mailman/listinfo/ncr-python.in&lt;/li&gt;
&lt;/ul&gt;</content><category term="python"></category></entry><entry><title>Octopress - Blog Frameworks</title><link href="https://sunnykrGupta.github.io/octopress-blog-frameworks.html" rel="alternate"></link><published>2014-05-26T17:23:31+05:30</published><updated>2014-05-26T17:23:31+05:30</updated><author><name>Sunny Kr Gupta</name></author><id>tag:sunnykrgupta.github.io,2014-05-26:/octopress-blog-frameworks.html</id><summary type="html">&lt;p&gt;Hello, Everyone! This is blog post for helping you setup your own blog using Octopress framework running over Jekyll.
I am Summer Intern at &lt;a href="http://www.ophio.co.in"&gt;Ophio&lt;/a&gt;, a New York based company. I have been given a task to set up my blog. My mentor Mr.&lt;a href="https://github.com/theskumar"&gt;Saurabh&lt;/a&gt; is a Senior Back-end Developer â¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;Hello, Everyone! This is blog post for helping you setup your own blog using Octopress framework running over Jekyll.
I am Summer Intern at &lt;a href="http://www.ophio.co.in"&gt;Ophio&lt;/a&gt;, a New York based company. I have been given a task to set up my blog. My mentor Mr.&lt;a href="https://github.com/theskumar"&gt;Saurabh&lt;/a&gt; is a Senior Back-end Developer. He developed many cool stuffs and active contibutor to open-source technology.&lt;/p&gt;
&lt;h3&gt;Deployment of Octopress : Blog Frameworks for Hackers&lt;/h3&gt;
&lt;p&gt;Octopress is a static blogging framework built on top of Jekyll. It uses scripts to build static files to be deployed to a server. You can design your blog and deploy on &lt;a href="https://pages.github.com/"&gt;Github-Pages&lt;/a&gt; very quickly.&lt;/p&gt;
&lt;h4&gt;Installation's :&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Git&lt;/li&gt;
&lt;li&gt;Jekyll&lt;/li&gt;
&lt;li&gt;Ruby Gem&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;How to setup Octopress?? &lt;a href="http://octopress.org/docs/setup/"&gt;Go ahead&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Variety of theme available &lt;a href="https://github.com/imathis/octopress/wiki/3rd-Party-Octopress-Themes"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Responsive Blog" src="images/main-screenshot.png" title=" Octopress Image"&gt;&lt;/p&gt;</content><category term="Octopress"></category><category term="ruby"></category></entry><entry><title>Pelican Python Blogging Framework</title><link href="https://sunnykrGupta.github.io/pelican-python-blogging-framework.html" rel="alternate"></link><published>2014-05-26T17:23:31+05:30</published><updated>2014-05-26T17:23:31+05:30</updated><author><name>Sunny Kr Gupta</name></author><id>tag:sunnykrgupta.github.io,2014-05-26:/pelican-python-blogging-framework.html</id><summary type="html">&lt;p&gt;Hello, Everyone! This is 2nd blog post about Pelican framework in python.&lt;/p&gt;
&lt;h4&gt;Deployment of Pelican : Blog Frameworks&lt;/h4&gt;
&lt;p&gt;Pelican is a static site generator, written in Python. It strikes as
a very interesting method as you can write your content purely in your favourite editor (in Markdown Flavour) and commit your â¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;Hello, Everyone! This is 2nd blog post about Pelican framework in python.&lt;/p&gt;
&lt;h4&gt;Deployment of Pelican : Blog Frameworks&lt;/h4&gt;
&lt;p&gt;Pelican is a static site generator, written in Python. It strikes as
a very interesting method as you can write your content purely in your favourite editor (in Markdown Flavour) and commit your post or style changes using Git. You can design your blog and deploy on &lt;a href="https://pages.github.com/"&gt;Github-Pages&lt;/a&gt; very quickly.&lt;/p&gt;
&lt;h4&gt;Installation's :&lt;/h4&gt;
&lt;p&gt;How to setup Pelican?? &lt;a href="http://docs.getpelican.com/en/stable/quickstart.html"&gt;Go ahead&lt;/a&gt; and detailed explanation of kicking a small webpage and deploying using github-pages, &lt;a href="http://seanazlin.com/creating-a-blog-on-GitHub-dot-io-with-Python.html"&gt;Creating-a-blog-on-GitHub-dot-io-with-Python&lt;/a&gt; by Sean Azlin. I got most of the help in building this blog from this.&lt;/p&gt;
&lt;p&gt;Variety of &lt;a href="https://github.com/getpelican/pelican-themes"&gt;themes&lt;/a&gt; available.&lt;/p&gt;
&lt;h4&gt;Note's :&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;Follow Markdown syntax to write post.&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Deploy your page by following and check your gitpage-url :&lt;/p&gt;
&lt;p&gt;$ ghp-import output&lt;/p&gt;
&lt;p&gt;$ git checkout master&lt;/p&gt;
&lt;p&gt;$ git merge gh-pages&lt;/p&gt;
&lt;p&gt;$ git push --all&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img alt="Responsive Blog" src="https://sunnykrGupta.github.io/images/Pelican.jpg" title="Pelican Image"&gt;&lt;/p&gt;</content><category term="pelican"></category><category term="python"></category></entry></feed>